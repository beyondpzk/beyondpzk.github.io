<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> VLM4VLA | Tenacious life, proud journey. </title> <meta name="author" content="P W Name"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://beyondpzk.github.io/blog/2026/VLM4VLA/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Tenacious life, proud journey. </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">VLM4VLA</h1> <p class="post-meta"> Created on January 06, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/category/vla"> <i class="fa-solid fa-tag fa-sm"></i> VLA</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>[TOC]</p> <h1 id="vlm4vla">VLM4VLA</h1> <h2 id="论文地址"><a href="https://arxiv.org/abs/2601.03309" rel="external nofollow noopener" target="_blank">论文地址</a></h2> <h1 id="vlm4vla--探究具身智能中的视觉-语言基座效应">VLM4VLA —— 探究具身智能中的视觉-语言基座效应</h1> <p><strong>参考论文：</strong> <em>VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models</em> <strong>目标：</strong></p> <ol> <li> <strong>解构</strong> VLA (Vision-Language-Action) 模型的标准范式与不同变体。</li> <li> <strong>剖析</strong> VLM4VLA 的实验控制变量法设计及其背后的科学严谨性。</li> <li> <strong>批判性思考</strong> “通用智能”与“具身控制”之间的特征表示差异（Feature Representation Gap）。</li> <li> <strong>掌握</strong> 评估 VLA 模型性能的核心指标与基准测试方法。</li> </ol> <hr> <h2 id="第一部分从大模型到具身智能的演进">第一部分：从大模型到具身智能的演进</h2> <h3 id="1-理论背景vlm-的解剖">1. 理论背景：VLM 的解剖</h3> <ul> <li> <strong>VLM 的通用架构</strong> <ul> <li> \[\text{Input} = \{ \text{Image}, \text{Text Instructions} \}\] </li> <li> \[\text{Architecture} = \text{Vision Encoder} (e.g., \text{SigLIP, ViT}) + \text{Projector} (e.g., \text{MLP, Q-Former}) + \text{LLM Backbone}\] </li> </ul> </li> <li> <strong>现状回顾：</strong> 目前开源社区有大量的 VLM（如 Qwen-VL, Paligemma, LLaVA）。它们在 VQA（视觉问答）、Captioning（图像描述）上表现出色。</li> <li> <strong>核心假设：</strong> 既然 VLM 懂物理世界的语义（比如知道“杯子”是用来“喝水”的，知道“把手”在哪里），那么能否直接用它来控制机器人？</li> <li> <strong>VLA 的定义：</strong> 将“动作 (Action)”视为一种特殊的“模态”或“语言”。 <ul> <li>早期尝试：RT-2 (Google) —— 将动作离散化为 Token (e.g., “128”, “255”)，直接用 LLM 自回归生成。</li> </ul> </li> </ul> <h3 id="2-本文的研究动机乱象中的反思">2. 本文的研究动机：乱象中的反思</h3> <ul> <li> <strong>当前 VLA 领域的乱象：</strong> <ul> <li>每篇新论文都提出一个新的架构（Diffusion Policy, ACT, Flow Matching）。</li> <li>每篇论文都换一个 VLM 基座（Llama, Qwen, Vicuna）。</li> <li> <strong>痛点：</strong> 当一个新模型 SOTA 时，我们不知道是因为架构好了？还是因为基座 VLM 变强了？还是数据清洗得更干净了？</li> </ul> </li> <li> <strong>VLM4VLA 的核心定位：</strong> <ul> <li>它不是为了刷榜（SOTA），而是为了建立一个<strong>受控实验台 (Controlled Testbed)</strong>。</li> <li> <strong>Research Question (RQ):</strong> VLM 的选择和能力，如何转化为下游 VLA 的策略性能？<alphaxiv-paper-citation title="Core Question" page="1" first="how VLM choice" last="policies performance?"></alphaxiv-paper-citation> </li> </ul> </li> </ul> <h3 id="3-vla-的系统-1与系统-2之争">3. VLA 的“系统 1”与“系统 2”之争</h3> <ul> <li> <strong>讨论：</strong> 机器人控制的层级。 <ul> <li> <em>System 2 (高层规划):</em> “去厨房煮咖啡” -&gt; 需要 VLM 的推理能力。</li> <li> <em>System 1 (底层控制):</em> “关节转动 0.5 弧度，手爪闭合” -&gt; 需要高频、精确的几何感知。</li> </ul> </li> <li> <strong>本文的关注点：</strong> 本文关注的是 End-to-End 的策略学习，即 VLM 是否能胜任 <em>System 1</em> 的角色？这挑战了 VLM 原本的预训练目标（语义对齐）。</li> </ul> <hr> <h2 id="第二部分方法论与实验架构详解">第二部分：方法论与实验架构详解</h2> <h3 id="1-vlm4vla-管道设计极简主义的胜利-20分钟">1. VLM4VLA 管道设计：极简主义的胜利 (20分钟)</h3> <ul> <li> <strong>架构概览 (结合 Figure 2 讲解)：</strong> <ul> <li>为了公平比较，必须剔除所有花哨的技巧（Tricks）。</li> <li> <strong>输入序列设计：</strong> \(\text{Sequence} = [ \langle \text{img} \rangle ... \langle \text{img} \rangle, \langle \text{text} \rangle ... \langle \text{text} \rangle, \langle \text{ActionQuery} \rangle ]\)</li> <li> <strong>关键组件：ActionQuery Token</strong> <ul> <li>这是一个可学习的 Token。它的作用是从 VLM 的深层特征中“汇聚”出与动作相关的信息。</li> </ul> </li> <li> <strong>解码头 (Policy Head)：</strong> <ul> <li>作者仅仅使用了一个 <strong>MLP</strong>。</li> <li> <strong>深度提问：</strong> 为什么不用现在流行的 Diffusion Head？</li> <li> <strong>答案：</strong> 为了<strong>减少随机性</strong>。Diffusion 引入了采样随机性，这会增加评估方差，干扰对 VLM 基座能力的判断。作者需要一个确定性的比较环境。<alphaxiv-paper-citation title="Why MLP" page="5" first="We use a" last="flow-matching) approach,"></alphaxiv-paper-citation> </li> </ul> </li> </ul> </li> </ul> <h3 id="2-损失函数与训练目标-15分钟">2. 损失函数与训练目标 (15分钟)</h3> <ul> <li> <strong>公式 (Equation 1)：</strong> \(\mathcal{L} = \frac{1}{|B|} \sum_{B} \left( \| a_{pos} - \hat{a}_{pos} \|^2_2 + \text{BCE}(a_{end}, \hat{a}_{end}) \right)\) <ul> <li> <strong>第一项：</strong> 修正的 MSE Loss (Huber Loss)，用于回归连续的关节位置/末端执行器位姿 ($a_{pos}$)。</li> <li> <strong>第二项：</strong> BCE Loss，用于二分类（比如夹爪的开/合状态 $a_{end}$）。</li> </ul> </li> <li> <strong>全参数微调 (Full Fine-tuning)：</strong> <ul> <li>作者微调了 VLM 的<strong>所有参数</strong>（包括 Vision Encoder, LLM, Word Embeddings）。</li> <li>这非常关键，因为如果是 LoRA 或 Freeze，可能会掩盖基座模型的真实潜力。</li> </ul> </li> </ul> <h3 id="3-实验设置的严谨性">3. 实验设置的严谨性</h3> <ul> <li> <strong>数据处理：</strong> 所有图像统一 Resize 到 $224 \times 224$。</li> <li> <strong>输入限制：</strong> 只用单视角图像，不使用本体感知 (Proprioception)。这是为了强迫模型必须依赖视觉理解，防止模型“作弊”（通过本体感知死记硬背动作）。</li> <li> <strong>基准测试 (Benchmarks) 的选择逻辑：</strong> <ul> <li> <strong>Calvin:</strong> 测试<strong>长程序列</strong> (Long-horizon)，看模型能不能连续做对5件事。</li> <li> <strong>SimplerEnv (Google):</strong> 测试<strong>泛化性</strong>，在模拟器中测试真实世界的分布偏移 (Sim-to-Real-to-Sim)。</li> <li> <strong>Libero:</strong> 测试<strong>任务多样性</strong>。</li> </ul> </li> </ul> <h2 id="第三部分核心实验结果与反直觉发现">第三部分：核心实验结果与反直觉发现</h2> <h3 id="1-vlm-基座大比拼">1. VLM 基座大比拼</h3> <ul> <li> <strong>参赛选手：</strong> <ul> <li>Qwen2.5-VL (3B/7B), Qwen3-VL</li> <li>Paligemma (Google, 专门为迁移学习设计)</li> <li>Kosmos-2 (Microsoft, 擅长 Grounding)</li> <li>OpenVLA, Pi0 (作为 SOTA 基线)</li> </ul> </li> <li> <strong>核心图表解读 (Figure 3)：相关性分析</strong> <ul> <li> <strong>现象：</strong> 作者画了一张散点图，横轴是 VLM 在通用任务（如 MMBench, Math, Coding）上的得分，纵轴是 VLA 的成功率。</li> <li> <strong>结论：</strong> <strong>弱相关甚至无相关。</strong> <ul> <li>例如：Kosmos 在某些任务上打败了参数量更大、通用评分更高的 Qwen 和 Paligemma。</li> <li> <strong>关键引用：</strong> <alphaxiv-paper-citation title="Prediction Failure" page="1" first="VLM’s general capabilities" last="downstream task performance."></alphaxiv-paper-citation> </li> </ul> </li> <li> <strong>意义：</strong> 这打破了业界的迷信——“只要把基座模型做大做强，机器人就自动变强了”。事实并非如此。</li> </ul> </li> </ul> <h3 id="2-辅助任务的滑铁卢">2. 辅助任务的“滑铁卢”</h3> <ul> <li> <strong>假设验证：</strong> 如果我在 VLM 上先训练一些“相关任务”，效果会好吗？(即串行, 先在辅助任务上面训,再训练VLA。并不是像 $\pi_{0.5}$ 中的Co-Training一样.)</li> <li> <strong>测试任务集：</strong> <ol> <li> <strong>Robopoint:</strong> 给图，输出物体坐标（点选）。</li> <li> <strong>Depth Estimation:</strong> 估计深度图。</li> <li> <strong>Embodied QA:</strong> 机器人视角的问答。</li> </ol> </li> <li> <strong>实验结果 (Figure 4)：</strong> <strong>大部分都是负收益或无收益。</strong> <ul> <li>即使在 Robopoint 上微调让点选准确率提升了 20%，但变成 VLA 后，抓取成功率反而可能下降。</li> <li> <strong>深层原因探讨：</strong> 这种“感知能力”与“控制策略”是解耦的。知道物体坐标 (X,Y) 是一回事，生成一条平滑的、避障的 7-DoF 轨迹是完全另一回事。</li> </ul> </li> </ul> <h3 id="3-消融实验谁才是瓶颈">3. 消融实验：谁才是瓶颈?</h3> <ul> <li> <strong>实验设计：</strong> <ul> <li>(A) 冻结 Vision Encoder。</li> <li>(B) 冻结 LLM。</li> <li>(C) 冻结 Word Embeddings。</li> </ul> </li> <li> <strong>震耳欲聋的结论 (Table 3)：</strong> <ul> <li> <strong>冻结视觉编码器 = 毁灭性打击。</strong> 性能暴跌 (e.g., Calvin 得分从 3.8 跌到 0.5)。</li> <li><strong>冻结 LLM = 影响有限。</strong></li> <li> <strong>解读：</strong> VLM 的瓶颈不在于“推理”（LLM部分），而在于“看”（Vision Encoder）。现有的 CLIP/SigLIP 视觉特征主要是为了“语义对齐”（Semantic Alignment），而不是为了“几何控制”（Geometric Control）。<alphaxiv-paper-citation title="Modality Ablation" page="1" first="identifies the visual" last="performance bottleneck."></alphaxiv-paper-citation> </li> </ul> </li> </ul> <hr> <h2 id="第四部分深入探究视觉鸿沟与未来展望">第四部分：深入探究“视觉鸿沟”与未来展望</h2> <h3 id="1-到底是仿真的问题还是语义的问题">1. 到底是“仿真”的问题，还是“语义”的问题?</h3> <ul> <li> <strong>质疑：</strong> 也许视觉编码器表现不好，是因为 VLM 是在真实世界图片上训练的，而测试是在仿真环境（Sim）里？是不是 Sim-to-Real 的 Gap？</li> <li> <strong>精妙的验证实验 (Section 4.4)：</strong> <ul> <li>作者使用了 <strong>BridgeV2</strong> 数据集（全真实世界图像）。</li> <li>作者设计了一个 VLM 微调任务：用 VLM 预测离散化的动作 Token (Fast-Token)。</li> <li> <strong>对比组：</strong> <ol> <li>微调 VLM 时冻结视觉编码器。</li> <li>微调 VLM 时解冻视觉编码器。</li> </ol> </li> <li> <strong>结果 (Table 4)：</strong> 即使是在全是真实图像的数据上，如果冻结视觉编码器，性能依然很差。只有解冻并训练视觉部分，性能才提升。</li> <li> <strong>结论：</strong> 这证明了 Gap <strong>不是</strong>来自于 Sim-to-Real 的风格差异，而是来自于 <strong>Pretraining Objective (图文匹配)</strong> 与 <strong>Control Objective (动作输出)</strong> 之间的本质语义鸿沟。<alphaxiv-paper-citation title="Real World Exp" page="11" first="Even when training" last="improve downstream performance."></alphaxiv-paper-citation> </li> </ul> </li> </ul> <h3 id="2-总结与讨论">2. 总结与讨论</h3> <ul> <li> <strong>VLM4VLA 的启示：</strong> <ul> <li>不要盲目追求大参数量 VLM。</li> <li>视觉表征（Visual Representation）是目前具身智能最大的短板。</li> <li>未来的方向不应该是“更多的数据”，而应该是“更具身的数据”来预训练视觉编码器（例如使用视频预测、光流、物理交互数据）。</li> </ul> </li> <li> <strong>开放问题讨论：</strong> <ul> <li> <em>Q1:</em> 如果让你设计一个新的预训练任务来替代 CLIP，专门服务于机器人，你会怎么设计？（提示：考虑 Inverse Dynamics, State Estimation）。</li> <li> <em>Q2:</em> 论文中提到 MLP Head 是为了公平比较，但实际应用中，你认为 Diffusion Head 能弥补 VLM 基座的不足吗？</li> </ul> </li> </ul> <p><strong>思考：</strong> 既然 VLM 预训练特征（语义导向）与控制任务（几何/动作导向）之间存在“域差异 (Domain Gap)”，为什么 VLM 初始化依然比从头训练 (Training from Scratch) 表现要好？下面结合“流形学习 (Manifold Learning)”的概念，并参考论文 Figure 5 (关于特征空间可视化的部分) 进行解释。</p> <h4 id="核心论点从无序混沌到结构化流形的跃迁"><strong>核心论点：从“无序混沌”到“结构化流形”的跃迁</strong></h4> <p>尽管 VLM 的预训练目标（图文对齐）没有直接教导机器人如何“运动”，但它为神经网络提供了一个<strong>高度结构化的特征流形 (Structured Feature Manifold)</strong>。这种结构化的初始状态，远比随机初始化的“混沌状态”更容易通过微调收敛到最优策略。</p> <h4 id="1-流形学习视角的解释"><strong>1. 流形学习视角的解释</strong></h4> <ul> <li> <strong>高维数据的低维流形：</strong> 图像是极高维的数据（$224 \times 224 \times 3$ 像素），但在高维空间中，有效的图像数据分布在一个低维的流形上。</li> <li> <strong>从头训练 (Random Init) 的困境：</strong> <ul> <li>如果从零开始训练，视觉编码器 (Vision Encoder) 必须同时解决两个难题： <ol> <li> <strong>表征学习 (Representation Learning)：</strong> 学会如何从像素中提取边缘、纹理、物体边界，构建视觉流形。</li> <li> <strong>策略学习 (Policy Learning)：</strong> 学会根据特征输出动作。</li> </ol> </li> <li>由于机器人数据通常很稀缺（相比于互联网图文数据），从头训练的模型很难在有限数据下构建出鲁棒的视觉流形，容易陷入过拟合或无法捕捉复杂的物体关系。</li> </ul> </li> <li> <strong>VLM 初始化的优势 (The “Warm Start”)：</strong> <ul> <li>预训练的 VLM（如 SigLIP 或 CLIP 编码器）已经通过数十亿张图像的学习，构建了一个成熟的视觉流形。在这个流形中，相似语义的物体已经聚集在一起，背景噪声已经被过滤。</li> <li> <strong>克服“域差异”：</strong> 虽然这个流形是“语义”的（比如它知道这是“杯子”，但不知道“杯柄坐标”），但它已经具备了<strong>可塑性 (Plasticity)</strong>。将一个已经能识别“物体”的特征空间，微调成能识别“几何坐标”的空间，在优化路径上比从纯噪声开始要短得多、容易得多。</li> </ul> </li> </ul> <h4 id="2-结合-figure-5-特征空间可视化-的分析"><strong>2. 结合 Figure 5 (特征空间可视化) 的分析</strong></h4> <p>参考论文中的 Figure 5（通常展示 t-SNE 或 PCA 的特征投影图），我们可以观察到以下现象，佐证上述理论：</p> <ul> <li> <strong>图表描述：</strong> 该图展示了不同模型处理输入图像后得到的 Token Embedding 在二维空间中的分布。</li> <li> <strong>VLM 初始化 (Fine-tuned VLM)：</strong> <ul> <li>其特征点的分布呈现出<strong>清晰的聚类 (Clustering)</strong> 结构。</li> <li>这意味着模型能够将不同的任务指令（如“打开抽屉”与“抓取苹果”）对应的视觉场景，在特征空间中清晰地分离开来。</li> <li> <strong>关键点：</strong> 这种分离能力很大程度上继承自预训练权重。VLM 能够“理解”场景内容的变化，因此策略头 (MLP Head) 只需要学习简单的线性或非线性映射即可输出动作。</li> </ul> </li> <li> <strong>从头训练 (Scratch / Random Init)：</strong> <ul> <li>虽然论文并未直接画出 Scratch 失败的 t-SNE，但对比可以看出，未经过大规模预训练的特征空间往往是<strong>纠缠 (Entangled)</strong> 的。</li> <li>在纠缠的空间中，不同的任务状态混合在一起，决策边界极其复杂，导致策略学习失败。</li> </ul> </li> </ul> <h4 id="3-总结">3. 总结</h4> <ol> <li>VLM 初始化之所以有效，是因为它解决了 <strong>“感知 (Perception)”</strong> 这一最困难的第一步。 虽然 VLM 的“感知”是不完美的（缺乏几何精度，即 Domain Gap），但它提供了一个 <strong>包含物体概念和场景结构的先验知识库</strong>。微调过程实质上是在这个良好的“地基”上进行修补和对齐，而不是在平地上从头盖楼。因此，尽管通用能力不能完美预测控制性能，但预训练本身是构建高性能 VLA 不可或缺的基石。 <alphaxiv-paper-citation title="Conclusion" page="8" first="VLM initialization offers" last="training from scratch"></alphaxiv-paper-citation> </li> </ol> <h2 id="对上面两个开放讨论题的一些思考">对上面两个开放讨论题的一些思考</h2> <h3 id="q1-如果让你设计一个新的预训练任务来替代-clip专门服务于机器人你会怎么设计"><strong>Q1: 如果让你设计一个新的预训练任务来替代 CLIP，专门服务于机器人，你会怎么设计？</strong></h3> <p><strong>背景：</strong> CLIP 的对比学习目标（Contrastive Learning）主要是为了对齐<strong>高层语义</strong>（例如：图片里有“狗”和“草地”），它忽略了<strong>底层几何</strong>（物体具体的空间位置）和<strong>时间动力学</strong>（物体如何运动）。机器人需要的是后两者。</p> <p>我们可以构想一个 <strong>“Physics-Aware &amp; Action-Centric” (物理感知与动作中心)</strong> 的预训练框架，包含以下三个互补的子任务：</p> <h4 id="1-逆动力学预测-inverse-dynamics-prediction"><strong>1. 逆动力学预测 (Inverse Dynamics Prediction)</strong></h4> <ul> <li> <strong>设计逻辑：</strong> 不要只看静态图片。给模型看视频片段中的两帧：$I_t$ (当前帧) 和 $I_{t+k}$ (未来帧)。</li> <li> <strong>任务目标：</strong> 预测 $I_t$ 到 $I_{t+k}$ 之间发生了什么动作？ \(\text{Loss} = \| f(I_t, I_{t+k}) - a_{t:t+k} \|^2\)</li> <li> <strong>为什么有效：</strong> CLIP 只能告诉你“这里有一个被推到的杯子”。逆动力学预训练能强迫视觉编码器理解 <strong>“是什么动作导致了这种视觉变化”</strong>。这种特征对机器人控制（即根据目标状态反推动作）是直接同构的。</li> </ul> <h4 id="2-视频掩码预测-masked-video-modeling-with-physical-constraints"><strong>2. 视频掩码预测 (Masked Video Modeling with Physical Constraints)</strong></h4> <ul> <li> <strong>设计逻辑：</strong> 类似于 MAE (Masked Autoencoder)，但在视频流上做。</li> <li> <strong>任务目标：</strong> 遮挡住视频中物体接触的瞬间，让模型基于物理规律“脑补”出中间帧。</li> <li> <strong>关键改进：</strong> 不仅仅预测像素颜色（Pixel Loss），还要预测<strong>光流 (Optical Flow)</strong> 或 <strong>深度图 (Depth Map)</strong> 的变化。</li> <li> <strong>为什么有效：</strong> 这强迫模型学习物体恒常性 (Object Permanence) 和基础物理属性（重力、碰撞）。它让视觉特征包含“可供性 (Affordance)”信息——即知道物体是可以被推动或抓取的。</li> </ul> <h4 id="3-密集点跟踪-dense-point-tracking"><strong>3. 密集点跟踪 (Dense Point Tracking)</strong></h4> <ul> <li> <strong>设计逻辑：</strong> 参考 Google 的 TAP (Tracking Any Point) 技术。</li> <li> <strong>任务目标：</strong> 随机选择图像上的一个像素点（比如杯柄的一个点），要求模型在随后的视频序列中持续追踪这一个点，即使它被遮挡或旋转。</li> <li> <strong>为什么有效：</strong> 机器人抓取需要极高的几何精度。通过追踪点，视觉编码器被迫学习细粒度的<strong>对应关系 (Correspondence)</strong>，而不是像 CLIP 那样只关注全局的语义标签。</li> </ul> <p><strong>总结：</strong> 未来的预训练不应是 Image-Text Pair，而应是 <strong>Video-Action Pair</strong> 或纯视频流，目标是从“识别物体”转向“理解物理交互”。</p> <h3 id="q2-论文中提到-mlp-head-是为了公平比较但实际应用中diffusion-head-能弥补-vlm-基座的不足吗"><strong>Q2: 论文中提到 MLP Head 是为了公平比较，但实际应用中，Diffusion Head 能弥补 VLM 基座的不足吗？</strong></h3> <p><strong>背景：</strong> 这是一个关于 <strong>“策略表达能力 (Policy Expressivity)” vs. “感知瓶颈 (Perceptual Bottleneck)”</strong> 的辩证讨论。Diffusion Policy 是目前的 SOTA，它能建模多模态分布（Multimodal Distribution），比简单的 MLP 强得多。</p> <p>我的回答是：<strong>Diffusion Head 可以cover部分基座的缺陷，但无法修复根本性的感知盲区。</strong></p> <h4 id="1-diffusion-head-的补救作用-the-band-aid-effect"><strong>1. Diffusion Head 的“补救”作用 (The “Band-Aid” Effect)</strong></h4> <ul> <li> <strong>解决多模态分布问题：</strong> 当 VLM 基座给出的特征不够明确时，可能有多种合理的动作（比如抓杯子可以抓杯口，也可以抓杯柄）。 <ul> <li> <strong>MLP 的缺陷：</strong> MLP 倾向于输出所有可能动作的<strong>平均值</strong>（Mean），这往往是一个无效动作（抓空气）。</li> <li> <strong>Diffusion 的优势：</strong> 它可以建模复杂的分布，随机采样出其中一种合理的动作。因此，即使 VLM 特征有点模糊，Diffusion 也能通过强大的拟合能力生成平滑、拟人的轨迹。</li> </ul> </li> <li> <strong>平滑噪声：</strong> Diffusion 的去噪过程本身具有平滑轨迹的作用，可以抵消 VLM 特征中微小的抖动或不稳定性。</li> </ul> <h4 id="2-感知瓶颈的不可逾越性-garbage-in-garbage-out"><strong>2. 感知瓶颈的不可逾越性 (Garbage In, Garbage Out)</strong></h4> <ul> <li> <strong>核心论点：</strong> 策略头（Head）的能力上限受限于感知器（Backbone）的信息量。 \(P(Action | Image) = P(Action | Feature) \times P(Feature | Image)\)</li> <li> <strong>“看不见”的问题：</strong> 如果 VLM 的 Vision Encoder 根本没有编码“透明玻璃杯”的边缘特征（因为 CLIP 训练数据里透明物体很少），那么特征 $Z$ 中就不包含杯子的位置信息。 <ul> <li>在这种情况下，无论后面的 Diffusion Head 多么强大，它本质上是在<strong>瞎猜 (Hallucinating)</strong>。它可能会生成一条非常平滑、非常像人类动作的轨迹，但位置完全是错的（比如抓向了杯子左边 10 厘米处）。</li> </ul> </li> <li> <strong>VLM4VLA 的发现佐证：</strong> 论文中 Table 3 显示，冻结视觉编码器会导致性能崩盘。这证明了如果特征层（Perception）没有被调整到适应控制任务，后端的策略层再怎么训练也无力回天。</li> </ul> <h4 id="3-为什么-vlm4vla-坚持用-mlp"><strong>3. 为什么 VLM4VLA 坚持用 MLP？</strong></h4> <ul> <li> <strong>显微镜效应：</strong> 正因为 Diffusion 太强了，它可能会把 60 分的基座和 80 分的基座都拉到 90 分（在简单任务上）。</li> <li> <strong>结论：</strong> 在科研中，为了看清基座的差异，我们需要 MLP 这种“直肠子”网络作为显微镜。但在工程落地中，我们应该<strong>同时</strong>使用最好的基座（经过解冻微调的）和最好的头（Diffusion），以追求最佳性能。</li> </ul> <h1 id="关键术语表-glossary">关键术语表 (Glossary)</h1> <ul> <li> <strong>Proprioception:</strong> 本体感知（机器人的关节角度、速度等内部状态）。</li> <li> <strong>Action Chunking:</strong> 动作分块，一次预测未来 $k$ 步动作，用于平滑轨迹。</li> <li> <strong>Affordance:</strong> 可供性，物体提供的交互可能性（例如杯柄“提供”了抓取的可能性）。</li> <li> <strong>Domain Gap:</strong> 域差异，通常指训练数据分布与测试数据分布的不一致。</li> </ul> <h1 id="other-thoughts">Other thoughts</h1> <p>我认为当前具身智能（Embodied AI）研究中最核心的痛点：<strong>什么样的视觉表征（Visual Representation）才是机器人真正需要的？</strong> VLM4VLA 的结果在很大程度上暗示了现有的 VLM（以 CLIP/SigLIP 为视觉基座）并不是 VLA 最理想的预训练模型。 <strong>Dino-World</strong> (基于 DINOv2 特征的世界模型) 代表了另一条更有希望的技术路线——<strong>以物体和几何为中心的自监督学习</strong>。</p> <h3 id="1-为什么-vlm-的视觉基座clipsiglip不仅是不完美的甚至是有害的">1. 为什么 VLM 的视觉基座（CLIP/SigLIP）不仅是不完美的，甚至是有害的？</h3> <p>在 $VLM4VLA$ 论文中，我们看到冻结视觉编码器会导致性能崩盘。这背后的根本原因在于 <strong>预训练目标（Pre-training Objective）的错位</strong>：</p> <ul> <li> <strong>CLIP/SigLIP 的目标：语义对齐 (Semantic Alignment)</strong> <ul> <li>它们的目标是拉近“图片 embedding”和“文本 embedding”的距离。</li> <li> <strong>副作用（空间压缩）：</strong> 为了对齐文本（通常是抽象的），编码器倾向于扔掉“无关”的细节。</li> <li> <em>例子：</em> 对于文本“一只狗在草地上”，CLIP 只需要知道“有狗”和“有草”就行了。至于狗的左脚坐标是多少？草地的纹理摩擦力如何？这些信息对于图文匹配是<strong>噪音</strong>，因此被<strong>压缩掉（Discarded）</strong>了。</li> </ul> </li> <li> <strong>VLA 的需求：几何与物理 (Geometry &amp; Physics)</strong> <ul> <li>机器人控制需要的是：精确的 3D 坐标、物体边缘的接触点、物体之间的相对深度。</li> <li> <strong>矛盾点：</strong> 机器人最需要的信息，恰恰是 CLIP 在预训练中最想扔掉的信息。</li> </ul> </li> </ul> <p>这就是为什么VLM4VLA 发现必须<strong>解冻</strong>视觉编码器。解冻的本质，就是为了<strong>找回</strong>那些被预训练丢掉的空间几何信息。</p> <h3 id="2-为什么-dino-及-dino-world-是更好的候选者">2. 为什么 DINO (及 Dino-World) 是更好的候选者？</h3> <p><strong>Dino-World</strong> (Back to the Features: DINO as a Foundation for Video World Models) 这篇论文的核心论点是：<strong>DINOv2 的特征本身就包含了构建物理世界模型所需的一切，无需微调。</strong></p> <p>如果以 DINO/Dino-World 作为 VLA 的基座，优势在于：</p> <h4 id="a-更加密集的局部特征-dense--local-features">A. 更加密集的局部特征 (Dense &amp; Local Features)</h4> <ul> <li> <strong>VLM (CLIP):</strong> 关注全局语义（Global Token），是一个高度抽象的向量。</li> <li> <strong>DINO:</strong> 关注局部补丁（Patch-level Features）。DINOv2 的注意力图（Attention Map）天然就能分割出物体（Object Segmentation），即使没有监督信号。</li> <li> <strong>对 VLA 的意义：</strong> 机器人操作通常是局部的（比如“抓取杯柄”）。DINO 能清晰地看见“杯柄”作为一个独立的几何部件，而 CLIP 可能只看见“杯子”这个整体概念。</li> </ul> <h4 id="b-几何与对应关系-geometry--correspondence">B. 几何与对应关系 (Geometry &amp; Correspondence)</h4> <ul> <li> <strong>DINO 的特性：</strong> DINO 特征在不同视角下具有极强的一致性（Correspondence）。如果摄像头移动了，DINO 特征能精确地追踪图像上的同一个点。</li> <li> <strong>对 VLA 的意义：</strong> 这正是<strong>手眼协调 (Hand-Eye Coordination)</strong> 的基础。机器人需要知道“我现在的机械手位置”和“目标位置”在空间上的几何关系。Dino-World 证明了仅仅利用这些特征就能预测下一帧的物理状态，这意味着它懂<strong>物理 (Physics)</strong>。</li> </ul> <h4 id="c-无需语言的物理常识">C. 无需语言的物理常识</h4> <ul> <li>Dino-World 证明了在没有语言输入的情况下，仅靠视觉特征就能模拟物体掉落、液体流动。这种<strong>“前语言 (Pre-linguistic)”的物理直觉</strong>，正是 <em>System 1</em>（底层控制）所急需的。</li> </ul> <h3 id="3-硬币的另一面为什么还需要-vlmthe-alignment-problem">3. 硬币的另一面：为什么还需要 VLM？(The Alignment Problem)</h3> <p>虽然 DINO 在“看”和“动”方面更强，但如果完全用 DINO 替代 VLM，会面临一个巨大的挑战：<strong>语言指令的丢失 (The Grounding Gap)</strong>。</p> <ul> <li> <strong>VLM 的优势：</strong> 用户说“拿红色的苹果”，VLM 知道哪个是“红色”，哪个是“苹果”。</li> <li> <strong>DINO 的劣势：</strong> DINO 知道这里有一个圆形的物体，那里有一个方形的物体，但它<strong>不知道</strong>哪个叫“苹果”。DINO 的特征空间与语言空间没有对齐。</li> </ul> <h3 id="4-终极架构预测双流混合模型-hybrid-dual-stream-architecture">4. 终极架构预测：双流混合模型 (Hybrid Dual-Stream Architecture)</h3> <p>结合VLM4VLA的发现和Dino-World$ 的思考，未来的 VLA SOTA 架构很可能不是单纯的 VLM，也不是单纯的 DINO，而是两者的融合：</p> <ul> <li> <strong>架构设想：</strong> <ol> <li> <strong>视觉流 (Action Stream):</strong> 使用 <strong>DINOv2 / Dino-World</strong> 作为主视觉编码器。它负责提供高频、高精度的几何特征，直接输入给策略头 (Policy Head) 用于生成动作轨迹。</li> <li> <strong>语义流 (Semantic Stream):</strong> 使用轻量级的 <strong>VLM</strong> 或 <strong>CLIP</strong>。它负责处理用户的文本指令，生成一个“语义引导向量 (Semantic Guidance Vector)”。</li> <li> <strong>融合 (Fusion):</strong> 利用 Cross-Attention，让 DINO 的特征去“查询” VLM 的语义。 <ul> <li> <em>逻辑：</em> VLM 告诉 DINO “我们要找红色的苹果”，DINO 回答 “收到，在坐标 (x, y) 处有一个符合描述的物体，这是它的边缘几何信息”。</li> </ul> </li> </ol> </li> </ul> <h3 id="5-总结">5. 总结</h3> <p>VLM4VLA从反面证明了 <strong>“以语义为中心的 VLM 不适合直接做控制”</strong>。 而 <strong>Dino-World</strong> 这类工作指出了正确的方向：<strong>回归特征本身 (Back to the Features)</strong>。未来的 VLA 预训练模型，一定是在海量视频数据上通过自监督学习（学习物体恒常性、逆动力学）得到的，而不是仅仅通过图文对齐得到的。</p> <p><strong>思考：</strong> 现有的 “Foundation Models” 大多是基于 Internet Text/Image 的。我们是否需要一个专门的 <strong>“Robotics Foundation Model”</strong>？它的预训练数据不应该是 <code class="language-plaintext highlighter-rouge">&lt;Image, Text&gt;</code>，而应该是 <code class="language-plaintext highlighter-rouge">&lt;Video, Action&gt;</code> 或者纯粹的 <code class="language-plaintext highlighter-rouge">&lt;Physics Interactions&gt;</code>。</p> <h2 id="vision-与-language-的对齐是否有必要">vision 与 language 的对齐是否有必要</h2> <p><strong>文章并没有直接说“对齐（Alignment）没有必要”，但大量的实验证据强烈暗示：现有的“视觉-语言对齐”（即 CLIP/SigLIP 式的对齐）对于下游的控制任务来说，不仅是不充分的，甚至可能是次要的。</strong> 论文通过以下三个核心实验，间接回答这个问题：</p> <h3 id="1-证据一模态消融实验--语言并不那么重要">1. 证据一：模态消融实验 —— “语言并不那么重要”</h3> <p>在 <strong>Table 3</strong>（Modality-level ablations）中，作者做了一个极具启发性的对比：</p> <ul> <li> <strong>实验 A（冻结视觉）：</strong> 保持 Vision Encoder 不动（即保持完美的 VLM 预训练对齐状态），只微调 LLM 和 Projector。 <ul> <li> <strong>结果：</strong> 性能崩盘（例如 Calvin 分数从 3.8 跌到 0.5）。</li> <li> <strong>解读：</strong> 这说明即使保留了最完美的“视觉-语言对齐”特征，机器人也无法工作。</li> </ul> </li> <li> <strong>实验 B（冻结语言）：</strong> 保持 Word Embeddings 不动（即不再调整语言空间的映射），只微调 Vision Encoder。 <ul> <li> <strong>结果：</strong> 性能几乎没有下降（3.856 vs 3.849）。</li> <li> <strong>深度解读：</strong> 这说明<strong>语言端的对齐甚至不需要微调</strong>。机器人只要能看懂图像里的几何信息（Vision），语言指令（Language）只需要作为一个静态的“触发器”或“条件索引”就足够了。<strong>强求更深度的视觉-语言交互并没有带来额外收益。</strong> </li> </ul> </li> </ul> <alphaxiv-paper-citation title="Word Embedding Ablation" page="10" first="freeze word embedding" last="(-0.021)"></alphaxiv-paper-citation> <h3 id="2-证据二辅助任务微调--更强的对齐-neq-更强的控制">2. 证据二：辅助任务微调 —— “更强的对齐 $\neq$ 更强的控制”</h3> <p>在 <strong>Section 4.2</strong> 和 <strong>Figure 4</strong> 中，作者尝试通过 VQA（视觉问答）、Captioning（图像描述）等任务来增强 VLM。</p> <ul> <li> <strong>逻辑：</strong> 这些任务本质上都是在<strong>增强</strong> Vision 和 Language 的语义对齐能力。</li> <li> <strong>结果：</strong> 几乎所有的“对齐增强”操作（如 RoboPoint, Vica, Embodied VQA），在转化为 VLA 后，性能都<strong>没有提升</strong>，甚至略有下降。 (hmm~~~)</li> <li> <strong>结论：</strong> 进一步强化语义对齐是<strong>徒劳</strong>的。机器人需要的不是“更懂图文对应关系”，而是“更懂动作”。</li> </ul> <alphaxiv-paper-citation title="Auxiliary Tasks Failure" page="1" first="improving a VLM’s" last="control performance."></alphaxiv-paper-citation> <h3 id="3-证据三section-44-的动作注入实验--为了控制必须破坏对齐">3. 证据三：Section 4.4 的“动作注入”实验 —— “为了控制，必须破坏对齐”</h3> <p>这是一个非常微妙的推论。</p> <ul> <li>在 Section 4.4 中，作者发现必须解冻 Vision Encoder 并注入 Action Token 的监督信号，模型才能工作。</li> <li> <strong>思考一下这意味着什么：</strong> 当我们为了 Action Loss 去更新 Vision Encoder 的参数时，我们实际上是在<strong>破坏</strong>它原本与 Text Encoder 建立好的 CLIP 对齐空间。</li> <li> <strong>权衡（Trade-off）：</strong> 实验表明，为了获得控制能力（Control），模型“甚至愿意”牺牲一部分原本的图文对齐特性（通过改变视觉特征分布）。这证明了在 VLA 任务中，<strong>“动作对齐 (Action Alignment)”的优先级远高于“语言对齐 (Language Alignment)”。</strong> </li> </ul> <h3 id="总结">总结</h3> <p><strong>“视觉-语言对齐” (V-L Alignment) 仅仅是入场券，而不是胜负手。</strong></p> <ol> <li> <strong>入场券作用：</strong> 它让机器人知道“杯子”是哪个物体（Instruction Following）。没有对齐，机器人就连任务目标都找不到。</li> <li> <strong>非核心作用：</strong> 一旦锁定了目标，<strong>如何移动手臂</strong>（Motor Control）完全不依赖于语言对齐，而是依赖于几何感知。</li> <li> <strong>论文的启示：</strong> 现有的 VLM 预训练不仅<strong>过度关注</strong>了对齐（导致几何信息丢失），而且在微调时，我们实际上是在<strong>对抗</strong>这种对齐，试图把视觉特征从“语义空间”拉回到“物理空间”。</li> </ol> <p>所以，<strong>论文体现出，为了做好 VLA，保持或增强通用的视觉-语言对齐是没有必要的，甚至我们需要打破这种对齐，去建立新的“视觉-动作”对齐。</strong></p> <h2 id="论文的-figure-3-已经说了vla的能力和vlm的能力并不是正相关-那辅助tasks为什么还设计成-vqa的形式">论文的 Figure 3 已经说了VLA的能力和VLM的能力并不是正相关, 那辅助tasks为什么还设计成 VQA的形式</h2> <p>我的疑问： 前提 (Result from Fig 3): 作者自己已经证明了 VLM 通用能力（VQA分数） $\nRightarrow$ VLA 控制能力。 行为 (Method in Sec 4.2): 作者却依然把辅助任务设计成 VQA 形式（试图通过提升 VQA 分数来提升 VLA 能力）。 矛盾： 这不是明知故犯吗？既然已经知道 VQA 分数没用，为什么还要用 VQA 的方式来做辅助任务？</p> <p>这是一个<strong>极其犀利且一针见血</strong>的逻辑拷问！你发现了一个论文内部在逻辑设计上看似“自相矛盾”或“不够自洽”的地方。</p> <p><strong>你的质疑：</strong></p> <ol> <li> <strong>前提 (Result from Fig 3):</strong> 作者自己已经证明了 VLM 通用能力（VQA分数） $\nRightarrow$ VLA 控制能力。</li> <li> <strong>行为 (Method in Sec 4.2):</strong> 作者却依然把辅助任务设计成 VQA 形式（试图通过提升 VQA 分数来提升 VLA 能力）。</li> <li> <strong>矛盾：</strong> 既然已经知道 VQA 分数没用，为什么还要用 VQA 的方式来做辅助任务？</li> </ol> <hr> <h3 id="这里的逻辑层级差异">这里的逻辑“层级”差异</h3> <p>我们需要区分 <strong>“通用 VQA”</strong> 和 <strong>“领域特定 VQA (Domain-Specific VQA)”</strong> 这两个概念。</p> <p>作者的逻辑可能是这样的：</p> <h4 id="1-第一层验证通用能力-vs-具身能力-fig-3-的结论"><strong>1. 第一层验证：通用能力 vs. 具身能力 (Fig 3 的结论)</strong></h4> <ul> <li> <strong>对象：</strong> MMBench, Math, Coding, General Captioning。</li> <li> <strong>发现：</strong> 你懂数学、懂代码、懂识别蒙娜丽莎（通用 VQA），这对机器人拧螺丝没帮助。</li> <li> <strong>潜台词：</strong> “内容”不对口。虽然大家都是 VQA 形式，但你考的是“历史地理”，我要的是“物理体育”。</li> </ul> <h4 id="2-第二层验证如果内容对口了呢-sec-42-的动机"><strong>2. 第二层验证：如果内容对口了呢？ (Sec 4.2 的动机)</strong></h4> <ul> <li> <strong>假设：</strong> 既然“内容”不对口是问题，那我就把内容换成对口的！</li> <li> <strong>操作：</strong> 我让 VQA 的内容变成“深度估计”、“坐标点选”、“机器人规划”。这些都是机器人急需的知识。</li> <li> <strong>核心问题：</strong> 如果<strong>内容</strong>是对口的（都是机器人知识），但<strong>形式</strong>依然是 VQA（文本问答），那还有用吗？</li> <li> <strong>实验目的：</strong> 这正是 Sec 4.2 想要探究的——<strong>是“知识领域”的问题，还是“模态形式”的问题？</strong> </li> </ul> <h3 id="结论的递进">结论的递进</h3> <p>通过这两步实验，论文实际上完成了一个<strong>双重否定</strong>的逻辑闭环，这反而增强了论文的深度：</p> <ol> <li> <strong>Step 1 (Fig 3):</strong> 通用知识（General Knowledge）没用。 $\rightarrow$ <strong>结论：知识领域要对口。</strong> </li> <li> <strong>Step 2 (Sec 4.2):</strong> 即使知识领域对口了（用了 Robopoint/Depth VQA），如果还是用 VQA 这种文本形式（Text Form）来训练，依然没用！ $\rightarrow$ <strong>结论：模态形式也要对口（不能只用文本 Token）。</strong> </li> <li> <strong>Step 3 (Sec 4.4):</strong> 只有解冻视觉编码器，直接注入控制信号（破坏原有的文本对齐），才有用。 $\rightarrow$ <strong>终极结论：必须深入到特征层 (Feature Level) 进行改造。</strong> </li> </ol> <h3 id="你的质疑为何依然有价值">你的质疑为何依然有价值？</h3> <p>尽管有上述解释，你的质疑依然非常有力，因为：</p> <p><strong>如果在做 Sec 4.2 之前，作者已经有了深刻的洞察（Insight），他们本应该预见到 VQA 形式的局限性。</strong></p> <p>这也反映了当前 AI 社区的一种<strong>路径依赖 (Path Dependence)</strong>：大家太习惯于“把所有东西都 Token 化，扔进 LLM 里微调”。</p> <ul> <li>VLM4VLA 的这部分实验，某种程度上像是一次<strong>“撞南墙”的实证记录</strong>。</li> <li>它告诉社区：<strong>“看，我们试过了，想偷懒用 VQA 统一接口来做具身增强，这条路走不通。别再试了，去改特征吧。”</strong> </li> </ul> <blockquote> <p>“作者之所以明知 VQA 没用还要试，是为了<strong>控制变量</strong>，彻底排除‘是因为训练数据内容无关’这一借口，从而最终锁定‘VQA/文本模态本身不适合表达精确控制’这一根本性结论。”</p> </blockquote> <h2 id="some-thoughts">some thoughts</h2> <p>我觉得 串行模式也极有可能是导致为什么辅助任务无效的原因. 因为会有灾难性遗忘, 以及特征空间drift的问题. 或许采用Co-training,或者multi-task的方式,对于VLA的任务应该会有所帮助.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/DriveJEPA/">DriveJEPA</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/C_RADIOv4/">C_RADIOv4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/GeRo/">GeRo</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 P W Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>