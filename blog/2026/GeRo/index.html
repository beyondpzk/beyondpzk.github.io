<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> GeRo | Tenacious life, proud journey. </title> <meta name="author" content="P W Name"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://beyondpzk.github.io/blog/2026/GeRo/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Tenacious life, proud journey. </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">GeRo</h1> <p class="post-meta"> Created on January 16, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>[TOC]</p> <h1 id="gero-generative-scenario-rollouts-for-end-to-end-autonomous-driving">GeRo: Generative Scenario Rollouts for End-to-End Autonomous Driving</h1> <p><a href="https://arxiv.org/abs/2601.11475" rel="external nofollow noopener" target="_blank">paper link</a></p> <h3 id="takeaways-for-me">takeaways for me:</h3> <ol> <li>rollouts &amp; GRPO</li> <li>planning head. (猜测CVAE), 这和之前看到的用mlp,或者drive policy不一样. 有独到之处.</li> </ol> <p>要剖析的这篇论文是 <strong>“Generative Scenario Rollouts for End-to-End Autonomous Driving” (简称 GeRo)</strong>。这篇工作由高通 AI 研究院（Qualcomm AI Research）提出，它代表了当前自动驾驶领域的一个重要趋势：将视觉-语言-动作（VLA, Vision-Language-Action）模型从单纯的“模仿学习者”进化为具备“生成式推演能力”的智能体。</p> <p>我们将从背景挑战、GeRo 核心框架、生成式场景推演（Scenario Rollouts）、基于 GRPO 的强化学习优化以及实验分析五个维度进行深入拆解。</p> <hr> <h3 id="第一部分背景与动机-introduction--motivation">第一部分：背景与动机 (Introduction &amp; Motivation)</h3> <h4 id="11-端到端自动驾驶的演进">1.1 端到端自动驾驶的演进</h4> <p>在深入 GeRo 之前，我们需要理解当前的语境。传统的自动驾驶系统通常采用模块化设计（感知、预测、规划分离），而近年来的趋势是转向端到端（End-to-End）的学习范式，直接从原始传感器输入映射到车辆控制信号或轨迹。</p> <p>而在端到端范式中，<strong>VLA（Vision-Language-Action）模型</strong>正在崛起。这类模型利用大语言模型（LLM）的推理能力，将语言上下文整合到运动规划中，试图实现更具解释性和逻辑性的驾驶决策。 <alphaxiv-paper-citation title="VLA Context" page="1" first="Vision-Language-Action (VLA) models" last="autonomous driving systems."></alphaxiv-paper-citation></p> <h4 id="12-现有-vla-模型的局限性">1.2 现有 VLA 模型的局限性</h4> <p>尽管 VLA 模型引入了语言推理，但作者敏锐地指出了当前方法的四个核心痛点：</p> <ol> <li> <p><strong>稀疏的语言-动作监督 (Sparse language-action supervision)</strong>： 现有的数据集往往只提供场景级别的描述，缺乏与时间轴紧密结合的细粒度动作描述。例如，模型可能难以区分“变道”和“超车”在时序上的细微差别。 <alphaxiv-paper-citation title="Sparse Supervision" page="1" first="Sparse language-action supervision:" last="multi-step maneuvers [10]."></alphaxiv-paper-citation></p> </li> <li> <p><strong>生成能力的未充分利用 (Under-utilized generative capability)</strong>： 目前的 VLA 方法大多仅依赖模仿学习（Imitation Learning）来拟合专家轨迹，忽略了模型本身作为生成式模型（Generative Model）进行自回归推演和探索的潜力。 <alphaxiv-paper-citation title="Generative Potential" page="1" first="Under-utilized generative capability:" last="reasoning and exploration."></alphaxiv-paper-citation></p> </li> <li> <p><strong>描述性语言 vs. 程序性语言</strong>： 目前的语言监督多是描述“正在发生什么”（Descriptive），而不是指导“如何执行”（Procedural），这限制了规划层面的逻辑深度。</p> </li> <li> <p><strong>语言-动作错位 (Language-action misalignment)</strong>： 由于很多数据集的指令是事后生成的，往往会出现视觉输入与语言指令不匹配的情况（例如视觉上是红灯，但语言指令却是加速），导致模型产生幻觉。 <alphaxiv-paper-citation title="Misalignment" page="2" first="Language-action misalignment:" last="paired with acceleration."></alphaxiv-paper-citation></p> </li> </ol> <p><strong>GeRo 的核心思想</strong>：提出一个即插即用（Plug-and-Play）的框架，不仅利用 VLA 做规划，更利用其进行<strong>生成式场景推演（Generative Scenario Rollouts）</strong>，即在潜空间（Latent Space）中自回归地生成未来的交通场景和自我意图，并利用强化学习来对齐语言和动作。</p> <hr> <h3 id="第二部分gero-模型架构与预训练-architecture--pretraining">第二部分：GeRo 模型架构与预训练 (Architecture &amp; Pretraining)</h3> <p>GeRo 的架构设计是为了让 VLA 模型能够理解并生成动态的驾驶场景。整个流程分为两个阶段：预训练（Pretraining）和生成式推演（Generative Scenario Rollout）。</p> <h4 id="21-基础架构-backbone">2.1 基础架构 (Backbone)</h4> <p>GeRo 构建在多模态大语言模型之上（例如 Qwen2.5-VL）。其核心组件包括：</p> <ul> <li> <strong>视觉编码器 (Vision Encoder)</strong>：使用预训练的 ViT（如 EVA-ViT）将多视角图像转化为视觉 Token。</li> <li> <strong>文本 Tokenizer</strong>：处理场景描述和问答文本。</li> <li> <strong>LLM 核心</strong>：作为中枢大脑，将多模态 Token 投影到共享的潜空间（Shared Token Space）。</li> <li> <strong>输出头 (Output Heads)</strong>： <ul> <li> <strong>生成式规划头 (Generative Planning Head)</strong>：通常是一个变分自编码器（VAE），用于解码自车轨迹。</li> <li> <strong>运动预测头 (Motion Prediction Head)</strong>：用于预测周围交通参与者（Agents）的未来轨迹和边界框。 <alphaxiv-paper-citation title="Architecture Details" page="7" first="GeRo leverages the" last="motion prediction heads."></alphaxiv-paper-citation> </li> </ul> </li> </ul> <h4 id="22-第一阶段预训练-pretraining">2.2 第一阶段：预训练 (Pretraining)</h4> <p>这一阶段的目标是学习一个紧凑且语义丰富的<strong>潜空间表示 (Latent Tokenization)</strong>。模型需要将自车（Ego）和他车（Agents）的动力学特征编码为潜变量 Token。</p> <p>在预训练中，模型同时接受三个任务的监督：</p> <ol> <li> <strong>规划任务 (Planning Task)</strong>：预测自车的未来轨迹。</li> <li> <strong>运动预测任务 (Motion Prediction Task)</strong>：预测场景中其他 Agent 的轨迹和 3D 边界框。这对于理解场景动态至关重要。</li> <li> <strong>视觉问答任务 (VQA Task)</strong>：生成场景描述并回答关于自车行为的问题。</li> </ol> <p><strong>数学表达</strong>： 预训练的损失函数 $L_{pre}$ 由三部分组成： \(L_{pre} = L_{plan} + L_{mot} + L_{VLA}\)</p> <p>其中，$L_{plan}$ 使用 L1 损失回归航点；$L_{mot}$ 结合了分类损失（Focal Loss）和回归损失（L1 Loss 用于轨迹和 BBox）；$L_{VLA}$ 是标准的语言交叉熵损失。 <alphaxiv-paper-citation title="Pretraining Loss" page="3" first="The planning head" last="Lplan + Lmot + LVLA."></alphaxiv-paper-citation></p> <p>这一步的关键在于“绑定”：将语言表征与行为表征在潜空间中强耦合，为后续的文本对齐生成打下基础。 <alphaxiv-paper-citation title="Pretraining Goal" page="2" first="This stage is" last="language-action misalignment."></alphaxiv-paper-citation></p> <hr> <h3 id="第三部分生成式场景推演-generative-scenario-rollouts">第三部分：生成式场景推演 (Generative Scenario Rollouts)</h3> <p>这是 GeRo 最核心的创新点。模型不仅仅是一次性输出轨迹，而是像在大脑中“预演”未来一样，进行自回归的生成。</p> <h4 id="31-推演机制-rollout-mechanism">3.1 推演机制 (Rollout Mechanism)</h4> <p>给定当前时刻 $t$ 的多视角图像、场景描述 $s$ 和关于自车行为的问题 $q$，GeRo 执行以下步骤：</p> <ol> <li> <strong>编码</strong>：计算当前的自车潜变量 Token $z_e^t$ 和他车潜变量 Token ${z_{a_i}^t}$。</li> <li> <strong>自回归生成</strong>：利用 LLM 预测<strong>下一时刻</strong> $t+1$ 的潜在 Token ${\tilde{z}^{t+1}}$ 以及针对当前问题的文本回答。</li> <li> <strong>解码</strong>：将预测出的 Token 解码为具体的自车轨迹、他车轨迹。</li> <li> <strong>循环</strong>：将预测出的 $t+1$ 时刻的 Token 作为新的输入，配合更新后的场景描述，继续预测 $t+2$ 时刻，如此循环 $T$ 步。 <alphaxiv-paper-citation title="Rollout Process" page="3" first="Given latent tokens" last="ego-action questions."></alphaxiv-paper-citation> </li> </ol> <h4 id="32-为什么需要推演">3.2 为什么需要推演？</h4> <p>这种机制允许模型进行<strong>长时程推理 (Long-horizon Reasoning)</strong>。传统的 VLA 往往只关注单步输出，而 GeRo 通过 Rollout 强制模型思考：“如果我这样做，环境会变成什么样？接下来的动作是什么？”</p> <h4 id="33-推演一致性损失-rollout-consistency-loss">3.3 推演一致性损失 (Rollout-Consistency Loss)</h4> <p>为了防止自回归过程中的误差累积（Drift），作者引入了推演一致性损失。</p> <ul> <li> <strong>潜空间对齐</strong>：使用 KL 散度（KL-Divergence），强制推演生成的潜变量分布与预训练模型（Teacher）在未来时刻生成的潜变量分布保持一致。</li> <li> <strong>轨迹监督</strong>：如果有 Ground Truth，则直接监督生成的轨迹。</li> </ul> <p>这种设计使得 GeRo 能够生成在时间上连贯（Temporally Consistent）且以语言为基础（Language-Grounded）的推演序列。 <alphaxiv-paper-citation title="Consistency Loss" page="2" first="Predictions are stabilized" last="using KL-Divergence."></alphaxiv-paper-citation></p> <hr> <h3 id="第四部分基于-grpo-的强化学习-rl-with-grpo">第四部分：基于 GRPO 的强化学习 (RL with GRPO)</h3> <p>仅仅依靠模仿学习（Imitation Learning）是不够的，因为存在协变量偏移（Covariate Shift）问题，且模仿学习难以处理长尾场景。GeRo 引入了强化学习来进一步优化推演过程。</p> <h4 id="41-grpo-算法">4.1 GRPO 算法</h4> <p>GeRo 采用了 <strong>GRPO (Group Relative Policy Optimization)</strong> 算法。与 PPO 需要额外的价值网络（Value Network）不同，GRPO 通过对一组输出进行采样并计算其相对优势，从而更加高效且稳定，特别适合大语言模型的微调。 <alphaxiv-paper-citation title="RL Strategy" page="2" first="Therefore, we introduce" last="scenario rollouts."></alphaxiv-paper-citation></p> <h4 id="42-奖励函数设计-reward-engineering">4.2 奖励函数设计 (Reward Engineering)</h4> <p>为了兼顾驾驶安全性和语义一致性，作者设计了一套新颖的奖励函数：</p> <ol> <li> <strong>安全性奖励</strong>：包含碰撞避免（Collision Avoidance）和碰撞时间（Time-to-Collision, TTC）。这是硬约束，确保规划出的轨迹是物理安全的。</li> <li> <strong>语言对齐奖励</strong>：利用问答对（Q&amp;A）作为反馈。模型生成的文本解释必须与实际生成的轨迹相匹配。这增强了模型的可解释性（Interpretability）。 <alphaxiv-paper-citation title="Reward Functions" page="2" first="We propose a" last="time-to-collision."></alphaxiv-paper-citation> </li> </ol> <p>通过 RL，模型学会了在生成的幻境中“试错”，并根据安全和逻辑的反馈来调整其策略。</p> <hr> <h3 id="第五部分实验结果与讨论-experiments--discussion">第五部分：实验结果与讨论 (Experiments &amp; Discussion)</h3> <h4 id="51-闭环测试-closed-loop-evaluation">5.1 闭环测试 (Closed-Loop Evaluation)</h4> <p>在 Bench2Drive 榜单上，GeRo 展现了显著的提升。</p> <ul> <li> <strong>基线对比</strong>：相比于基础的 Qwen2.5-VL 模型，GeRo (Qwen) 将驾驶得分（Driving Score）提升了 <strong>+15.7</strong>，成功率（Success Rate）提升了 <strong>+26.2%</strong>。</li> <li> <strong>SOTA 对比</strong>：即便是对比强大的 ORION 模型，GeRo 也能带来明显的增益。 <alphaxiv-paper-citation title="Closed-Loop Results" page="7" first="On Bench2Drive," last="respectively."></alphaxiv-paper-citation> </li> </ul> <h4 id="52-开环测试与零样本泛化-open-loop--zero-shot">5.2 开环测试与零样本泛化 (Open-Loop &amp; Zero-Shot)</h4> <p>在 nuScenes 数据集上，GeRo 展示了极强的泛化能力。</p> <ul> <li> <strong>轨迹误差</strong>：相比基线，L2 轨迹误差降低了约 60-70%。</li> <li> <strong>零样本能力</strong>：即便是在 Bench2Drive 上训练，直接在 nuScenes 上测试（Zero-shot），GeRo 依然保持了极低的碰撞率，这证明了生成式推演学到了通用的驾驶逻辑，而非死记硬背训练集。 <alphaxiv-paper-citation title="Zero-Shot Results" page="8" first="In the zero-shot" last="scenario-grounded rollouts."></alphaxiv-paper-citation> </li> </ul> <h4 id="53-案例分析-qualitative-analysis">5.3 案例分析 (Qualitative Analysis)</h4> <p>论文展示了定性结果（图 4）。在复杂的路口交互、恶劣天气下的事故处理中，GeRo 不仅能生成安全的轨迹，还能生成与动作高度对齐的文本解释（例如：“减速让行，因为检测到行人”）。这种<strong>言行一致</strong>是传统黑盒模型无法做到的。 <alphaxiv-paper-citation title="Qualitative Examples" page="8" first="Each frame includes" last="safety-aware decision."></alphaxiv-paper-citation></p> <h4 id="54-消融实验-ablation-study">5.4 消融实验 (Ablation Study)</h4> <p>通过消融实验（表 4），作者证明了每一个组件的重要性：</p> <ul> <li>加入 <strong>Scenario Description</strong> 和 <strong>VQA</strong> 提升了基础性能。</li> <li>加入 <strong>Rollout Consistency Loss</strong> 显著提升了推演的稳定性。</li> <li>加入 <strong>GRPO (RL)</strong> 进一步大幅提升了长尾场景下的成功率。 <alphaxiv-paper-citation title="Ablation Analysis" page="8" first="Using collision and" last="scenario rollouts."></alphaxiv-paper-citation> </li> </ul> <hr> <h3 id="总结与思考-conclusion">总结与思考 (Conclusion)</h3> <p>GeRo 的成功向我们展示了自动驾驶研究的一个新范式：<strong>Thinking Fast and Slow</strong>。</p> <ul> <li>传统的端到端模型像是 “Thinking Fast”（直觉反应）。</li> <li>GeRo 引入的生成式推演则像是 “Thinking Slow”（逻辑推理与预演）。</li> </ul> <p>通过将 VLA 模型与自回归生成、强化学习相结合，GeRo 不仅提高了驾驶的安全性，更重要的是赋予了自动驾驶系统<strong>可解释的推理能力</strong>。</p> <p><strong>思考</strong>：</p> <ol> <li>GeRo 中的 Rollout 机制会增加推理延迟（Latency），在实际车端部署时应如何平衡生成深度与实时性？</li> <li>GRPO 的奖励函数目前主要关注安全，如何设计更复杂的奖励函数来体现“舒适性”或“礼貌驾驶”？</li> </ol> <h2 id="论文中figure-2详细解读">论文中Figure 2详细解读</h2> <h3 id="第一部分左侧多任务预训练与潜空间构建-stage-1-multi-task-pretraining">第一部分：左侧——多任务预训练与潜空间构建 (Stage 1: Multi-Task Pretraining)</h3> <p>Figure 2 的左半部分。这一阶段的核心目标是<strong>“压缩与对齐”</strong>。模型需要将高维的图像信息和复杂的文本信息，压缩成紧凑的数学向量（Latent Tokens），并让这些向量同时包含视觉语义和运动物理规律。</p> <p><strong>数据流向示例：</strong> 假设我们的自车正行驶在一个繁忙的十字路口，准备左转。</p> <ol> <li> <strong>输入层</strong>：模型接收两类输入。一是<strong>多视角图像 ($I_t$)</strong>，比如前视摄像头拍到了红绿灯变绿，侧视摄像头拍到了斑马线上有一个正在过马路的行人；二是<strong>文本提示 ($P_{text}$)</strong>，比如“当前任务：在路口左转”。</li> <li> <strong>编码层</strong>：图像经过 Vision Encoder（如 EVA-ViT）处理，文本经过 Text Encoder 处理。</li> <li> <strong>LLM 处理与 Tokenization</strong>：这些特征进入 LLM 主干网络。关键点来了，模型将整个场景的动态——包括自车的状态和他车（那个行人）的状态——编码为一组<strong>潜变量 Token ($z_t$)</strong>。</li> <li> <strong>多任务输出头</strong>：为了确保这些 $z_t$ Token 真的听懂了物理规律，Figure 2 展示了三个监督信号： <ul> <li> <strong>Motion Head (运动头)</strong>：利用 $z_t$ 预测行人的未来。比如，它预测行人下一秒会移动到 $(x=12.5, y=5.0)$ 的位置。</li> <li> <strong>Planning Head (规划头)</strong>：利用 $z_t$ 预测自车的轨迹。比如，预测自车未来 3 秒的轨迹是一条平滑的左转曲线，且在第 1 秒时速度从 30km/h 降至 10km/h（为了避让行人）。</li> <li> <strong>VQA Head (问答头)</strong>：利用 $z_t$ 回答文本问题。当被问到“为什么要减速？”时，输出“因为检测到行人正在横穿马路”。</li> </ul> </li> </ol> <p>通过这一阶段，模型学会了将“图像像素”转化为“物理概念”和“语言逻辑”。 <alphaxiv-paper-citation title="Pretraining Structure" page="3" first="Specifically, the VLA" last="a shared latent space."></alphaxiv-paper-citation></p> <h3 id="第二部分右侧生成式场景推演-stage-2-generative-scenario-rollouts">第二部分：右侧——生成式场景推演 (Stage 2: Generative Scenario Rollouts)</h3> <p>现在我们将视线移到 Figure 2 的右半部分，这是 GeRo 的灵魂所在。这一阶段不再是简单的单步预测，而是<strong>时间维度上的自回归生成</strong>。模型开始像人类一样在大脑中“预演”未来的发展。</p> <p><strong>推演过程示例：</strong> 继续刚才的路口场景。现在时间是 $t=0$。</p> <ol> <li> <strong>启动推演 ($t \to t+1$)</strong>： 模型拿着 $t=0$ 时刻的潜变量 $z_0$（包含了当前行人和车的状态）以及一个动作问题 $q$（例如“接下来该怎么做？”），输入到 LLM 中。 LLM <strong>不是</strong>直接输出轨迹，而是<strong>生成</strong>了下一时刻 $t=1$ 的潜变量 Token $\tilde{z}_1$ 和一段文本解释。 <ul> <li> <strong>生成的文本</strong>：“我应该减速并观察行人是否通过。”</li> <li> <strong>生成的 Token $\tilde{z}_1$</strong>：这个隐向量代表了模型“想象”出的下一秒世界状态。</li> </ul> </li> <li> <p><strong>解码与验证</strong>： 我们把生成的 $\tilde{z}_1$ 扔给 Planning Head 和 Motion Head 解码，发现解码出的自车位置前进了 2 米（减速了），而行人的位置向路中间移动了 1 米。这与物理规律是自洽的。</p> </li> <li> <strong>持续推演 ($t+1 \to t+2$)</strong>： 模型将刚刚生成的 $\tilde{z}_1$ 作为新的历史记忆，继续询问自己：“然后呢？” LLM 再次生成 $t=2$ 的 Token $\tilde{z}_2$ 和文本：“行人已通过车道，可以开始加速。” 此时解码出的轨迹显示自车速度回升到 20km/h，轨迹开始大幅度左转。</li> </ol> <p>Figure 2 中展示的循环箭头正是这个<strong>自回归（Autoregressive）</strong>过程。它强调了模型是在<strong>潜空间（Latent Space）</strong>中进行推理，而不是在像素空间，这大大降低了计算量并提高了语义一致性。 <alphaxiv-paper-citation title="Rollout Mechanism" page="3" first="Next, GeRo performs" last="long-horizon rollouts."></alphaxiv-paper-citation></p> <h3 id="第三部分一致性损失与强化学习-rollout-consistency--grpo">第三部分：一致性损失与强化学习 (Rollout-Consistency &amp; GRPO)</h3> <p>在 Figure 2 的右下角，你会看到几个关键的损失函数标记，它们是保证推演质量的“监工”。</p> <p><strong>1. 推演一致性损失 ($L_{roll}$)</strong>： 模型在“想象”未来时容易跑偏（Drift）。比如推演到第 3 秒，模型可能幻想行人突然消失了。为了防止这种情况，Figure 2 展示了通过 KL 散度（KL-Divergence）来约束生成的分布。简单来说，就是强制要求模型“想象”出的未来 $\tilde{z}_{t+k}$，必须和如果我们真的把车开到那个时刻看到的真实状态（或教师模型的预测）尽可能一致。这确保了推演的<strong>物理真实性</strong>。</p> <p><strong>2. GRPO (Group Relative Policy Optimization)</strong>： 这是 Figure 2 中提到的强化学习部分。模型可能会生成多种可能的未来（比如“激进抢行”和“保守礼让”）。GRPO 算法会根据奖励函数（Reward）对这些生成的剧本进行打分：</p> <ul> <li>如果模型选择了“抢行”导致碰撞，<strong>Collision Reward</strong> 会给出极低的负分。</li> <li>如果模型选择了“礼让”且文本解释合理，<strong>Text Alignment Reward</strong> 会给出高分。 通过这种机制，Figure 2 展示了模型如何通过自我博弈和反馈，逐渐学会选择最安全、最符合人类逻辑的驾驶策略。 <alphaxiv-paper-citation title="RL Optimization" page="2" first="By integrating reinforcement" last="open-loop performance."></alphaxiv-paper-citation> </li> </ul> <h3 id="other">other</h3> <ol> <li>为了给grpo使用,会进行多次的rollouts， 产生多种可能的未来.</li> <li>ego token, agent tokens 都是learnable tokens, 或者认为是query, ego token有一个, agent tokens有N个, QA的answer在最后面.</li> </ol> <h2 id="planning-head的结构是一个cvae">planning head的结构是一个CVAE</h2> <p>在 <strong>3.2 Architecture</strong> 和 <strong>3.3 Pretraining</strong> 章节中，作者明确提到了这一点。这种设计不是随意的，而是为了解决自动驾驶中一个核心问题：<strong>多模态性 (Multi-modality)</strong> 和 <strong>生成能力的增强</strong>。</p> <p>这个 VAE Planning Head 的结构和工作原理。</p> <h3 id="1-为什么用-vaewhy-vae">1. 为什么用 VAE？(Why VAE?)</h3> <p>如果仅仅是用一个 MLP（多层感知机）去回归轨迹（Regression），模型往往会倾向于预测一条“平均轨迹”（Mean Trajectory）。</p> <ul> <li> <em>例子</em>：前方有障碍物，左边能绕，右边也能绕。</li> <li> <em>MLP 结果</em>：输出一条撞向障碍物的中间轨迹（因为左+右的平均值在中间）。</li> <li> <em>VAE 结果</em>：学习一个潜在分布（Latent Distribution），可以采样出“左绕”或“右绕”两条截然不同的轨迹。</li> </ul> <h3 id="2-planning-head-的具体结构">2. Planning Head 的具体结构</h3> <p>虽然论文没有画出详细的 Head 内部图，但根据其描述和标准 VAE 结构，我们可以推断出如下架构：</p> <h4 id="a-输入端-input">A. 输入端 (Input)</h4> <ul> <li> <strong>Conditioning (条件)</strong>：来自 LLM 的 <strong>Ego Token ($z_e$)</strong>。这个 $z_e$ 包含了当前环境的上下文信息（Context）。</li> <li> <strong>Target (训练时)</strong>：真实的未来轨迹 $\tau_{gt}$（Ground Truth Trajectory）。</li> </ul> <h4 id="b-编码器-encoder---仅训练时使用">B. 编码器 (Encoder - 仅训练时使用)</h4> <ul> <li> <strong>结构</strong>：一个神经网络（通常是 MLP 或简单的 Transformer Encoder）。</li> <li> <strong>输入</strong>：$z_e$ (条件) + $\tau_{gt}$ (目标)。</li> <li> <strong>输出</strong>：预测潜在变量的分布参数——均值 $\mu$ 和方差 $\sigma$。 \(q_\phi(z_{plan} | z_e, \tau_{gt}) = \mathcal{N}(\mu, \sigma^2)\)</li> <li> <strong>作用</strong>：将“当前情况 $z_e$ 下应该走的轨迹 $\tau_{gt}$”编码为一个高斯分布。</li> </ul> <h4 id="c-潜在空间采样-latent-sampling">C. 潜在空间采样 (Latent Sampling)</h4> <ul> <li>利用重参数化技巧 (Reparameterization Trick)： \(z_{sample} = \mu + \epsilon \cdot \sigma, \quad \epsilon \sim \mathcal{N}(0, I)\) 这个 $z_{sample}$ 就是具体的“意图向量”。</li> </ul> <h4 id="d-解码器-decoder---训练推理都用-我理解即planning-head">D. 解码器 (Decoder - 训练/推理都用, 我理解即planning head)</h4> <ul> <li> <strong>结构</strong>：另一个神经网络（MLP / GRU / Transformer Decoder）。</li> <li> <strong>输入</strong>：$z_e$ (条件) + $z_{sample}$ (意图)。</li> <li> <strong>输出</strong>：重构的轨迹 $\hat{\tau}$。 \(\hat{\tau} = \text{Decoder}(z_e, z_{sample})\)</li> <li> <strong>推理时 (Inference)</strong>：直接从标准正态分布 $\mathcal{N}(0, I)$ 中采样 $z_{sample}$，或者直接取均值（如果只想最可能的轨迹），结合 $z_e$ 输入 Decoder 生成轨迹。</li> </ul> <h3 id="3-loss-function-损失函数">3. Loss Function (损失函数)</h3> <p>VAE 的训练包括两部分损失：</p> <ol> <li> <strong>重构损失 (Reconstruction Loss)</strong>： \(L_{recon} = || \hat{\tau} - \tau_{gt} ||_1\) 希望解码出来的轨迹和真实轨迹越像越好。</li> <li> <strong>KL 散度 (KL Divergence)</strong>： \(L_{KL} = D_{KL}(q_\phi(z_{plan}|...) || \mathcal{N}(0, I))\) 强制让 Encoder 学到的分布接近标准正态分布，防止过拟合，保证采样的多样性。</li> </ol> <p>在 GeRo 中，这个 VAE Head 作为一个整体被集成在 LLM 的输出端。</p> <h3 id="小结">小结</h3> <p>GeRo 的 Planning Head 不是简单的回归器，而是一个<strong>条件 VAE (CVAE)</strong>。</p> <ul> <li> <strong>输入</strong>：LLM 提取的 Ego Token ($z_e$)。</li> <li> <strong>核心</strong>：引入随机变量 $z_{plan}$。</li> <li> <strong>输出</strong>：多样化的未来轨迹分布。</li> </ul> <p>这种结构是 GeRo 能够进行 <strong>Generative Scenario Rollouts</strong> 的基石——它允许模型在遇到不确定路况时，不仅能给出一个“最佳答案”，还能探索“多种可能性”。</p> <h2 id="未来推演的多样性来源">未来推演的多样性来源</h2> <p>GeRo 中<strong>多样性（Diversity）的双重来源</strong>。</p> <p><strong>在单步轨迹生成中，VAE 确实负责多样性；但在长时程推演（Rollout）中，LLM 的 Top-K 才是主角。</strong></p> <p>GeRo 的多样性其实分两个层次：<strong>“微观动作多样性”</strong> 和 <strong>“宏观剧本多样性”</strong>。</p> <h3 id="1-微观动作多样性由-vae-负责-spatial-diversity">1. 微观动作多样性：由 VAE 负责 (Spatial Diversity)</h3> <p>这是指在<strong>同一个时间步 $t$</strong> 内，面对相同的情境 $z_e^t$，车辆具体的<strong>轨迹几何形状</strong>可能有多种微小的变化。</p> <ul> <li> <strong>场景</strong>：前面有个水坑，你是从左边绕一点，还是从右边绕一点？或者仅仅是车速快一点慢一点？</li> <li> <strong>机制</strong>：<strong>VAE Planning Head</strong>。 <ul> <li>输入：固定的 Ego Token $z_e^t$（来自 LLM）。</li> <li>采样：从 VAE 的隐空间采样不同的 $\epsilon$。</li> <li>输出：$\tau_{left}, \tau_{right}$（几何上的不同轨迹）。</li> </ul> </li> <li> <strong>作用</strong>：主要处理<strong>连续控制空间</strong>的不确定性，保证轨迹的平滑和多模态。</li> </ul> <h3 id="2-宏观剧本多样性由-llm-top-k-负责-temporalsemantic-diversity">2. 宏观剧本多样性：由 LLM Top-K 负责 (Temporal/Semantic Diversity)</h3> <p>这是指在<strong>时间轴 $t \to t+1 \to t+2$</strong> 的推演过程中，整个<strong>场景发展方向</strong>的分歧。</p> <ul> <li> <strong>场景</strong>：黄灯亮了。 <ul> <li> <strong>剧本 A</strong>：加速冲过去 $\to$ 下一秒已经在路口中间 $\to$ 再下一秒过了路口。</li> <li> <strong>剧本 B</strong>：急刹车 $\to$ 下一秒停在停车线前 $\to$ 再下一秒静止不动。</li> </ul> </li> <li> <strong>机制</strong>：<strong>LLM Autoregressive Generation</strong>。 <ul> <li>输入：历史 Context。</li> <li>采样：<strong>Top-K Sampling</strong> 预测下一个 <strong>Latent Token $z_e^{t+1}$</strong>。</li> <li>输出：$z_{accel}$ (代表加速意图的 Token) 或 $z_{brake}$ (代表刹车意图的 Token)。</li> </ul> </li> <li> <strong>作用</strong>：决定了<strong>离散语义空间</strong>的走向。这是 GeRo “Scenario Rollout” 的核心。</li> </ul> <h3 id="3-为什么论文强调-llm-的-top-k">3. 为什么论文强调 LLM 的 Top-K？</h3> <p>因为 <strong>GeRo 的核心贡献是“Scenario Rollout”（场景推演）</strong>。</p> <ul> <li>如果只是 VAE 多样性，那是传统的 CVAE-Planner 就能做的（比如 VAD, UniAD 也有类似机制）。</li> <li>GeRo 的突破在于：它能<strong>推演未来</strong>。 <ul> <li>它不是说“我现在可能怎么走”（VAE）。</li> <li>它是说“如果我现在决定加速（由 LLM 采样决定），那么<strong>下一秒的世界</strong>（Agent Tokens $z_a^{t+1}$ 和 Ego Token $z_e^{t+1}$）会变成什么样”。</li> </ul> </li> </ul> <p><strong>LLM 的采样决定了“剧情分支”，而 VAE 只是负责把这个剧情“画”成具体的轨迹线。</strong></p> <h3 id="总结">总结</h3> <p>我们可以把 GeRo 比作一个<strong>导演（LLM）</strong>和一个<strong>动作指导（VAE）</strong>：</p> <ol> <li> <strong>导演（LLM）说</strong>：“这场戏，主角要<strong>激进地超车</strong>！”（这是通过 <strong>Top-K 采样</strong> 选定的剧本方向）。</li> <li> <strong>动作指导（VAE）执行</strong>：“好，既然要超车，具体的路线是<strong>向左打方向盘 30 度</strong>。”（这是通过 <strong>VAE 采样</strong> 生成的具体轨迹）。</li> </ol> <p>如果导演选了另一个剧本：“主角要<strong>保守跟车</strong>”，那么动作指导就会生成完全不同的轨迹。</p> <p>所以，<strong>“多种可能的未来”主要由 LLM 的采样（决定意图和状态流转）主导，VAE 负责将这些意图落实为具体的物理轨迹。</strong> <alphaxiv-paper-citation title="Two-stage Generation" page="3" first="Next, GeRo performs" last="long-horizon rollouts."></alphaxiv-paper-citation></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/DriveJEPA/">DriveJEPA</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/C_RADIOv4/">C_RADIOv4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/VLM4VLA/">VLM4VLA</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 P W Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>