<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pi0 | Tenacious life, proud journey. </title> <meta name="author" content="P W Name"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://beyondpzk.github.io/blog/2024/Pi0/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Tenacious life, proud journey. </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Pi0</h1> <p class="post-meta"> Created on October 31, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/category/vla"> <i class="fa-solid fa-tag fa-sm"></i> VLA</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>[TOC]</p> <h1 id="pi_0-a-vision-language-action-flow-model-for-general-robot-control">$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control</h1> <p><a href="https://arxiv.org/abs/2410.24164v1" rel="external nofollow noopener" target="_blank">paper link</a></p> <p>Physical Intelligence 公司最近发表的一篇重要论文，题为 <strong>“$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control”</strong>（$\pi_0$：一种用于通用机器人控制的视觉-语言-动作流模型）。这项工作代表了所谓的 <em>通用机器人策略（generalist robot policies）</em> 演进中的关键一步——即系统 <strong>从单一任务的专用化向广泛的多具身（multi-embodiment）能力</strong> 转变。</p> <hr> <h3 id="范式转变从专用化到通用化"><strong>范式转变——从专用化到通用化</strong></h3> <p>首先，让我们来梳理一下问题的背景。几十年来，机器人学家在构建专用系统方面表现出色。我们可以制造出以亚毫米级精度焊接汽车或跑酷的机器人。然而，这些系统非常脆弱；改变光照、将物体移动两英寸，或者引入一条新指令，它们就会失效。这篇论文的作者引用了罗伯特·海因莱因（Robert Heinlein）的一句话作为开篇：<em>“分工是昆虫的事（Specialization is for insects）。”</em> 这为整项工作奠定了哲学基调：人类智能的定义不在于某项任务的巅峰表现，而在于跨越多种任务的通用性。<alphaxiv-paper-citation title="Philosophy" page="2" first="Specialization is for" last="is for insects."></alphaxiv-paper-citation></p> <p>$\pi_0$ 的核心论点是，要构建一个通用的机器人大脑，我们不能仅仅依赖机器人数据，因为这些数据稀缺且昂贵。相反，我们需要利用预训练的视觉-语言模型（VLMs）中蕴含的海量语义知识，这些模型是在互联网规模的数据上训练出来的。作者提出了一种 <strong>“视觉-语言-动作”（Vision-Language-Action, VLA）</strong> 模型。这不仅仅是一个输出文本计划的 VLM；它是一个经过“手术式”改造的 VLM，能够输出连续的、低层级的机器人动作。<alphaxiv-paper-citation title="VLA Concept" page="1" first="Our generalist robot" last="vision-language-action"></alphaxiv-paper-citation></p> <p>这里的潜力是双重的。首先，通过使用 VLM 骨干网络，机器人继承了“常识”——在看到机械臂之前，它就已经知道什么是“海绵”，什么是“擦拭”。其次，通过在大量多样化的机器人数据（不同的手臂、不同的夹爪、不同的任务）上进行训练，模型学习到了物理交互的通用表示。这种方法解决了“数据稀缺挑战”，因为它允许模型将知识从丰富的来源（互联网文本/图像）迁移到数据匮乏的机器人领域。<alphaxiv-paper-citation title="Data Scarcity" page="2" first="This can resolve" last="scarcity challenge,"></alphaxiv-paper-citation></p> <p>我们在大语言模型（LLMs）的成功中看到了这种方法的镜像。正如 GPT-4 通过在多样化文本上进行预训练成为通用推理机一样，$\pi_0$ 通过在多样化机器人数据上进行预训练成为通用操作机。目标是建立一个基础模型，它可以通过提示（prompting）来执行从未见过的任务，或者在少量高质量数据上进行微调，以掌握诸如叠衣服等复杂技能。<alphaxiv-paper-citation title="Goal" page="1" first="can be prompted" last="laundry folding."></alphaxiv-paper-citation></p> <hr> <h3 id="架构pi_0-的解剖学"><strong>架构——$\pi_0$ 的解剖学</strong></h3> <p>$\pi_0$ 的架构是一个混合系统。它不是从零开始的；它初始化自一个名为 <strong>PaliGemma</strong> 的预训练 VLM，这是一个拥有约 30 亿参数的开源模型。这个选择是经过深思熟虑的：30亿参数大到足以包含显著的语义知识，又小到在计算上对于机器人推理循环是可行的。<alphaxiv-paper-citation title="Base Model" page="5" first="we use PaliGemma" last="billion parameter VLM"></alphaxiv-paper-citation></p> <p>该模型处理多模态观测 $o_t$。这包括：</p> <ol> <li> <strong>图像：</strong> 多个 RGB 摄像头信号（例如，手腕摄像头和第三人称视角）。</li> <li> <strong>语言：</strong> 描述任务的文本指令 $\ell_t$（例如，“叠衬衫”）。</li> <li> <strong>本体感知（Proprioception）：</strong> 机器人的关节状态 $q_t$。 所有这些输入都被编码到一个共享的嵌入空间中。图像和关节状态通过线性层投影，以匹配语言 token 的维度，从而允许 Transformer 骨干网络统一处理它们。<alphaxiv-paper-citation title="Observation Space" page="5" first="The observation consists" last="embedding space as"></alphaxiv-paper-citation> </li> </ol> <p>(Action Head) 这里的关键创新是添加了一个 <strong>动作专家（Action Expert）</strong>。标准的 VLM 设计用于输出离散的文本 token（下一个词预测）。然而，机器人生活在一个连续的世界里；它们需要连续的关节速度或位置。为了弥合这一差距，$\pi_0$ 增加了一组专门的权重——大约 3 亿个参数——称为“动作专家”。这个专家接收来自 VLM 的处理后特征，并将它们解码为一系列未来的动作。<alphaxiv-paper-citation title="Action Expert" page="5" first="We refer to" last="the action expert."></alphaxiv-paper-citation></p> <p>至关重要的是，该模型不仅仅预测一个动作步。它预测一个 <strong>动作块（Action Chunk）</strong> $A_t = [a_t, a_{t+1}, \dots, a_{t+H-1}]$，其中 $H$ 是时间视界（在本论文中，$H=50$）。这种被称为 <em>动作分块（Action Chunking）</em> 的技术对于时间一致性至关重要。它可以防止在每一毫秒都重新规划的策略中常见的“抖动”。模型预测一个 50 步的轨迹，执行其中的一部分，然后重新规划。<alphaxiv-paper-citation title="Action Chunking" page="5" first="corresponds to an" last="action chunk of"></alphaxiv-paper-citation></p> <hr> <h3 id="数学引擎流匹配"><strong>数学引擎——流匹配</strong></h3> <p>如何生成这些高维的动作块？我们可以使用简单的均方误差（MSE）回归，但这假设了单峰分布（即只有一种拿杯子的有效方法）。实际上，机器人的行为是多峰的；你可以抓手柄，也可以抓杯沿。为了捕捉这一点，$\pi_0$ 使用了 <strong>流匹配（Flow Matching）</strong>，这是扩散模型（Diffusion Models）的一个强有力的替代方案。</p> <table> <tbody> <tr> <td>形式上，我们要对条件分布 $p(A_t</td> <td>o_t)$ 进行建模。作者采用了 <em>条件流匹配（Conditional Flow Matching）</em> 损失。核心思想是学习一个速度场 $v_\theta$，它在虚拟时间区间 $\tau \in [0, 1]$ 内将简单的噪声分布（高斯分布）推向复杂的有效机器人动作分布。<alphaxiv-paper-citation title="Loss Function" page="5" first="supervise these action" last="flow matching loss"></alphaxiv-paper-citation> </td> </tr> </tbody> </table> <p>损失函数定义为： \(L_\tau(\theta) = \mathbb{E}_{p(A_t|o_t), q(A_t^\tau|A_t)} ||v_\theta(A_t^\tau, o_t) - u(A_t^\tau|A_t)||^2\) 这里，$u(A_t^\tau|A_t)$ 是目标向量场。作者使用了一种特定的公式，称为 <strong>最优传输（Optimal Transport）</strong> 或 <strong>线性高斯（Linear-Gaussian）</strong> 概率路径。该路径将中间噪声样本 $A_t^\tau$ 定义为干净数据 $A_t$ 和噪声 $\epsilon$ 之间的线性插值： \(q(A_t^\tau|A_t) = \mathcal{N}(\tau A_t, (1-\tau)I)\) 这给出了样本构造： \(A_t^\tau = \tau A_t + (1-\tau)\epsilon\) 神经网络试图预测的目标向量场 $u$，实际上指向从噪声到数据的方向： \(u(A_t^\tau|A_t) = A_t - \epsilon\)</p> <alphaxiv-paper-citation title="Probability Path" page="5" first="given by q" last="linear-Gaussian (or"></alphaxiv-paper-citation> <p>在训练过程中，我们采样一个时间 $\tau$，将噪声与真实动作块混合得到 $A_t^\tau$，并要求网络预测方向 $A_t - \epsilon$。这种方法计算效率高且稳定。</p> <p>在 <strong>推理（Inference）</strong> 时，我们从纯噪声 $A_0^\tau \sim \mathcal{N}(0, I)$（其中 $\tau=0$）开始，并求解由我们学习到的向量场定义的常微分方程（ODE），以移动到 $\tau=1$。作者使用简单的 <strong>前向欧拉积分（Forward Euler integration）</strong>，步数为 10（$N=10$，步长 $\delta=0.1$）： \(A_t^{\tau+\delta} = A_t^\tau + \delta v_\theta(A_t^\tau, o_t)\) 这个迭代过程逐渐将随机潜变量“去噪”，转化为机器人可以执行的连贯、平滑的 50 步动作轨迹。<alphaxiv-paper-citation title="Inference" page="5" first="generate actions by" last="integration step size."></alphaxiv-paper-citation></p> <hr> <h3 id="数据引擎预训练与后训练"><strong>数据引擎——预训练与后训练</strong></h3> <p>模型的好坏取决于其数据。$\pi_0$ 以其严格的“预训练后微调（Pre-training then Post-training）”配方脱颖而出，这反映了 LLM 的演变过程（预训练 $\to$ SFT $\to$ RLHF）。</p> <p><strong>第一阶段：预训练（Pre-training）。</strong> 这里的目标是 <strong>鲁棒性和多样性</strong>。模型在海量的混合数据集上进行训练。这包括 Open X-Embodiment (OXE) 数据集——具体是一个经过过滤的子集，称为 “OXE Magic Soup”——以及 Physical Intelligence 收集的专有数据集。这些数据涵盖了 7 种不同的机器人配置（单臂、双臂、移动底座）和超过 68 项任务。这些数据在某种意义上是“低质量”的，因为它可能包含失败、次优路径或有噪声的遥操作数据。然而，它涵盖了极其广泛的物理现象、物体和场景。这一阶段教会机器人“世界是如何运作的”以及如何从错误中恢复。<alphaxiv-paper-citation title="Pre-training" page="2" first="The goal of" last="pre-training phase"></alphaxiv-paper-citation></p> <p><strong>第二阶段：后训练（Post-training）。</strong> 这里的目标是 <strong>流畅性和精确性</strong>。在预训练之后，模型针对特定的下游任务在高质量、精心策划的数据上进行微调。作者指出，虽然预训练提供了处理扰动的能力，但后训练才是使机器人变得“熟练（skillful）”的关键。例如，在叠衣服任务中，预训练帮助机器人识别衬衫，但后训练确保折叠整齐且动作流畅。这种关注点的分离——从多样化数据中学习 <em>做什么</em>，从高质量数据中学习 <em>如何做好</em>——是一个关键的结论。<alphaxiv-paper-citation title="Post-training" page="2" first="post-training dataset should" last="effective task execution,"></alphaxiv-paper-citation></p> <p>值得注意的是 <strong>跨具身（Cross-Embodiment）</strong> 方面。该模型控制具有不同运动学的不同机器人。这是通过共享的动作空间公式和 VLM 关注不同本体感知输入的能力来处理的。网络学会了将“拿起杯子”的视觉理解映射到 Franka Emika panda 机械臂与 UFactory xArm 所需的特定关节速度上。<alphaxiv-paper-citation title="Cross-Embodiment" page="1" first="diverse cross-embodiment" last="manipulation tasks."></alphaxiv-paper-citation></p> <hr> <h3 id="结果比较与未来方向"><strong>结果、比较与未来方向</strong></h3> <p>论文在极具挑战性的任务上评估了 $\pi_0$，如叠衣服、清理餐桌（bussing tables）和组装盒子。最引人注目的演示之一是叠衣服任务，移动操作机器人从烘干机中取出衣服，放入篮子，移动到桌子旁，然后折叠它们。这是一个多阶段、长视界的任务，既需要移动性也需要灵巧性。<alphaxiv-paper-citation title="Laundry Task" page="2" first="controls a mobile" last="article of clothing."></alphaxiv-paper-citation></p> <p>为了证明 VLM 骨干网络确实是必要的，作者将 $\pi_0$（33亿参数）与基线模型 <strong>$\pi_0$-small</strong>（4.7亿参数）进行了比较。$\pi_0$-small 使用相同的流匹配头和数据，但在初始化时 <em>没有</em> 使用 VLM 权重。结果表明，对于复杂的语义任务，VLM 初始化提供了显著的性能提升。“小”模型难以泛化到新指令或未见过的物体，这凸显了来自 VLM 的语义“世界知识”确实迁移到了控制中。<alphaxiv-paper-citation title="Baseline Comparison" page="5" first="Non-VLM baseline model." last="incorporating VLM pertaining."></alphaxiv-paper-citation></p> <p><strong>思考：</strong></p> <ol> <li> <strong>延迟（Latency）：</strong> 每个控制周期运行一个 30 亿参数的模型是昂贵的。50 步的动作分块如何缓解这个问题？（它将有效推理频率从例如 50Hz 降低到了 1Hz）。</li> <li> <strong>安全性（Safety）：</strong> 流匹配是概率性的。在强大的机器人硬件上运行随机策略是否安全？</li> <li> <strong>缩放定律（Scaling Laws）：</strong> 如果我们从 PaliGemma (3B) 转移到 700 亿参数的模型，机器人性能会线性扩展吗？还是说物理灵巧性受限于语义理解之外的其他因素？</li> </ol> <p>这篇论文表明，机器人的未来在于“下载”LLM/VLM 的大脑，并将其连接到一个在物理数据上训练的“脊髓”（动作专家）上。对于 NLP、计算机视觉和机器人技术融合为一个单一、统一的研究领域来说，这是一个令人信服的论据。</p> <h2 id="两阶段的侧重点">两阶段的侧重点</h2> <p><strong>Loss 函数在两个阶段是完全一样的，但数据集的构成和侧重点完全不同。</strong></p> <h3 id="1-loss-函数完全一致">1. Loss 函数：完全一致</h3> <p>无论是在 <strong>预训练 (Pre-training)</strong> 阶段还是 <strong>后训练 (Post-training)</strong> 阶段，$\pi_0$ 使用的都是同一个 <strong>条件流匹配 (Conditional Flow Matching)</strong> 损失函数：</p> \[L_\tau(\theta) = \mathbb{E}_{p(A_t|o_t),q(A_t^\tau|A_t)} ||v_\theta(A_t^\tau, o_t) - u(A_t^\tau|A_t)||^2\] <ul> <li> <strong>统一的优化目标</strong>：这一点非常重要。这意味着模型在两个阶段都在学习同一件事——预测动作分布的“速度场”。不存在从“对比学习”切换到“回归”这种目标的跳跃。</li> <li> <strong>微调机制</strong>：后训练本质上就是在一个更小、更专用的数据分布上继续优化这个 Loss。这类似于 LLM 的 SFT（Supervised Fine-Tuning），目标函数不变，只是数据分布变了。</li> </ul> <hr> <h3 id="2-数据集天壤之别">2. 数据集：天壤之别</h3> <p>虽然 Loss 一样，但“喂”给模型的数据在性质、规模和目的上有巨大的差异。</p> <h4 id="a-预训练数据-pre-training-mixture"><strong>A. 预训练数据 (Pre-training Mixture)</strong></h4> <ul> <li> <strong>规模与构成</strong>：这部分数据非常庞大，是“大杂烩”。它包含了： <ul> <li> <strong>开源数据 (Open X-Embodiment, OXE)</strong>：这部分约占 9.1%，涵盖了 22 种不同的机器人。</li> <li> <strong>自有数据 ($\pi$ dataset)</strong>：约 9 亿个时间步 (timesteps)，涵盖 7 种不同的机器人配置（单臂、双臂、移动底座等）和 68 种任务。</li> </ul> </li> <li> <strong>数据质量</strong>：这部分数据的质量参差不齐。它可能包含次优的轨迹、失败的尝试、带有噪声的遥操作数据。</li> <li> <strong>目的</strong>：目的是让模型见多识广。通过看各种各样的机器人做各种各样的事，模型学会了“通用物理常识”（比如物体怎么抓，关节怎么动）和“鲁棒性”（如何从奇怪的状态中恢复）。</li> <li> <strong>权重策略</strong>：为了防止某些任务主导训练，他们对数据进行了加权处理（$n^{0.43}$），稍微平衡了不同任务的比例。 <alphaxiv-paper-citation title="Pre-training Mixture" page="5" first="consists of a" last="diverse language labels"></alphaxiv-paper-citation> </li> </ul> <h4 id="b-后训练数据-post-training-data"><strong>B. 后训练数据 (Post-training Data)</strong></h4> <ul> <li> <strong>规模</strong>：相对非常小。对于简单的任务可能只需要 5 小时的数据，对于像叠衣服这样复杂的任务可能需要 100 小时左右的数据。</li> <li> <strong>数据质量</strong>：<strong>极高 (High-Quality)</strong>。这是经过精心策划和筛选的。这些数据展示了任务执行的“黄金标准”——动作流畅、策略一致、没有多余的动作。</li> <li> <strong>目的</strong>：目的是“专业化”和“对齐”。预训练模型可能知道怎么抓衣服，但可能抓得歪歪扭扭。后训练数据教模型：“在这个特定任务下，不仅要抓到，还要抓得优雅、高效。”</li> <li> <strong>不同任务不同数据</strong>：每个下游应用（比如叠衣服、收拾桌子）都有自己专属的后训练数据集。模型是在特定的数据集上微调，从而变成该任务的专家。 <alphaxiv-paper-citation title="Post-training Data" page="5" first="high-quality curated data" last="specific downstream tasks."></alphaxiv-paper-citation> </li> </ul> <h3 id="训练数据总结">训练数据总结</h3> <p>| 特性 | 预训练 (Pre-training) | 后训练 (Post-training) | | :— | :— | :— | | <strong>Loss 函数</strong> | <strong>流匹配 Loss (Flow Matching)</strong> | <strong>流匹配 Loss (Flow Matching)</strong> | | <strong>数据来源</strong> | OXE (开源) + 内部大规模混合数据 | 针对特定任务的高质量专有数据 | | <strong>数据规模</strong> | 巨大 (~9亿步) | 较小 (5~100小时) | | <strong>数据多样性</strong> | 极高 (多机器人、多场景) | 低 (专注于单一任务) | | <strong>数据质量</strong> | 混合 (含次优/噪声) | 极高 (精心筛选的演示) | | <strong>学习目标</strong> | 通用能力、鲁棒性、物理常识 | 任务熟练度、动作流畅性 |</p> <p>这种策略的核心逻辑是：<strong>用海量数据换取泛化能力，用少量精品数据换取专业技能。</strong> Loss 函数保持一致保证了知识的平滑迁移。</p> <p>** 另外两个阶段的参数更新策略是不一样的**。</p> <h3 id="1-预训练阶段-pre-training">1. 预训练阶段 (Pre-training)</h3> <p><strong>策略：几乎所有参数都参与训练 (Full Fine-tuning / Joint Training)</strong></p> <p>在预训练阶段，目标是让模型学会“通用的物理常识”。因此，不仅要训练新加的 <strong>Action Expert (动作专家)</strong>，还要让原本的 <strong>VLM Backbone (视觉-语言骨干)</strong> 适应机器人领域的图像和文本分布。</p> <ul> <li> <strong>Action Expert (300M)</strong>: 从随机初始化开始，<strong>全程参与训练</strong>。这是必须的，因为它要学会把语义特征翻译成动作。</li> <li> <strong>VLM Backbone (3B)</strong>: 虽然加载了预训练权重，但在这一阶段也是 <strong>解冻 (Unfrozen)</strong> 的。 <ul> <li> <strong>为什么？</strong> 因为原本的 VLM (PaliGemma) 是在互联网图片上训练的，它没见过这么多充满杂乱电线、机械臂特写、低分辨率工业相机的图片。如果锁死 VLM 参数，视觉特征提取能力会受限。</li> <li> <strong>LoRA (Low-Rank Adaptation)</strong>: 为了节省显存和防止灾难性遗忘，作者实际上在这个阶段使用了 <strong>LoRA</strong> 技术微调 VLM 部分的权重，而不是全参数微调。这是一种折中方案。</li> </ul> </li> </ul> <h3 id="2-后训练阶段-post-training">2. 后训练阶段 (Post-training)</h3> <p><strong>策略：针对特定任务的微调 (Task-Specific Fine-tuning)</strong></p> <p>在后训练阶段，我们希望模型专注于特定任务（例如叠衣服），但又不想让它忘记通用的物理知识。</p> <ul> <li> <strong>通常做法</strong>：在这个阶段，往往会 <strong>解冻所有参数</strong> 进行微调，但使用 <strong>非常小的学习率</strong>。 <ul> <li>因为数据集很小（几小时），如果只训练 Action Expert，模型可能学不到任务特有的视觉特征（比如识别某种特定的衣物折痕）。</li> <li>如果只训练 VLM，动作输出又跟不上。</li> <li>所以，<strong>全参数微调 (Full Fine-tuning)</strong> 或 <strong>LoRA微调</strong> 是首选。</li> </ul> </li> <li> <strong>特殊情况</strong>：有些论文会选择冻结 VLM，只微调 Action Head，但在 $\pi_0$ 这种追求极致性能的模型中，为了适应任务特有的视觉分布（例如叠衣服时特殊的布料纹理），通常会让 VLM 也跟着微调。</li> </ul> <p><strong>关键区别总结：</strong></p> <table> <thead> <tr> <th style="text-align: left">特性</th> <th style="text-align: left">预训练 (Pre-training)</th> <th style="text-align: left">后训练 (Post-training)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Action Expert</strong></td> <td style="text-align: left"><strong>全参数训练 (从零开始)</strong></td> <td style="text-align: left"><strong>全参数微调 (基于预训练)</strong></td> </tr> <tr> <td style="text-align: left"><strong>VLM Backbone</strong></td> <td style="text-align: left"><strong>LoRA 微调 (适应机器人域)</strong></td> <td style="text-align: left"><strong>全参数/LoRA 微调 (适应特定任务)</strong></td> </tr> <tr> <td style="text-align: left"><strong>Image Encoders</strong></td> <td style="text-align: left"><strong>解冻 (Unfrozen)</strong></td> <td style="text-align: left"><strong>解冻 (Unfrozen)</strong></td> </tr> <tr> <td style="text-align: left"><strong>学习率</strong></td> <td style="text-align: left">较大 (学习通用特征)</td> <td style="text-align: left">较小 (防止过拟合/遗忘)</td> </tr> </tbody> </table> <p><strong>特别注意：</strong> 虽然论文没有显式地大篇幅讨论“冻结”与否，但在附录 B.2 中提到使用了 LoRA。这暗示了为了保持 VLM 的通用语义能力，他们并没有直接破坏 VLM 的权重，而是通过外挂参数（LoRA）来适配机器人任务。这种做法在多模态大模型微调中非常标准。</p> <h2 id="详细的网络结构">详细的网络结构</h2> <h3 id="1-输入层-input-processing"><strong>1. 输入层 (Input Processing)</strong></h3> <p>模型的输入是一个元组 $(o_t, A_t^\tau)$，其中 $o_t$ 是观测，$A_t^\tau$ 是带噪声的动作块（仅在训练时输入，推理时是纯噪声）。</p> <p><strong>A. 图像 (Images) $I_t^1, \dots, I_t^n$</strong></p> <ul> <li> <strong>来源</strong>：这就好比人的眼睛。$\pi_0$ 接收 $n$ 个摄像头的 RGB 图像。 <ul> <li>例如：1个手腕相机 + 2个外部相机 = 3张图。</li> </ul> </li> <li> <strong>处理</strong>：这些图像通过 <strong>SigLIP</strong> (类似于 CLIP 的视觉编码器) 进行编码。</li> <li> <strong>Token化</strong>：每张图像被切分成多个 patches，拉平成一串 tokens。</li> <li> <strong>Shape</strong>：假设每张图变成 256 个 tokens，3张图就是 $3 \times 256 = 768$ 个 tokens。每个 token 的维度是 VLM 的隐藏层维度 $w$ (例如 2048)。 <ul> <li>$\text{Image Tokens}: [n \times 256, w]$</li> </ul> </li> </ul> <p><strong>B. 语言指令 (Language) $\ell_t$</strong></p> <ul> <li> <strong>来源</strong>：这就好比人的耳朵。比如用户输入的文本 “Fold the towel”。</li> <li> <strong>处理</strong>：通过标准的 Tokenizer 处理。</li> <li> <strong>Shape</strong>：变成一串文本 tokens。 <ul> <li>$\text{Text Tokens}: [L_{text}, w]$，其中 $L_{text}$ 是指令长度。</li> </ul> </li> </ul> <p><strong>C. 本体感知 (Proprioception) $q_t$</strong></p> <ul> <li> <strong>来源</strong>：这就好比人的肌肉感觉。机器人当前的关节角度、夹爪状态等。</li> <li> <strong>处理</strong>：这是一个低维向量 (例如 7个关节 + 1个夹爪 = 8维)。它通过一个 <strong>线性投影层 (Linear Projection)</strong> 映射到 VLM 的维度 $w$。</li> <li> <strong>Shape</strong>： <ul> <li>原始：$[1, d_q]$ (例如 $[1, 8]$)</li> <li>投影后：$[1, w]$ (作为一个单独的 token 混入序列) <alphaxiv-paper-citation title="Proprioception Projection" page="15" first="We add an input" last="a linear projection."></alphaxiv-paper-citation> </li> </ul> </li> </ul> <p><strong>D. 带噪声的动作块 (Noisy Action Chunk) $A_t^\tau$</strong></p> <ul> <li> <strong>来源</strong>：这是流匹配的核心。在训练时，它是真实动作 $A_t$ 加了噪声；在推理时，它是从高斯分布采样的纯噪声。</li> <li> <strong>结构</strong>：这是一个包含未来 $H$ 步动作的序列。 <ul> <li>$H = 50$ (预测未来50步)</li> <li>$d$ = 动作维度 (例如 14维：7关节+1夹爪 $\times$ 2个手臂)</li> </ul> </li> <li> <strong>时间嵌入 (Time Embedding)</strong>：这是关键细节！模型必须知道当前的去噪进度 $\tau \in [0, 1]$。 <ul> <li>$\tau$ 通过正弦位置编码 (Sinusoidal Positional Encoding) 变成向量。</li> <li>每个动作 token $a_t^\tau$ 都与时间嵌入 $\tau$ 拼接/相加。</li> </ul> </li> <li> <strong>处理</strong>：通过 <strong>MLP</strong> 投影到维度 $w$。</li> <li> <strong>Shape</strong>： <ul> <li>原始：$[H, d]$ (例如 $[50, 14]$)</li> <li>投影后：$[H, w]$ (50个 tokens) <alphaxiv-paper-citation title="Action Tokens" page="15" first="noisy action chunk" last="embedding dimension using"></alphaxiv-paper-citation> </li> </ul> </li> </ul> <hr> <h3 id="2-骨干网络-backbone-processing"><strong>2. 骨干网络 (Backbone Processing)</strong></h3> <p>现在我们把所有这些 tokens 拼接到一起，形成一个巨大的序列： \(\text{Token Sequence} = [\underbrace{\text{Image Tokens}}_{\text{VLM}}, \underbrace{\text{Text Tokens}}_{\text{VLM}}, \underbrace{\text{Proprioception Token}}_{\text{Expert}}, \underbrace{\text{Action Tokens}}_{\text{Expert}}]\)</p> <p>这个序列被送入 <strong>Transformer</strong>。但这里有个特殊的 <strong>混合专家 (MoE) 结构</strong>：</p> <ul> <li> <strong>VLM Backbone (PaliGemma)</strong>：处理图像和文本 tokens。参数量 ~3B。</li> <li> <strong>Action Expert</strong>：处理本体感知和动作 tokens。参数量 ~300M。</li> <li> <strong>交互</strong>：虽然有两套权重，但它们在 Transformer 的 <strong>Self-Attention</strong> 层是互通的！ <ul> <li>动作 tokens 可以 attend 到 图像 tokens (看图做动作)。</li> <li>动作 tokens 可以 attend 到 文本 tokens (听指令做动作)。</li> <li> <strong>但注意</strong>：为了防止破坏预训练 VLM 的知识，VLM tokens (图像/文本) <strong>看不到</strong> 动作 tokens (因果掩码)。</li> </ul> </li> </ul> <p><strong>Attention Mask 细节</strong>：</p> <ol> <li> <strong>[图像, 文本]</strong> $\to$ 可以互看 (双向)，但看不到后面。</li> <li> <strong>[本体感知]</strong> $\to$ 可以看 [图像, 文本] 和自己。</li> <li> <strong>[动作块]</strong> $\to$ 可以看 <strong>所有人</strong> (图像, 文本, 本体, 动作)。这是一个 <strong>全双向 Attention</strong> (Full Bidirectional Attention) 在动作块内部，意味着第 1 步动作可以看到第 50 步动作的信息，保证轨迹平滑。 <alphaxiv-paper-citation title="Attention Mask" page="15" first="uses a blockwise" last="attend to each other."></alphaxiv-paper-citation> </li> </ol> <hr> <h3 id="3-输出层-output-processing"><strong>3. 输出层 (Output Processing)</strong></h3> <p>Transformer 吐出处理后的特征序列。我们只关心 <strong>动作 Tokens</strong>对应的输出。</p> <ul> <li> <strong>提取</strong>：取出最后 $H$ 个 tokens 的输出特征。 <ul> <li>Shape: $[H, w]$</li> </ul> </li> <li> <strong>解码</strong>：通过一个 <strong>线性头 (Linear Head)</strong> 将特征映射回动作维度 $d$。</li> <li> <strong>最终输出</strong>：$v_\theta(A_t^\tau, o_t)$，即预测的速度场。 <ul> <li>Shape: $[H, d]$ (例如 $[50, 14]$)</li> </ul> </li> </ul> <p>这个输出就是告诉我们在当前噪声水平下，应该往哪个方向调整动作，才能让它更像真实的合理动作。</p> <hr> <h3 id="总结数据流向图"><strong>总结：数据流向图</strong></h3> <ol> <li> <strong>Input</strong>: <ul> <li>Images ($3 \times 256 \times w$)</li> <li>Text ($L \times w$)</li> <li>Joints ($1 \times w$)</li> <li>Noisy Actions ($50 \times w$) + Time $\tau$</li> </ul> </li> <li> <strong>Concat</strong>: 形成长序列 $[N_{total}, w]$</li> <li> <strong>Transformer</strong>: <ul> <li>VLM layers 处理前两部分</li> <li>Action Expert layers 处理后两部分</li> <li>Cross-Attention 让动作部分“看见”视觉和语言</li> </ul> </li> <li> <strong>Output Head</strong>: <ul> <li>只取最后 50 个 tokens</li> <li>Linear Projection $\to [50, 14]$</li> </ul> </li> <li> <strong>Result</strong>: 预测的速度向量 $v$，用于 ODE 积分器推下一步。</li> </ol> <h2 id="some-comments">Some comments</h2> <p>我理解 Action Expert 就是就相当于是 从action 空间到 LLM空间的一个projector, Action Expert 在训练的时候,可以输入的是 learnable tokens 也可以是加noisy 后的 action; 经过LLM输出后的token, 经过head, 直接回归action的”velocity”, 这就相当于是把LLM当成了 flow matching中的DiT了. 也可以把 action head 当成 “Dit”, 把LLM的输出action tokens当成条件.</p> <p>第一种理解是对的——<strong>$\pi_0$ 把整个 LLM（或者更准确地说是经过魔改的 VLM Backbone + Action Expert）直接当成了 Flow Matching 中的 DiT (Diffusion Transformer)。</strong></p> <p>这并不是一个“把 LLM 当特征提取器，后面接个 DiT Head”的结构（那是 Octo 或者 OpenVLA 的做法），而是一个 <strong>端到端、Token 级别混合的 DiT</strong>。</p> <h3 id="1-action-expert-不是简单的-projector">1. Action Expert 不是简单的 Projector</h3> <p>虽然 Action Expert 确实包含了一个 Linear Projection 层（把动作维度 $d$ 映射到 LLM 维度 $w$），但它<strong>不仅仅是</strong>一个 Projector。 在 $\pi_0$ 中，Action Expert 是一组 <strong>Transformer Layers（权重）</strong>，它们与 VLM 的 Transformer Layers 是并行的或者交织的（但在 $\pi_0$ 的实现中，更像是一个 Mixture-of-Experts，即某些 Token 走 VLM 权重，某些 Token 走 Action Expert 权重）。</p> <ul> <li> <strong>输入给 Action Expert 的是：</strong> <strong>加噪后的 Action Tokens ($A_t^\tau$)</strong>。</li> <li> <strong>位置编码：</strong> 这些 Tokens 加上了时间步 $\tau$ 的 Embedding。</li> </ul> <h3 id="2-llm-本身就是-dit">2. LLM 本身就是 DiT</h3> <p>在标准的 Diffusion/Flow Matching 中，我们需要一个网络 $\epsilon_\theta(x_t, t, c)$ 来预测噪声/速度。 在 $\pi_0$ 中，这个网络 $\epsilon_\theta$ <strong>就是整个 VLA 模型</strong>。</p> <ul> <li> <strong>Condition ($c$)</strong>: 图像 Tokens + 文本 Tokens + 本体感知 Token。</li> <li> <strong>Noisy Input ($x_t$)</strong>: 加噪动作 Tokens $A_t^\tau$。</li> <li> <strong>Time ($t$)</strong>: $\tau$ Embedding 加在动作 Tokens 上。</li> </ul> <p>这些 Tokens 全部 <strong>拼接 (Concat)</strong> 在一起，送入同一个 Transformer 序列中。</p> <ul> <li>图像/文本 Tokens 走 VLM 的权重（Self-Attention + MLP）。</li> <li>动作 Tokens 走 Action Expert 的权重（Self-Attention + MLP）。</li> <li> <strong>关键点</strong>：它们在 <strong>Self-Attention 层是互通的</strong>！ <ul> <li>动作 Token $a_i$ 会去 Attend 到 图像 Token $I_j$。</li> <li>这就是“条件生成”发生的时刻——噪声动作通过 Attention 机制“看”到了图像信息，从而知道该往哪里去噪。</li> </ul> </li> </ul> <h3 id="3-输出直接回归-velocity">3. 输出直接回归 Velocity</h3> <p>经过 $L$ 层 Transformer 处理后，对应 <strong>动作 Tokens 位置的输出</strong> 被拿出来。</p> <ul> <li>这些输出特征经过一个简单的 Linear Head（Projector 的逆变换）。</li> <li>直接回归出 <strong>目标速度 $v$</strong> (即 $A_t - \epsilon$)。</li> <li> <strong>没有额外的 Head 网络</strong>：不需要再接一个 MLP 或 ResNet 来做去噪，Transformer 的输出就是去噪后的结果。</li> </ul> <h3 id="总结它是哪种方式">总结：它是哪种方式？</h3> <table> <thead> <tr> <th style="text-align: left">方式</th> <th style="text-align: left">描述</th> <th style="text-align: left">$\pi_0$ 是吗？</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>方式 A (Head-only)</strong></td> <td style="text-align: left">LLM 输出 Features $\to$ 小型 DiT Head (Conditioned on Features)</td> <td style="text-align: left">❌ (这是 Octo/OpenVLA)</td> </tr> <tr> <td style="text-align: left"><strong>方式 B (End-to-End)</strong></td> <td style="text-align: left"><strong>Noisy Actions $\to$ LLM Input Tokens $\to$ LLM Output Tokens $\to$ Velocity</strong></td> <td style="text-align: left"><strong>✅ (这是 $\pi_0$)</strong></td> </tr> </tbody> </table> <p><strong>为什么选方式 B？</strong></p> <ul> <li> <strong>深度融合</strong>：动作生成过程可以利用 LLM 深层的语义理解，而不是只利用最后一层的特征。</li> <li> <strong>参数共享</strong>：虽然分了 Expert，但在 Attention 机制上是共享计算资源的，动作 Tokens 可以直接查询视觉 Tokens 的 Key/Value。</li> <li> <strong>扩展性</strong>：这种结构符合 Scaling Law，模型越大，生成动作的能力理论上越强。</li> </ul> <p>所以，$\pi_0$ 本质上是一个 <strong>多模态的 DiT</strong>，其中一部分模态（图像/文本）是固定的 Condition，另一部分模态（动作）是需要去噪的生成目标。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/DriveJEPA/">DriveJEPA</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/C_RADIOv4/">C_RADIOv4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/VLM4VLA/">VLM4VLA</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 P W Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>