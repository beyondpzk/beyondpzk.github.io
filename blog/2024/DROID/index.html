<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> DROID | Tenacious life, proud journey. </title> <meta name="author" content="P W Name"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://beyondpzk.github.io/blog/2024/DROID/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Tenacious life, proud journey. </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">DROID</h1> <p class="post-meta"> Created on March 19, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/category/vla"> <i class="fa-solid fa-tag fa-sm"></i> VLA</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>[TOC]</p> <h1 id="droid-a-large-scale-in-the-wild-robot-manipulation-dataset">DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset</h1> <p><a href="https://arxiv.org/abs/2403.12945" rel="external nofollow noopener" target="_blank">论文链接</a></p> <h1 id="据驱动的具身智能解构-droid-数据集与分布式机器人学习">据驱动的具身智能——解构 DROID 数据集与分布式机器人学习</h1> <p><strong>目标</strong>：通过深度剖析 DROID 论文，理解 Robot Learning 中“数据中心（Data-Centric）”范式的挑战、解决方案及最新进展。</p> <hr> <h2 id="机器人学习的数据墙与历史沿革">机器人学习的“数据墙”与历史沿革</h2> <h3 id="11-引言莫拉维克悖论的现代版">1.1 引言：莫拉维克悖论的现代版</h3> <ul> <li> <strong>讨论引入</strong>：为什么 GPT-4 可以写代码，但机器人还没法帮我收拾洗碗机？</li> <li> <strong>核心论点</strong>：机器人领域的瓶颈已从“算法”转向“数据”。 <ul> <li>CV/NLP 的成功公式：Transformer + 大规模算力 + <strong>互联网级数据</strong>。</li> <li>机器人的困境：缺乏“机器人的互联网”。数据无法爬取，必须物理执行。</li> <li>现状：大多数策略仅能在训练环境（In-Domain）工作，泛化能力（Generalization）极差。</li> </ul> </li> </ul> <h3 id="12-现有数据集的批判性回顾-critical-review">1.2 现有数据集的批判性回顾 (Critical Review)</h3> <p>在介绍 DROID 之前，我们需要回顾它想要取代或补充的“前辈”们。</p> <ul> <li> <strong>MIME / RoboTurk</strong>：早期尝试，主要靠人类遥操作，但规模小，场景单一。</li> <li> <strong>BridgeData V2</strong>：这是一个重要的里程碑。DROID 的很多设计哲学继承自 Bridge，但 Bridge 主要局限于加州大学伯克利分校（UC Berkeley）的厨房模型，场景多样性不足。</li> <li> <strong>Open X-Embodiment (OXE)</strong>：这是目前最大的聚合数据集。 <ul> <li> <em>缺点</em>：它是“大杂烩”，包含各种机器人形态、各种控制频率、各种相机角度。这种<strong>分布的不一致性（Inconsistency）</strong>给训练带来了巨大的噪声。</li> </ul> </li> <li> <strong>DROID 的生态位</strong>：DROID 试图在“规模（Scale）”和“一致性（Consistency）”之间找到平衡点。它比单一实验室数据大，但比 OXE 这种聚合数据更一致。 <alphaxiv-paper-citation title="Comparison Table" page="2" first="Comparison to existing" last="in this table."></alphaxiv-paper-citation> </li> </ul> <h3 id="13-核心概念定义">1.3 核心概念定义</h3> <ul> <li> <strong>In-the-Wild（野外/非受控）</strong>：不仅是背景变化，还包括光照、物体摆放、干扰物。DROID 强调在 52 个不同建筑中采集。 <alphaxiv-paper-citation title="Scale" page="2" first="564 scenes across" last="52 buildings"></alphaxiv-paper-citation> </li> <li> <strong>分布（Distribution）</strong>：从场景（Scene）、任务（Task/Verb）、视点（Viewpoint）三个维度理解数据的丰富程度。</li> </ul> <hr> <h2 id="分布式数据工程如何构建-droid">分布式数据工程——如何构建 DROID</h2> <p><em>这是最“硬核”的工程部分</em></p> <h3 id="21-组织层面的挑战分布式协作-the-social-engineering">2.1 组织层面的挑战：分布式协作 (The Social Engineering)</h3> <ul> <li> <strong>问题</strong>：如何让全球 13 个机构、50 个采集员收集出看起来像是一个人收集的数据？</li> <li> <strong>解决方案</strong>：极度的标准化。 <ul> <li> <strong>硬件统一</strong>：所有实验室必须使用完全相同的硬件清单（BOM）。</li> <li> <strong>软件统一</strong>：统一的 Docker 容器和数据记录脚本。</li> </ul> </li> </ul> <h3 id="22-硬件平台架构-the-hardware-stack">2.2 硬件平台架构 (The Hardware Stack)</h3> <p>详细拆解 DROID 的采集设备，这对于学生理解机器人系统集成至关重要：</p> <ul> <li> <strong>机器人本体</strong>：Franka Panda (7-DoF)。<em>讨论：为什么选 Panda？（灵敏的力控、科研界的标准机、易于移动）</em>。</li> <li> <strong>视觉系统</strong>： <ul> <li> <strong>3个 RGB 摄像头</strong>：为什么需要三个？ <ul> <li> <em>Wrist (手眼)</em>：处理精细操作，解决遮挡。</li> <li> <em>Side &amp; Front (第三视角)</em>：提供全局上下文。 <alphaxiv-paper-citation title="Camera Setup" page="2" first="three synchronized RGB" last="depth information"></alphaxiv-paper-citation> </li> </ul> </li> <li> <strong>Stereo Depth (立体深度)</strong>：使用 ZED 2i 相机。虽然目前的扩散策略主要用 RGB，但深度信息对未来的 3D 策略至关重要。</li> </ul> </li> <li> <strong>便携性设计</strong>：整个系统被设计为可移动推车。这是实现“In-the-Wild”的关键——如果搬运需要 3 小时，没人会去浴室采集数据。</li> </ul> <h3 id="23-数据采集协议与流程-collection-protocol">2.3 数据采集协议与流程 (Collection Protocol)</h3> <ul> <li> <strong>遥操作 (Teleoperation)</strong>：使用 VR 控制器还是 3D 鼠标？DROID 使用的是 Oculus Quest 2 手柄还是其它？（论文中提到是基于 VR 或 3D 空间鼠标的映射，通常为了灵活性）。</li> <li> <strong>元数据标注</strong>： <ul> <li> <strong>语言指令</strong>：采集时不仅是动动机械臂，还必须记录“我在做什么”。DROID 包含自然语言指令。</li> <li> <strong>成功/失败标注</strong>：这对于训练离线强化学习（Offline RL）至关重要。</li> </ul> </li> </ul> <h3 id="24-数据的时空分布分析">2.4 数据的时空分布分析</h3> <ul> <li> <strong>场景分布</strong>：展示论文中的地图分布。</li> <li> <strong>任务分布</strong>：分析 86 个 Verbs。不仅有简单的 Pick &amp; Place，还有 Open, Close, Wipe, Pour 等接触丰富（Contact-Rich）的任务。 <alphaxiv-paper-citation title="Task Diversity" page="1" first="86 Tasks / Verbs" last="86 Tasks / Verbs"></alphaxiv-paper-citation> </li> <li> <strong>视点多样性</strong>：DROID 拥有 1417 个独特的相机外参。这迫使模型学习几何特征，而不是死记硬背像素位置。</li> </ul> <hr> <h2 id="策略学习与算法验证">策略学习与算法验证</h2> <p><em>这部分重点讲解如何利用这些数据进行训练，涉及深度学习模型细节。</em></p> <h3 id="31-算法基准diffusion-policy">3.1 算法基准：Diffusion Policy</h3> <ul> <li> <strong>背景补充</strong>：简要回顾 Diffusion Policy（扩散策略）。它为什么比 BC-Transformer 或 LSTM-GMM 好？（能建模多模态分布，抗噪性强）。</li> <li> <strong>DROID 的训练设置</strong>： <ul> <li> <strong>输入</strong>：多视角图像 + 语言指令 + 机器人本体感知（Proprioception）。</li> <li> <strong>输出</strong>：未来的动作序列（Action Chunking）。</li> <li> <strong>架构</strong>：基于 CNN（ResNet/EfficientNet）的编码器 + Diffusion Head。</li> </ul> </li> </ul> <h3 id="32-实验设计co-training-联合训练">3.2 实验设计：Co-training (联合训练)</h3> <p>这是论文的核心发现。<strong>并不是只用 DROID 训练，而是把 DROID 作为一种“通用先验”混入到特定任务数据中。</strong></p> <ul> <li> <strong>实验组设置</strong>： <ol> <li>仅使用特定任务的小样本数据（Target Data Only）。</li> <li>特定任务数据 + OXE 数据。</li> <li>特定任务数据 + DROID 数据。</li> </ol> </li> <li> <strong>关键结论</strong>： <ul> <li> <strong>结论 1：DROID 显著提升性能</strong>。引入 DROID 数据后，平均成功率提升了 20%。 <alphaxiv-paper-citation title="Performance Boost" page="2" first="boosts policy performance," last="20% on average"></alphaxiv-paper-citation> </li> <li> <strong>结论 2：DROID &gt; OXE</strong>。在同等条件下，混入 DROID 数据比混入 OXE 数据效果更好。 <ul> <li> <em>深度讨论</em>：为什么？OXE 数据量更大（1.4M vs 76k），为什么输了？</li> <li> <em>解释</em>：<strong>域差异（Domain Gap）</strong>。OXE 里的机器人和环境差异太大，模型很难迁移。而 DROID 虽然场景变了，但机器人的动力学和相机相对位置（部分）是标准化的。这证明了高质量、同构数据的重要性。</li> </ul> </li> </ul> </li> </ul> <h3 id="33-鲁棒性与分布外泛化-ood-generalization">3.3 鲁棒性与分布外泛化 (OOD Generalization)</h3> <ul> <li> <strong>干扰物测试</strong>：在桌面上扔一堆没见过的垃圾，模型还能工作吗？实验表明 DROID 训练的模型更不在乎背景噪声。</li> <li> <strong>位置变化</strong>：移动相机或机器人底座。由于 DROID 训练集里包含了大量随机的相机位姿，模型学会了空间不变性。 <alphaxiv-paper-citation title="Robustness" page="8" first="robustness to distractor" last="spatial perturbations."></alphaxiv-paper-citation> </li> </ul> <hr> <h2 id="深度研讨与未来展望">深度研讨与未来展望</h2> <h3 id="开放性问题与未来方向">开放性问题与未来方向</h3> <ul> <li> <strong>Visual Foundation Models (VFM)</strong>：如何利用 DROID 训练像 CLIP 或 R3M 这样的视觉表征模型？</li> <li> <strong>World Models</strong>：能否用这 350 小时的视频训练一个从当前帧预测下一帧的世界模型？</li> <li> <strong>Language Grounding</strong>：DROID 的语言指令相对简单，如何扩展到复杂的推理任务？</li> </ul> <hr> <h2 id="具体到一个sample">具体到一个sample</h2> <hr> <h3 id="解剖一只麻雀详解-droid-训练样本">解剖一只“麻雀”——详解 DROID 训练样本</h3> <p>我们一直在说 76k 条轨迹，但每一条轨迹到底长什么样？如果现在用 Python 打开其中一个 <code class="language-plaintext highlighter-rouge">.h5</code> 或者 <code class="language-plaintext highlighter-rouge">.zarr</code> 文件，会看到什么？让我们以一个具体的任务为例：<strong>‘将苹果放入锅中’（Put apple in pot）</strong>，来解构一个标准的 DROID 样本。</p> <h4 id="1-基本概念轨迹-trajectory-vs-样本-sample">1. 基本概念：轨迹 (Trajectory) vs. 样本 (Sample)</h4> <p>首先要澄清，磁盘上存储的是<strong>轨迹（Trajectory）</strong>，而送入神经网络训练的是<strong>样本（Sample/Batch）</strong>。</p> <ul> <li> <strong>一条轨迹</strong>：是一个完整的交互过程，从机器人开始动，到任务完成，通常持续 10~30 秒。假设采样频率是 15Hz，一条 20 秒的轨迹就包含 $T=300$ 个时间步（Timesteps）。</li> <li> <strong>一个训练样本</strong>：通常是从这 300 个时间步中随机切片出来的一小段（或者是当前帧 + 历史帧）。</li> </ul> <p>我们将重点分析<strong>一个时间步（Timestep $t$）</strong>包含的所有数据字段。</p> <hr> <h4 id="2-数据字段详解-the-anatomy-of-a-timestep">2. 数据字段详解 (The Anatomy of a Timestep)</h4> <p>想象一个 Python 字典 <code class="language-plaintext highlighter-rouge">data_dict</code>，它包含以下核心键值对：</p> <h5 id="a-语言指令-language-instruction">A. 语言指令 (Language Instruction)</h5> <p>这是任务的“意图（Intent）”。</p> <ul> <li> <strong>Key</strong>: <code class="language-plaintext highlighter-rouge">language_instruction</code> </li> <li> <strong>Value (String)</strong>: <code class="language-plaintext highlighter-rouge">"Pick up the red apple and place it inside the silver cooking pot."</code> </li> <li> <strong>特点</strong>： <ul> <li> <strong>自然语言</strong>：不是由模版生成的僵硬指令（如 “Move to X, Y”），而是人类采集者自己写的或者口述的。这意味着会有同义词、语序变化甚至拼写错误，增加了数据的多样性。</li> <li> <strong>不变性</strong>：通常在整条轨迹中，这个指令是保持不变的（或者包含分解的子指令，但在 DROID 中主要是高层指令）。</li> </ul> </li> </ul> <h5 id="b-视觉观测-visual-observations--机器人的眼睛">B. 视觉观测 (Visual Observations) —— 机器人的“眼睛”</h5> <p>DROID 的核心优势在于多视角。对于每一个时间步 $t$，我们有三张 RGB 图片。</p> <ul> <li> <strong>Key</strong>: <code class="language-plaintext highlighter-rouge">image_primary</code> (或 <code class="language-plaintext highlighter-rouge">cam_high</code>) <ul> <li> <strong>内容</strong>：第三人称视角，俯瞰整个工作台。能看到机器人底座、桌子、苹果和锅的相对位置。</li> <li> <strong>作用</strong>：提供全局上下文（Context），防止局部视野丢失目标。</li> </ul> </li> <li> <strong>Key</strong>: <code class="language-plaintext highlighter-rouge">image_wrist</code> (或 <code class="language-plaintext highlighter-rouge">cam_left_wrist</code>) <ul> <li> <strong>内容</strong>：安装在机械臂手腕上的相机。随着手臂移动而移动。当手抓向苹果时，苹果在画面中会越来越大。</li> <li> <strong>作用</strong>：解决遮挡问题（Occlusion）。当机械臂去抓苹果时，主相机可能被手臂挡住，这时手眼相机是唯一能看清细节的。</li> </ul> </li> <li> <strong>Key</strong>: <code class="language-plaintext highlighter-rouge">image_secondary</code> (或 <code class="language-plaintext highlighter-rouge">cam_varied</code>) <ul> <li> <strong>内容</strong>：另一个第三人称视角，通常在侧面或正面。</li> <li> <strong>作用</strong>：提供立体视差信息，帮助模型理解深度。</li> </ul> </li> <li> <strong>数据格式</strong>：通常是 $(3, H, W)$ 或 $(H, W, 3)$ 的张量，像素值归一化到 $[0, 1]$ 或 $[-1, 1]$。 <ul> <li><em>注：DROID 还包含深度图（Depth Maps），但在训练 Diffusion Policy 时通常只用 RGB。</em></li> </ul> </li> </ul> <h5 id="c-本体感知-proprioception--机器人的触觉体感">C. 本体感知 (Proprioception) —— 机器人的“触觉/体感”</h5> <p>机器人需要知道自己的手臂现在在哪里。</p> <ul> <li> <strong>Key</strong>: <code class="language-plaintext highlighter-rouge">joint_position</code> (关节角度) <ul> <li> <strong>Value</strong>: $\mathbb{R}^7$ 向量（因为 Franka Panda 是 7 自由度机械臂）。</li> <li> <strong>示例</strong>: <code class="language-plaintext highlighter-rouge">[0.12, -0.45, 0.0, -1.2, 0.05, 1.5, 0.8]</code> (弧度制)。</li> </ul> </li> <li> <strong>Key</strong>: <code class="language-plaintext highlighter-rouge">gripper_position</code> (夹爪开度) <ul> <li> <strong>Value</strong>: $\mathbb{R}^1$ 标量。</li> <li> <strong>示例</strong>: <code class="language-plaintext highlighter-rouge">0.08</code> (表示夹爪张开 8cm，处于 Open 状态) 或 <code class="language-plaintext highlighter-rouge">0.0</code> (Closed)。</li> </ul> </li> <li> <strong>Key</strong>: <code class="language-plaintext highlighter-rouge">end_effector_pose</code> (末端执行器位姿) <ul> <li> <strong>Value</strong>: $\mathbb{R}^7$ 向量 (位置 $x,y,z$ + 四元数 $qx,qy,qz,qw$)。</li> <li> <strong>作用</strong>：告诉模型“我的手现在在这个笛卡尔坐标系的什么位置”。</li> </ul> </li> </ul> <h5 id="d-动作-action--模型的预测目标-label">D. 动作 (Action) —— 模型的“预测目标” (Label)</h5> <p>这是<strong>最关键</strong>的部分。在模仿学习中，我们在时间步 $t$，希望模型预测<strong>未来</strong>的动作。</p> <ul> <li> <strong>Key</strong>: <code class="language-plaintext highlighter-rouge">action</code> </li> <li> <strong>Value</strong>: 通常是<strong>目标位姿</strong>或<strong>增量位姿</strong>。在 DROID 的标准训练设置（如 Diffusion Policy）中，Action 通常表示为<strong>下一步的末端执行器位姿变化</strong>或<strong>绝对关节角度</strong>。 <ul> <li>如果使用 <strong>Delta Cartesian Control</strong> (常用)： <ul> <li>Action $\in \mathbb{R}^{6+1}$ (6D 位姿变化 $\Delta x, \Delta y, \dots$ + 1D 夹爪开合)。</li> <li>示例：<code class="language-plaintext highlighter-rouge">[0.01, 0.0, -0.01, 0, 0, 0, 1]</code> —— 意思是“向前移动 1cm，向下移动 1cm，夹爪保持张开”。</li> </ul> </li> </ul> </li> <li> <strong>Action Chunking (动作分块)</strong>: <ul> <li>虽然我们在时间步 $t$，但 Diffusion Policy 不止预测 $t+1$，而是预测未来 $k$ 步的动作序列（例如 $k=16$）。</li> <li>所以，训练时的 Label 实际上是一个序列：$A_{t:t+k}$，形状为 $(16, 7)$。</li> </ul> </li> </ul> <hr> <h4 id="3-具体的代码数据结构演示">3. 具体的代码/数据结构演示</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 一个 DROID 训练 Batch 的结构 (Batch Size = B, Chunk Size = T_pred)
</span>
<span class="n">batch</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># 1. 语言指令 (经过 BERT/T5 编码后的 Embedding)
</span>    <span class="sh">"</span><span class="s">language_embedding</span><span class="sh">"</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="mi">768</span><span class="p">)),</span> 
    <span class="c1"># 对应原文: "Put the apple in the pot"
</span>
    <span class="c1"># 2. 视觉观测 (当前帧 t 的图像)
</span>    <span class="sh">"</span><span class="s">observation</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">image_primary</span><span class="sh">"</span><span class="p">:</span>   <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)),</span> <span class="c1"># 俯视
</span>        <span class="sh">"</span><span class="s">image_wrist</span><span class="sh">"</span><span class="p">:</span>     <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)),</span> <span class="c1"># 手眼
</span>        <span class="sh">"</span><span class="s">proprioception</span><span class="sh">"</span><span class="p">:</span>  <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>            <span class="c1"># 7关节 + 1夹爪
</span>    <span class="p">},</span>

    <span class="c1"># 3. 动作标签 (Ground Truth, 从 t 到 t+k 的未来动作序列)
</span>    <span class="sh">"</span><span class="s">action</span><span class="sh">"</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span> 
    <span class="c1"># 16步长的动作序列。前3维是位置，后3维是旋转(如欧拉角)，最后1维是夹爪
</span><span class="p">}</span>
</code></pre></div></div> <hr> <h4 id="4-为什么-droid-的数据难处理">4. 为什么 DROID 的数据难处理?</h4> <p>DROID 数据处理的难点，体现“In-the-Wild”的挑战：</p> <ol> <li> <strong>光照剧变</strong>：你在 <code class="language-plaintext highlighter-rouge">image_primary</code> 里看到的像素值分布，在不同厨房里完全不同。有的厨房是暖黄光，有的是冷白光，有的甚至有阳光直射造成的过曝。这就要求模型具有极强的<strong>视觉不变性（Visual Invariance）</strong>。</li> <li> <strong>背景杂乱</strong>：在实验室数据中，背景通常是干净的桌子。但在 DROID 中，<code class="language-plaintext highlighter-rouge">image_primary</code> 的背景里可能有洗洁精、抹布、微波炉、甚至走动的人。模型必须学会<strong>注意力机制（Attention）</strong>，只关注指令里提到的“Apple”和“Pot”，忽略旁边的“Dish soap”。</li> <li> <strong>相机抖动</strong>：虽然有外参标定，但推车可能会有微小的晃动，导致外参不仅仅是静态的，这增加了学习坐标映射的难度。</li> </ol> <h2 id="baseline-model">baseline model</h2> <p>作者选择的 Baseline 是 <strong>Diffusion Policy（扩散策略）</strong>。 具体的实现是基于 <strong>Robomimic</strong> 代码库构建的 <strong>CNN-based U-Net Diffusion Policy</strong>。</p> <hr> <h3 id="baseline-模型解构diffusion-policy">Baseline 模型解构——Diffusion Policy</h3> <p>为了验证 DROID 数据的威力，我们不需要发明新的轮子。我们需要的是一辆性能最好的‘赛车’来测试这种‘新燃油’（数据）。在这篇论文中，作者选择了 <strong>Diffusion Policy</strong> [Chi et al., RSS 2023]。为什么选它？因为它目前在模仿学习领域统治力最强，能够处理多模态分布（Multimodal Distribution），非常适合处理复杂的非受控环境数据。</p> <h4 id="1-整体架构-high-level-architecture">1. 整体架构 (High-Level Architecture)</h4> <p>模型可以分为三个主要部分：<strong>感知编码器（Perception Encoders）</strong>、<strong>特征融合（Feature Fusion）</strong> 和 <strong>扩散生成头（Diffusion Generation Head）</strong>。</p> <h5 id="a-感知编码器-the-eyes--ears">A. 感知编码器 (The Eyes &amp; Ears)</h5> <p>模型如何处理多模态输入？</p> <ul> <li> <strong>视觉主干 (Vision Backbone)</strong>： <ul> <li>使用 <strong>ResNet-50</strong>。</li> <li> <strong>预训练</strong>：使用标准的 <strong>ImageNet</strong> 预训练权重。这意味着模型在看到机器人数据之前，已经认识了“边缘”、“纹理”和一般的物体特征。</li> <li> <strong>输入处理</strong>：图像被调整为 $128 \times 128$ 分辨率。为了增加鲁棒性，训练时使用了随机裁剪（Random Crop）和颜色抖动（Color Jitter）作为数据增强。 <alphaxiv-paper-citation title="Visual Encoder" page="18" first="ResNet-50 visual encoder" last="of the visual inputs."></alphaxiv-paper-citation> </li> </ul> </li> <li> <strong>语言主干 (Language Backbone)</strong>： <ul> <li>使用 <strong>DistilBERT</strong>。</li> <li> <strong>冻结 (Frozen)</strong>：在训练过程中，DistilBERT 的参数是不更新的。它只负责把自然语言指令（如 “Put the apple in the pot”）转换成一个固定的语义向量（Embedding）。 <alphaxiv-paper-citation title="Language Encoder" page="18" first="frozen DistilBERT [45]" last="language embedding"></alphaxiv-paper-citation> </li> </ul> </li> </ul> <h5 id="b-特征融合-feature-aggregation">B. 特征融合 (Feature Aggregation)</h5> <ul> <li> <strong>拼接 (Concatenation)</strong>： <ul> <li>作者没有使用复杂的 Cross-Attention，而是简单粗暴但有效地使用了<strong>拼接</strong>。</li> <li>将 ResNet-50 提取的视觉特征向量、DistilBERT 提取的语言向量、以及机器人的本体感知（关节角度）向量，直接拼接成一个长向量。</li> <li>这个长向量随后通过一个多层感知机（MLP, specifically <code class="language-plaintext highlighter-rouge">[1024, 512, 512]</code>）进行降维和特征融合。 <alphaxiv-paper-citation title="MLP Fusion" page="18" first="concatenated features are" last="Processing MLP"></alphaxiv-paper-citation> </li> </ul> </li> </ul> <h5 id="c-扩散生成头-diffusion-head--核心大脑">C. 扩散生成头 (Diffusion Head) —— 核心大脑</h5> <p>这是模型最关键的部分。它不是直接输出动作 $a$，而是学习<strong>去噪（Denoising）</strong>。</p> <ul> <li> <strong>架构</strong>：<strong>Conditional U-Net</strong>。 <ul> <li>这是一个 1D U-Net（处理时间序列）。</li> <li>它接收两个输入： <ol> <li> <strong>带噪声的动作序列</strong> (Noisy Action Sequence, $A_k$)。</li> <li> <strong>条件特征</strong> (Conditioning Features, 来自上面的 MLP 输出)。</li> </ol> </li> </ul> </li> <li> <strong>任务</strong>：预测噪声 $\epsilon$。即给定当前状态特征和带噪声的动作，预测加了多少噪声，从而反向推导出干净的动作。</li> </ul> <hr> <h4 id="2-输入与输出-inputs--outputs">2. 输入与输出 (Inputs &amp; Outputs)</h4> <p>这定义了模型实际上在“吃”什么和“产”什么。</p> <ul> <li> <strong>输入 (Observation Horizon = 2)</strong>： <ul> <li>模型不仅看当前帧 $t$，还看前一帧 $t-1$。</li> <li>这意味着输入张量包含了历史信息，有助于推断速度和加速度。 <alphaxiv-paper-citation title="Observation Horizon" page="18" first="Observation Horizon" last="2"></alphaxiv-paper-citation> </li> </ul> </li> <li> <strong>输出 (Prediction Horizon = 16)</strong>： <ul> <li> <strong>Action Chunking（动作分块）</strong>：模型一次性生成未来 <strong>16 步</strong> 的动作序列 $(a_t, a_{t+1}, \dots, a_{t+15})$。</li> <li> <strong>执行策略 (Receding Horizon Control)</strong>： <ul> <li>虽然预测了 16 步，但机器人只执行前 <strong>8 步</strong>（Action Horizon = 8）。</li> <li>执行完 8 步后，重新推理，再预测 16 步，再执行 8 步。</li> <li>为什么要这样？这增加了动作的连贯性（Smoothness），并允许模型以比控制频率更低的频率进行推理（节省算力）。* <alphaxiv-paper-citation title="Action Horizon" page="18" first="Prediction Horizon 16" last="Action Horizon 8"></alphaxiv-paper-citation> </li> </ul> </li> </ul> </li> </ul> <hr> <h4 id="3-训练流程-training-process">3. 训练流程 (Training Process)</h4> <ol> <li> <strong>采样</strong>：从 Dataset 中随机抽取一段长度为 16 的真实动作序列 $A_{gt}$。</li> <li> <strong>加噪</strong>：在 $A_{gt}$ 上添加高斯噪声 $\epsilon \sim \mathcal{N}(0, I)$，得到 $A_{noisy}$。</li> <li> <strong>前向传播</strong>： <ul> <li>图片/语言 $\rightarrow$ ResNet/BERT $\rightarrow$ 特征向量 $Z$。</li> <li>U-Net 接收 $(A_{noisy}, Z, \text{timestep } k)$。</li> </ul> </li> <li> <strong>预测</strong>：U-Net 输出预测的噪声 $\hat{\epsilon}$。</li> <li> <table> <tbody> <tr> <td> <strong>Loss 计算</strong>：计算 MSE Loss: $\mathcal{L} =</td> <td> </td> <td>\epsilon - \hat{\epsilon}</td> <td> </td> <td>^2$。</td> </tr> </tbody> </table> </li> <li> <strong>反向传播</strong>：更新 ResNet 和 U-Net 的参数（DistilBERT 冻结）。</li> </ol> <p><strong>关键超参数</strong>：</p> <ul> <li> <strong>Batch Size</strong>: 128</li> <li> <strong>Learning Rate</strong>: 1e-4 (with Linear Scheduler)</li> <li> <strong>Optimizer</strong>: AdamW</li> <li> <strong>Training Steps</strong>: 25,000 ~ 50,000 步（相对较短，说明预训练特征很有效）。 <alphaxiv-paper-citation title="Hyperparameters" page="18" first="Batch Size 128" last="Train Steps 25000"></alphaxiv-paper-citation> </li> </ul> <hr> <h4 id="4-为什么选择这个-baselinewhy-this-model">4. 为什么选择这个 Baseline？(Why This Model?)</h4> <ol> <li> <strong>处理多模态分布</strong>：机器人操作中，同一个指令（”拿起杯子”）可能有多种合法的动作（左手拿、右手拿、先推后拿）。传统的 MSE Regression 会输出这几种动作的“平均值”，导致机器人卡在中间不动。Diffusion Policy 能表达<strong>多峰分布（Multimodal Distribution）</strong>，这是处理 DROID 这种多样化数据的关键。</li> <li> <strong>稳定性</strong>：相比于 GAN 或 Flow-based models，Diffusion 训练更稳定，超参数敏感度更低。</li> <li> <strong>标准化</strong>：作者的目标是评测数据。使用社区公认的 SOTA 模型，排除了“你的模型太弱导致效果不好”的质疑。</li> </ol> <hr> <h3 id="总结">总结</h3> <p>“简单来说，Baseline 就是一个<strong>‘看图说话’的生成模型</strong>。它看着当前的 RGB 图像和语言指令，并在脑海中（U-Net）通过去噪过程，想象出未来 1秒钟（16步）内手该怎么动。DROID 的作用，就是提供了海量的‘图-文-动作’对，让这个模型见多识广，不再因为换了个桌布就不知道手往哪放了。”</p> <h2 id="注意特征融合不是按channel拼接">注意特征融合不是按channel拼接</h2> <p>在 DROID 使用的这套 Diffusion Policy 架构中，融合是发生在<strong>一维向量（1D Vector）</strong>层面的。</p> <p>接下来详细拆解一下这个过程，这对于理解如何处理“图像+语言+本体”这种异构数据非常关键。</p> <h3 id="1-为什么不能直接按-channel-拼接">1. 为什么不能直接按 Channel 拼接？</h3> <p>按 Channel 拼接（例如 <code class="language-plaintext highlighter-rouge">torch.cat([feat1, feat2], dim=1)</code>）通常要求两个特征图在<strong>空间维度（Height, Width）</strong>上是一致的。</p> <p>但在 DROID 的设置中：</p> <ul> <li> <strong>视觉特征</strong>：来自 ResNet，原本是有空间结构的。</li> <li> <strong>语言特征</strong>：来自 BERT，是一个语义向量，没有 $H \times W$ 的空间概念。</li> <li> <strong>本体感知</strong>：是关节角度（如 7 个浮点数），更没有空间概念。</li> </ul> <p>你无法把一个 $7 \times 1$ 的关节角度向量直接拼到一个 $128 \times 128$ 的特征图后面。</p> <h3 id="2-droid-的融合方式扁平化拼接-flat-concatenation">2. DROID 的融合方式：扁平化拼接 (Flat Concatenation)</h3> <p>具体的融合流程如下：</p> <h4 id="第一步特征向量化-vectorization">第一步：特征“向量化” (Vectorization)</h4> <p>首先，所有的模态都被压缩或映射成了<strong>一维向量</strong>。</p> <ol> <li> <strong>视觉 (Vision)</strong>： <ul> <li>$128 \times 128$ 的图像经过 ResNet-50。</li> <li>ResNet 的末端通常会接一个 <strong>Global Average Pooling (全局平均池化)</strong> 或者 <strong>Spatial Softmax</strong>（Robomimic 的常用做法）。</li> <li>结果：变成了形状为 $(B, D_{vision})$ 的一维向量（例如 $D=2048$ 或 $D=512$）。<alphaxiv-paper-citation title="Visual Embeddings" page="18" first="produce embeddings for" last="visual inputs."></alphaxiv-paper-citation> </li> </ul> </li> <li> <strong>语言 (Language)</strong>： <ul> <li>经过 DistilBERT，取 <code class="language-plaintext highlighter-rouge">[CLS]</code> token 或平均池化。</li> <li>结果：形状为 $(B, 768)$ 的一维向量。</li> </ul> </li> <li> <strong>本体 (Proprioception)</strong>： <ul> <li>本身就是形状为 $(B, D_{prop})$ 的一维向量（如关节角+夹爪状态）。</li> </ul> </li> </ol> <h4 id="第二步直接拼接-direct-concatenation">第二步：直接拼接 (Direct Concatenation)</h4> <p>将上述三个向量在<strong>特征维度</strong>上直接串联起来。</p> \[E_{total} = [E_{vision\_cam1}, E_{vision\_cam2}, E_{vision\_wrist}, E_{language}, E_{prop}]\] <p>拼接后的向量会非常长。假设视觉是 512 维，用了 3 个相机，语言 768 维，本体 8 维： \(\text{Length} \approx 512 \times 3 + 768 + 8 = 2312\)</p> <alphaxiv-paper-citation title="Concatenation" page="18" first="These embeddings are" last="observation keys."></alphaxiv-paper-citation> <h4 id="第三步mlp-融合-mlp-fusion">第三步：MLP 融合 (MLP Fusion)</h4> <p>这个超长的拼接向量 $E_{total}$ 随后被送入一个 <strong>Observation Processing MLP</strong>（多层感知机）。</p> <ul> <li> <strong>结构</strong>：<code class="language-plaintext highlighter-rouge">[1024, 512, 512]</code> </li> <li> <strong>作用</strong>：将不同模态的信息进行非线性混合，并压缩维度。</li> <li> <strong>最终输出</strong>：一个紧凑的条件特征向量（Conditioning Vector），这个向量会被送入 Diffusion U-Net 去控制噪声的预测。</li> </ul> <alphaxiv-paper-citation title="MLP Architecture" page="18" first="fed through an" last="Processing MLP"></alphaxiv-paper-citation> <h3 id="3-这种做法的优缺点">3. 这种做法的优缺点</h3> <ul> <li> <strong>优点</strong>： <ul> <li> <strong>简单高效</strong>：工程实现非常简单，计算量也比 Cross-Attention 小。</li> <li> <strong>全局信息整合</strong>：MLP 层能很好地学习到“当语言是 X 且 图像是 Y 时”的非线性关系。</li> </ul> </li> <li> <strong>缺点</strong>： <ul> <li> <strong>丢失空间信息</strong>：这是最大的痛点。当你把 ResNet 的 Feature Map 池化成一个向量时，你丢失了“苹果在图片的左上角”这种精确的空间坐标信息。</li> <li><em>注：虽然 Spatial Softmax 可以保留一定的坐标信息，但相比于保留完整的 2D Feature Map，信息损失依然很大。</em></li> </ul> </li> <li> <strong>替代方案 (SOTA 方向)</strong>： <ul> <li>现在的 Transformer-based 策略（如 RT-2, ACT）倾向于保留 Visual Tokens（即不池化），然后用 <strong>Cross-Attention</strong> 来融合语言和图像。这允许模型在像素级别关注特定的物体。</li> </ul> </li> </ul> <p><strong>总结</strong>：DROID 的 Baseline 采用的是<strong>一维向量拼接（Vector Concatenation）</strong>，这是为了适配随后全连接层（MLP）的处理方式。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/DriveJEPA/">DriveJEPA</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/C_RADIOv4/">C_RADIOv4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/GeRo/">GeRo</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 P W Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>