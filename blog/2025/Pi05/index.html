<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pi05 | Tenacious life, proud journey. </title> <meta name="author" content="P W Name"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://beyondpzk.github.io/blog/2025/Pi05/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Tenacious life, proud journey. </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Pi05</h1> <p class="post-meta"> Created on April 22, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/vla"> <i class="fa-solid fa-tag fa-sm"></i> VLA</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>[TOC]</p> <h1 id="pi_05-a-vision-language-action-model-with-open-world-generalization">$\pi_{0.5}$: a Vision-Language-Action Model with Open-World Generalization</h1> <p><a href="https://arxiv.org/abs/2504.16054" rel="external nofollow noopener" target="_blank">paper link</a></p> <h2 id="先快速对比一下-pi_0">先快速对比一下 $\pi_{0}$</h2> <p>目前的演进路线主要包含两个关键节点：<strong>基础模型 $\pi_0$ (Pi-Zero)</strong> 和 <strong>增强泛化能力的 $\pi_{0.5}$ (Pi-Zero-Point-Five)</strong>。</p> <p>这篇论文明确指出 $\pi_{0.5}$ 是基于 $\pi_0$ 构建的，二者的演进逻辑并非简单的“参数量增加”，而是从<strong>通用的物理操作能力</strong>向<strong>开放世界的语义理解与长程规划能力</strong>的跃迁。</p> <p>以下是演进路线：</p> <h3 id="1-基石pi_0-the-foundation">1. 基石：$\pi_0$ (The Foundation)</h3> <ul> <li> <strong>定位：</strong> 通用基础 VLA（视觉-语言-动作）模型。</li> <li> <strong>核心能力：</strong> $\pi_0$ 旨在解决<strong>通用的物理灵巧性（Dexterity）</strong>。它能够控制多种不同的机器人形态（如双臂机器人、单臂机器人、移动底盘等），执行各种低层物理动作。</li> <li> <strong>架构特点：</strong> 采用基于 Flow Matching（流匹配）的策略来生成连续的动作轨迹，具有极强的物理执行能力。</li> <li> <strong>局限性：</strong> 尽管 $\pi_0$ 在执行特定动作上非常强，但单纯的 $\pi_0$ 在面对完全陌生的环境（如从未见过的厨房）或需要长时间序列推理（如“打扫整个房间”）时，可能缺乏足够的高层语义规划能力。</li> </ul> <h3 id="2-进化pi_05-the-generalist">2. 进化：$\pi_{0.5}$ (The Generalist)</h3> <ul> <li> <strong>定位：</strong> 面向<strong>开放世界泛化（Open-World Generalization）</strong>的增强版模型。</li> <li> <strong>演进动力：</strong> 解决“机器人走出实验室”的问题。仅仅会“抓取”是不够的，机器人需要知道“在乱糟糟的房间里先抓哪个、放到哪里、为什么要这么做”。</li> <li> <strong>关键演进点（相对 $\pi_0$）：</strong> <ol> <li> <strong>引入高层语义推理（High-Level Reasoning）：</strong> $\pi_{0.5}$ 不仅输出电机指令，还会在每一步先预测一个“语义子任务”（Semantic Subtask，如“打开柜子”）。这就像给 $\pi_0$ 装上了一个“大脑”，让它能进行思维链推理。 <alphaxiv-paper-citation title="Architecture" page="2" first="predicts the semantic subtask" last="low-level robot action"></alphaxiv-paper-citation> </li> <li> <strong>异构协同训练（Heterogeneous Co-training）：</strong> $\pi_{0.5}$ 的训练数据不再局限于机器人操作数据，而是大量引入了网络数据（VQA、定位）、其他形态机器人的数据以及人类的口头指导数据。这使得模型获得了超越物理经验的“常识”。 <alphaxiv-paper-citation title="Data Strategy" page="2" first="overwhelming majority of training" last="from these other sources"></alphaxiv-paper-citation> </li> <li> <strong>长程任务能力：</strong> $\pi_0$ 可能擅长 10 秒的抓取，而 $\pi_{0.5}$ 被设计用来执行 10-15 分钟的复杂任务（如整理床铺、清洁厨房）。 <alphaxiv-paper-citation title="Long Horizon" page="1" first="performing complex multi-stage" last="10 to 15 minutes."></alphaxiv-paper-citation> </li> </ol> </li> </ul> <h3 id="演进路线图">演进路线图</h3> <table> <thead> <tr> <th style="text-align: left">特性</th> <th style="text-align: left">$\pi_0$ (基座)</th> <th style="text-align: left">$\pi_{0.5}$ (当前版本)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>核心目标</strong></td> <td style="text-align: left"> <strong>怎么做 (How)</strong>：解决物理控制与灵巧操作</td> <td style="text-align: left"> <strong>做什么 (What &amp; Why)</strong>：解决语义理解与任务规划</td> </tr> <tr> <td style="text-align: left"><strong>训练数据</strong></td> <td style="text-align: left">主要是机器人轨迹数据</td> <td style="text-align: left">机器人数据 + <strong>海量网络数据 + 语义标注 + 口头指令</strong> </td> </tr> <tr> <td style="text-align: left"><strong>推理模式</strong></td> <td style="text-align: left">观察 $\to$ 动作</td> <td style="text-align: left">观察 $\to$ <strong>语义意图</strong> $\to$ 动作</td> </tr> <tr> <td style="text-align: left"><strong>应用场景</strong></td> <td style="text-align: left">实验室环境、单一任务</td> <td style="text-align: left"> <strong>全新家庭环境 (In the wild)</strong>、长程家务任务</td> </tr> </tbody> </table> <p>可以将 $\pi_0$ 想象成一个 <strong>“肢体发达”</strong>的运动员，它的肌肉记忆（Motor Control）非常完美；而 $\pi_{0.5}$ 则是给这个运动员配备了<strong>“生活常识”</strong>和<strong>“任务规划手册”</strong>，让他不仅能跑能跳，还能理解“把家里打扫干净”意味着要把脏衣服放进篮子而不是扔在地上。这篇论文正是通过 $\pi_{0.5}$ 展示了如何通过数据和架构的调整，让机器人模型从“专才”走向“通才”。</p> <h2 id="notes-of-pi_05">Notes of $\pi_{0.5}$</h2> <p>在过去的几年里，以深度学习为基础的系统在自然语言处理（NLP）和计算机视觉（CV）领域取得了规模化的成功。然而，物理智能（Physical Intelligence）——即智能体在物理世界中执行复杂任务的能力——仍然面临着巨大的挑战。其中最核心的难题之一，就是 <strong>开放世界泛化（Open-World Generalization）</strong>。 《$\pi_{0.5}$: a Vision-Language-Action Model with Open-World Generalization》。这篇论文由 Physical Intelligence 公司发布，它展示了一个端到端的学习系统，能够控制移动操作机器人在从未见过的家庭环境中执行长周期的家务任务。</p> <hr> <h3 id="第一模块核心动机与挑战-introduction--motivation">第一模块：核心动机与挑战 (Introduction &amp; Motivation)</h3> <h4 id="11-机器人学习的泛化墙">1.1 机器人学习的“泛化墙”</h4> <p>在传统的机器人研究中，我们在实验室受控环境下训练机器人，它们往往表现优异。但是，一旦我们将机器人移出实验室，面对现实世界中纷繁复杂的场景、光照、物体布局，系统的鲁棒性往往会急剧下降。</p> <p>论文开篇引用了雷·布拉德伯里（Ray Bradbury）的名言：“Stuff your eyes with wonder… See the world.” 这不仅是文学上的修辞，更点出了物理智能的愿景：具身系统只有离开实验室，处理现实世界中的意外事件，才真正有用。 <alphaxiv-paper-citation title="Introduction" page="1" first="Open-world generalization represents" last="in the real world."></alphaxiv-paper-citation></p> <h4 id="12-视觉-语言-动作vla模型的兴起">1.2 视觉-语言-动作（VLA）模型的兴起</h4> <p>为了解决泛化问题，学术界引入了视觉-语言-动作模型（VLA）。VLA通过将预训练的视觉-语言模型（VLM）微调用于机器人控制，试图利用互联网规模的语义知识。然而，现有的VLA模型虽然展示了惊人的语言遵循能力，但主要还是在与训练数据非常相似的环境中进行评估。</p> <p>本论文提出的 $\pi_{0.5}$ 模型，基于 $\pi_0$ 架构，旨在回答一个核心问题：这种模型在野外（in the wild）究竟能泛化到什么程度？ <alphaxiv-paper-citation title="Abstract" page="1" first="While vision-language-action (VLA)" last="generalize in the wild."></alphaxiv-paper-citation></p> <h4 id="13-pi_05-的核心突破">1.3 $\pi_{0.5}$ 的核心突破</h4> <p>$\pi_{0.5}$ 的核心贡献在于它不仅使用了移动操作机器人的数据，还引入了异构任务的协同训练（co-training）。这使得机器人能够在从未见过的家庭中执行长达10到15分钟的复杂多阶段行为，例如清洁厨房或卧室。 <alphaxiv-paper-citation title="Performance" page="1" first="performing complex multi-stage" last="10 to 15 minutes."></alphaxiv-paper-citation></p> <hr> <h3 id="第二模块异构数据策略-heterogeneous-data-strategy">第二模块：异构数据策略 (Heterogeneous Data Strategy)</h3> <p>这篇论文最引人注目的地方在于其数据配方。为了实现类似人类的泛化能力——即利用间接经验、书本知识和类比推理——$\pi_{0.5}$ 并没有单纯依赖大规模的特定机器人数据。</p> <h4 id="21-数据来源的多样性">2.1 数据来源的多样性</h4> <p>$\pi_{0.5}$ 的训练数据包含以下几类：</p> <ol> <li> <strong>移动操作机器人数据（Mobile Manipulator Data）：</strong> 直接在真实家庭中收集，约400小时。</li> <li> <strong>静态机器人数据（Static Robot Data）：</strong> 来自实验室或其他环境的非移动机械臂数据。</li> <li> <strong>高层语义预测（High-Level Semantic Prediction）：</strong> 预测下一步应该做什么（例如“捡起盘子”）。</li> <li> <strong>网络数据（Web Data）：</strong> 包括图像描述、问答和物体定位。</li> <li> <strong>口头指令（Verbal Instructions）：</strong> 人类监督员逐步指导机器人的数据。</li> </ol> <p>令人惊讶的是，在第一阶段训练中，提供给 $\pi_{0.5}$ 的绝大多数训练样本（97.6%）并非来自移动操作机器人执行家务任务，而是来自其他机器人或网络数据。 <alphaxiv-paper-citation title="Data Sources" page="2" first="The overwhelming majority" last="from the web."></alphaxiv-paper-citation></p> <h4 id="22-为什么需要异构数据">2.2 为什么需要异构数据？</h4> <p>这种策略背后的逻辑是知识迁移。</p> <ul> <li> <strong>动作迁移：</strong> 移动机器人可以从其他静态机器人的数据中学习低层控制策略（如抓取）。</li> <li> <strong>语义迁移：</strong> 高层推理过程可以从网络数据和语义标注中受益。</li> <li> <strong>指令理解：</strong> 通过人类监督员像教人一样教机器人，模型学习了如何将复杂任务分解。 <alphaxiv-paper-citation title="Reasoning" page="3" first="instructing it (much" last="completing a complex task"></alphaxiv-paper-citation> </li> </ul> <hr> <h3 id="第三模块分层架构与推理-hierarchical-architecture--inference">第三模块：分层架构与推理 (Hierarchical Architecture &amp; Inference)</h3> <p>$\pi_{0.5}$ 采用了一种简单而有效的分层架构设计，这种设计与其异构数据策略紧密结合。</p> <h4 id="31-两阶段推理过程">3.1 两阶段推理过程</h4> <p>在推理时的每一步，模型执行以下两个阶段：</p> <ol> <li> <strong>语义子任务预测（High-Level Semantic Action）：</strong> 首先，模型根据任务结构和场景语义，预测下一个适当的行为（Subtask），例如“把物品放入抽屉”或“重新整理枕头”。</li> <li> <strong>低层动作生成（Low-Level Action Chunk）：</strong> 基于预测出的子任务，模型生成具体的机器人动作块。</li> </ol> <p>这种架构使得系统能够兼顾长周期的多阶段任务推理和精细的动作执行。 <alphaxiv-paper-citation title="Architecture" page="2" first="At runtime, during" last="level robot action chunk"></alphaxiv-paper-citation></p> <h4 id="32-统一模型的优势">3.2 统一模型的优势</h4> <p>与许多使用两个独立模型（一个VLM做规划，一个Policy做执行）的方法不同，$\pi_{0.5}$ 使用同一个模型进行高层和低层推理。这类似于思维链（Chain-of-Thought）或测试时计算（Test-time Compute）的方法。低层推理受益于其他机器人的动作数据，而高层推理则受益于网络语义数据。 <alphaxiv-paper-citation title="Design" page="2" first="This simple architecture" last="two levels: the low-level"></alphaxiv-paper-citation></p> <hr> <h3 id="第四模块实验评估-experimental-evaluation">第四模块：实验评估 (Experimental Evaluation)</h3> <p>实验设计旨在验证模型在“全新环境”中的表现。研究团队构建了模拟环境（Mock Environments）用于定量对比，并在真实家庭（Real Homes）中进行了最终测试。</p> <h4 id="41-评估环境设置">4.1 评估环境设置</h4> <ul> <li> <strong>模拟房间：</strong> 用于可复现的定量评估。</li> <li> <strong>真实家庭：</strong> 包含从未在训练数据中出现的全新厨房和卧室，具有新颖的物体、背景和布局。 <alphaxiv-paper-citation title="Environments" page="8" first="We evaluate pi0.5" last="and layouts."></alphaxiv-paper-citation> </li> </ul> <h4 id="42-定量与定性结果">4.2 定量与定性结果</h4> <p>在真实家庭的评估中，机器人被要求执行如“把物品放入抽屉”、“把盘子放入水槽”等任务。结果显示，$\pi_{0.5}$ 能够成功完成这些任务。图表显示，模型在模拟环境中的表现与其在真实家庭中的表现具有代表性一致性。 <alphaxiv-paper-citation title="Results" page="8" first="find that pi0.5's" last="in real homes."></alphaxiv-paper-citation></p> <p>特别值得注意的是，$\pi_{0.5}$ 展示了在完全陌生的家庭中清洁厨房或卧室的能力，这是之前的VLA模型难以企及的。 <alphaxiv-paper-citation title="Capability" page="3" first="demonstrate an end-to-end" last="in entirely new homes."></alphaxiv-paper-citation></p> <h4 id="43-消融实验什么才是关键">4.3 消融实验：什么才是关键？</h4> <p>为了理解各个组件的重要性，论文进行了详细的消融研究（Ablation Study）。以下是几个关键发现：</p> <ol> <li> <strong>高层推理的重要性：</strong> 完整的 $\pi_{0.5}$ 模型（包含显式的高层和低层推理）表现最佳。</li> <li> <strong>隐式高层的意外发现：</strong> 有趣的是，“Implicit HL”设置（训练时包含高层数据，但推理时不输出子任务）是表现第二好的模型。这表明，仅仅在训练混合数据中包含子任务预测数据，就能显著提升模型性能。 <alphaxiv-paper-citation title="Ablation" page="11" first="predicting subtask labels" last="in the training mixture."></alphaxiv-paper-citation> </li> <li> <strong>口头指令的关键性：</strong> 去除口头指令数据（No VI）会导致性能显著下降。尽管这部分数据仅占高层移动操作样本的11%，但它对模型理解复杂任务至关重要。 <alphaxiv-paper-citation title="Verbal Instructions" page="11" first="which only constitutes" last="significantly weaker."></alphaxiv-paper-citation> </li> <li> <strong>GPT-4 的局限性：</strong> 直接使用 GPT-4 作为高层策略（Zero-shot）的表现最差，证明了使用机器人数据适配 VLM 的必要性。 <alphaxiv-paper-citation title="GPT-4 Comparison" page="11" first="GPT-4 ablation attains" last="with robot data."></alphaxiv-paper-citation> </li> </ol> <hr> <h3 id="第五模块局限性与未来展望-limitations--future-work">第五模块：局限性与未来展望 (Limitations &amp; Future Work)</h3> <h4 id="51-存在的挑战">5.1 存在的挑战</h4> <p>论文坦诚地讨论了失败案例：</p> <ul> <li> <strong>物理交互难点：</strong> 模型在处理不熟悉的抽屉把手，或者物理上难以打开的柜子时会遇到困难。</li> <li> <strong>部分可观测性（Partial Observability）：</strong> 例如，机械臂可能会遮挡住需要擦拭的污渍。</li> <li> <strong>高层推理干扰：</strong> 有时高层推理会被干扰，导致机器人重复打开和关闭抽屉。 <alphaxiv-paper-citation title="Limitations" page="11" first="Some environments present" last="away items)."></alphaxiv-paper-citation> </li> </ul> <h4 id="52-未来方向">5.2 未来方向</h4> <p>未来的工作可能会集中在解决这些感知和物理交互的限制上。此外，论文提到 $\pi_{0.5}$ 处理的提示词（Prompts）相对简单。未来的系统需要结合更丰富的上下文和记忆能力，以处理跨房间导航或记忆物体位置等更复杂的任务。 <alphaxiv-paper-citation title="Future Work" page="11" first="incorporating richer context" last="objects are stored."></alphaxiv-paper-citation></p> <h3 id="小结">小结</h3> <p>$\pi_{0.5}$ 并不单纯是数据规模的胜利，而是<strong>数据多样性</strong>和<strong>架构设计</strong>的胜利。它证明了通过混合协同训练（Co-training），我们可以利用非机器人数据（如网络数据）和非特定任务数据（如其他机器人数据）来弥补特定场景数据的不足，从而跨越“泛化墙”，迈向真正的物理智能。</p> <h2 id="关键点-co-training的细节">关键点 Co-Training的细节</h2> <p>这是现代大规模多模态训练（Multi-modal Training）中处理异构数据（Heterogeneous Data）的标准做法。</p> <h3 id="1-你的模型batch-内部的分治法">1. 你的模型：Batch 内部的“分治法”</h3> <p>在 $\pi_{0.5}$ 的训练中，一个 Batch（比如 size=100）确实是一个混合体。就像你说的：</p> <ul> <li> <strong>前 50 条数据（非机器人任务，如 VQA/Caption）：</strong> <ul> <li> <strong>输入：</strong> 图片 + 问题（如“图里有什么？”）</li> <li> <strong>目标：</strong> 文本回答（如“一个苹果”）</li> <li> <strong>Loss 计算：</strong> 计算交叉熵损失（Cross-Entropy Loss）。</li> <li> <strong>关键点：</strong> 对于这 50 条数据，<strong>Flow Matching Loss 会被强制置为 0（Masked out）</strong>。因为这些数据没有物理动作标签，我们不想让模型去“瞎猜”一个动作，从而干扰动作生成的权重。</li> </ul> </li> <li> <strong>后 50 条数据（机器人任务）：</strong> <ul> <li> <strong>输入：</strong> 机器人视角图片 + 指令（如“捡起苹果”）</li> <li> <strong>目标：</strong> <ol> <li> <strong>语义子任务（Subtask）：</strong> 文本（如 <code class="language-plaintext highlighter-rouge">&lt;pick_object&gt;</code>）</li> <li> <strong>物理动作（Action）：</strong> 连续轨迹</li> </ol> </li> <li> <strong>Loss 计算：</strong> <ul> <li> <strong>部分 A：</strong> 针对子任务文本，计算 <strong>交叉熵损失</strong>。</li> <li> <strong>部分 B：</strong> 针对动作轨迹，计算 <strong>Flow Matching Loss</strong>。</li> </ul> </li> <li> <strong>关键点：</strong> $\pi_{0.5}$ 的独特之处在于，机器人数据<strong>同时也贡献了文本 Loss</strong>（因为要预测子任务），这是它与普通 VLA 不同的地方。 <alphaxiv-paper-citation title="Hierarchical Architecture" page="2" first="predicts the semantic subtask" last="low-level robot action chunk"></alphaxiv-paper-citation> </li> </ul> </li> </ul> <h3 id="2-数学上的总-loss">2. 数学上的“总 Loss”</h3> <p>对于整个 Batch，最终的 Loss 是所有样本 Loss 的平均值。但在计算梯度时，是根据样本类型加权的。</p> <p>我们可以写成一个通用的公式：</p> \[L_{\text{total}} = \frac{1}{N} \sum_{i=1}^{N} \left( \mathbb{I}_{i \in \text{Text}} \cdot L_{\text{CE}}^{(i)} + \mathbb{I}_{i \in \text{Robot}} \cdot (L_{\text{CE\_Subtask}}^{(i)} + \lambda L_{\text{Flow}}^{(i)}) \right)\] <p>其中：</p> <ul> <li>$\mathbb{I}$ 是指示函数（Indicator Function）：如果是该类任务则为 1，否则为 0。这就是所谓的<strong>掩码（Mask）</strong>。</li> <li>对于 VQA 数据，只有第一项生效。</li> <li>对于机器人数据，后两项生效。</li> </ul> <h3 id="3-为什么要混在一个-batch-里shared-backbone">3. 为什么要混在一个 Batch 里？（Shared Backbone）</h3> <p>你可能会问，既然 Loss 是分开算的，为什么不干脆训练两个模型？</p> <p>原因在于<strong>梯度回传（Backpropagation）</strong>。</p> <p>虽然 Loss 是在最后分开算的，但梯度会沿着网络反向传播，最终汇聚到<strong>同一个</strong>视觉编码器（Vision Encoder）和<strong>同一个</strong> Transformer 主干（Backbone）上。</p> <ul> <li>当你用 VQA 数据更新参数时，你在教视觉编码器：“看，这个圆圆红红的东西叫苹果”。</li> <li>当你用机器人数据更新参数时，你在教同一个视觉编码器：“看到这个圆圆红红的东西（苹果），机械臂要伸过去”。</li> </ul> <p>通过在同一个 Batch 里混合训练，视觉编码器学会了提取<strong>既能回答问题、又能指导动作</strong>的通用特征。这就是为什么 $\pi_{0.5}$ 能泛化的核心原因。 <alphaxiv-paper-citation title="Co-training" page="2" first="co-training framework for VLAs" last="enable broad generalization."></alphaxiv-paper-citation></p> <h3 id="4-特殊设计action-expert-动作专家">4. 特殊设计：Action Expert (动作专家)</h3> <p>还有一个细节需要补充：$\pi_{0.5}$ 在架构上使用了 <strong>Action Expert</strong>。</p> <p>虽然主干网络是共享的，但在最后输出层，处理动作数据的部分是一个独立的小型网络（Expert）。</p> <ul> <li> <strong>文本数据</strong> 只走 Text Head。</li> <li> <strong>机器人动作数据</strong> 会走 Action Expert Head。</li> </ul> <p>这意味着，虽然大家共用大脑（Backbone），但“说话”用的是嘴（Text Head），“动手”用的是手（Action Expert）。这种设计防止了动作生成的特殊需求（连续值、高精度）干扰到语言生成的逻辑。 <alphaxiv-paper-citation title="Action Expert" page="4" first="use a different set" last="the rest of the LLM"></alphaxiv-paper-citation></p> <h3 id="小结-1">小结</h3> <p>Batch 混合 + Loss 掩码是 VLA 训练的标准操作。而 $\pi_{0.5}$ 的特殊之处在于：</p> <ol> <li> <strong>机器人数据也算文本 Loss</strong>（为了预测子任务）。</li> <li> <strong>使用了 Action Expert</strong>（专门的动作输出头）。</li> </ol> <h2 id="对比自动驾驶中的先决策后执行逻辑">对比自动驾驶中的”先决策后执行”逻辑</h2> <p>$\pi_{0.5}$ 的架构本质上就是一种<strong>分层决策（Hierarchical Decision Making）</strong>，这在自动驾驶领域是非常经典的范式。</p> <p>我们可以把自动驾驶的 <strong>“规划-控制”</strong>流程与 $\pi_{0.5}$ 的流程做一个严丝合缝的映射：</p> <h3 id="1-自动驾驶-vs-pi_05核心流程映射">1. 自动驾驶 vs. $\pi_{0.5}$：核心流程映射</h3> <table> <thead> <tr> <th style="text-align: left">阶段</th> <th style="text-align: left">自动驾驶 (AD) 的类比</th> <th style="text-align: left">$\pi_{0.5}$ (本论文) 的实现</th> <th style="text-align: left">作用 (Why?)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Input</strong></td> <td style="text-align: left">摄像头/激光雷达看到路口</td> <td style="text-align: left">机器人看到乱糟糟的厨房</td> <td style="text-align: left">感知环境状态</td> </tr> <tr> <td style="text-align: left"><strong>High-Level Decision</strong></td> <td style="text-align: left"> <strong>行为规划 (Behavior Planning)</strong><br>决策：“左转”、“变道”、“减速让行”</td> <td style="text-align: left"> <strong>语义子任务 (Semantic Subtask)</strong><br>决策：“打开微波炉”、“捡起抹布”、“把盘子放进水槽”</td> <td style="text-align: left"> <strong>确定意图</strong>：将复杂的长任务分解为可执行的短片段。</td> </tr> <tr> <td style="text-align: left"><strong>Low-Level Control</strong></td> <td style="text-align: left"> <strong>轨迹规划/控制 (Trajectory/Control)</strong><br>输出：方向盘转角 15°，油门 20%</td> <td style="text-align: left"> <strong>动作分块 (Action Chunk)</strong><br>输出：机械臂 7 个关节的角度变化、夹爪开合</td> <td style="text-align: left"> <strong>执行意图</strong>：将高层决策转化为具体的物理运动。</td> </tr> </tbody> </table> <p>论文中明确提到：$\pi_{0.5}$ 首先预测语义子任务（semantic subtask），然后将其作为上下文（Context）来预测低层的动作块（action chunk）。这与 AD 中先做行为决策再做轨迹规划的逻辑<strong>完全一致</strong>。 <alphaxiv-paper-citation title="Hierarchical Inference" page="2" first="predicts the semantic subtask" last="low-level robot action chunk"></alphaxiv-paper-citation></p> <h3 id="2-但是pi_05-有一个关键的进化">2. 但是，$\pi_{0.5}$ 有一个关键的“进化”</h3> <p>虽然逻辑相似，但在<strong>实现方式</strong>上，$\pi_{0.5}$ 走得比传统 AD 更远，它更接近于特斯拉 FSD v12 那种 <strong>End-to-End（端到端）</strong> 的理念。</p> <ul> <li> <strong>传统 AD：</strong> 通常是模块化的。感知模块是一个网络，决策模块可能是一个状态机或另一个网络，控制模块是 PID 或 MPC。它们是<strong>分离</strong>的。</li> <li> <strong>$\pi_{0.5}$：</strong> 是<strong>统一</strong>的。 <ul> <li>同一个 Transformer 大脑（Backbone）。</li> <li>先“自言自语”说出决策（生成文本 Token：”Open microwave”）。</li> <li>紧接着，基于这个决策，立刻生成动作（生成动作 Token）。</li> </ul> </li> </ul> <p>这种方式在论文中被类比为 <strong>Chain-of-Thought (思维链)</strong>。模型不需要切换大脑，它只是先“想”一下（输出文本），然后马上“做”（输出动作）。 <alphaxiv-paper-citation title="Chain-of-Thought" page="2" first="analogously to chain" last="of thought inference [82]."></alphaxiv-paper-citation></p> <h3 id="3-为什么这一步决策对-pi_05-如此重要">3. 为什么这一步（决策）对 $\pi_{0.5}$ 如此重要？</h3> <p>在自动驾驶中，如果你不先决定“左转”，直接去预测轨迹，车可能会在路口左右摇摆（模式坍缩）。</p> <p>同理，对于 $\pi_{0.5}$ 这种长周期任务（10-15分钟的家务），如果没有这个中间的“决策层”，机器人很容易迷失。</p> <ul> <li> <strong>例子：</strong> 整理床铺。</li> <li> <strong>没有决策层：</strong> 机器人看着乱糟糟的床，不知道该拉床单还是拍枕头，动作可能会由于概率分布的混杂而变得抽搐。</li> <li> <strong>有决策层：</strong> 模型先强制自己输出文本：“<strong>Grab the pillow</strong>”。一旦这几个字生成出来，后面的动作预测就被<strong>锁定</strong>在了“抓枕头”这个子空间里，动作就会非常坚定和准确。</li> </ul> <p><strong>实验证明：</strong> 论文中的消融实验显示，如果去掉这个高层语义预测（No HL），模型的成功率会大幅下降。这证明了显式的“决策”步骤对于长程任务是不可或缺的。 <alphaxiv-paper-citation title="High-Level Ablation" page="11" first="no HL ablation" last="performs significantly worse."></alphaxiv-paper-citation></p> <h2 id="模型训完后vlm的能力保持的如何还需要保持vlm的能力吗">模型训完后,VLM的能力保持的如何?还需要保持VLM的能力吗?</h2> <p>在传统的微调（Fine-tuning）范式中，我们经常担心“灾难性遗忘”（Catastrophic Forgetting），即模型在学习新任务（如机器人控制）时，丢失了原本预训练获得的通用知识（如识别“红苹果”与“青苹果”的区别）。 对于 $\pi_{0.5}$ 而言，<strong>保持 VLM 的能力不仅是“通过与否”的测试，而是其实现泛化的核心机制</strong>。</p> <h3 id="1-训完之后vlm-能力保持得如何">1. 训完之后，VLM 能力保持得如何？</h3> <p>在 $\pi_{0.5}$ 的架构中，VLM 的能力通过一种称为“协同训练”（Co-training）的策略得到了很好的保持，** 甚至在特定领域得到了增强**。研究团队并没有只用机器人数据去“刷写”模型，而是将机器人数据与大量的网络数据（Web Data）混合在一起训练。</p> <p>这种策略使得模型在学习“如何移动手臂”的同时，不仅没有忘记“什么是苹果”，反而加强了将视觉语义与物理动作关联的能力。实验中的消融研究（Ablation Study）强有力地证明了这一点：当研究人员移除网络数据（即 VQA、图像描述等维持 VLM 能力的数据）时，机器人的物理操作性能显著下降。这表明，模型正是利用了 VLM 的通用语义能力来理解陌生的环境。 <alphaxiv-paper-citation title="Co-training Importance" page="11" first="no WD ablation is" last="improving the high-level policy."></alphaxiv-paper-citation></p> <p>具体来说，$\pi_{0.5}$ 的训练数据中包含了数百万张带有边框（Bounding Boxes）和描述的图像。这意味着模型在训练过程中一直在复习和巩固其视觉识别能力。因此，虽然论文没有展示该模型在纯文本基准测试（如 MMLU）上的得分，但在具身场景下，其“理解世界”的能力（即 VLM 的核心能力）是被刻意保留且表现优异的。 <alphaxiv-paper-citation title="Web Data" page="5" first="We co-train with" last="VQA and captioning."></alphaxiv-paper-citation></p> <h3 id="2-训完-vla-之后还需要保持-vlm-的能力吗">2. 训完 VLA 之后，还需要保持 VLM 的能力吗？</h3> <p><strong>绝对需要。</strong> 甚至可以说，<strong>VLM 能力的保持是 VLA 能够实现“开放世界泛化”的前提</strong>。 (但是VLM4VLA这个论文里面又说VLM的能力和VLA的能力并不是正相关的.)</p> <p>如果我们只关心机器人在流水线上重复同一个动作（例如拧螺丝），我们确实不需要 VLM 的能力，传统的策略网络（Policy Network）就足够了。但是，$\pi_{0.5}$ 的目标是让机器人走进从未见过的家庭，去处理从未见过的物体。</p> <p>首先，<strong>语义理解是泛化的基础</strong>。当用户指令是“把那袋薯片拿给我”时，如果模型因为微调过度而忘记了“薯片”长什么样（VLM 能力丢失），或者无法区分“这袋”和“那袋”的语义差别，那么无论它的机械臂控制得多么平滑，任务都会失败。$\pi_{0.5}$ 的高层策略（High-Level Policy）本质上就是一个 VLM，它负责观察环境并输出文本形式的子任务（例如“Open the microwave”）。如果 VLM 能力丧失，这个“大脑”就瘫痪了。 <alphaxiv-paper-citation title="High-Level Reasoning" page="2" first="predicts the semantic subtask" last="low-level robot action"></alphaxiv-paper-citation></p> <p>其次，<strong>处理意外情况依赖常识</strong>。在非结构化环境中，机器人会遇到各种意外（例如抽屉被卡住、物体被遮挡）。VLM 预训练中包含的物理常识（虽然是隐式的）和逻辑推理能力，能帮助机器人进行简单的因果推断。论文中提到，去除高层语义预测数据的模型表现会变差，这进一步证实了保留这种类似 VLM 的推理能力对于处理复杂长程任务是必须的。 <alphaxiv-paper-citation title="Ablation Results" page="11" first="implicit HL ablation" last="subtask prediction, in training."></alphaxiv-paper-citation></p> <p>VLM 能力和 VLA 能力看作是“零和博弈”。在 $\pi_{0.5}$ 这种现代具身智能范式中，<strong>VLM 能力是 VLA 的“底座”</strong>。我们不需要模型能写诗（纯文本 VLM 能力），但我们必须保持它识别万物、理解复杂指令和进行因果推理的能力。一旦这些能力丢失，机器人就会退化回传统的、只能在实验室特定环境下工作的“特化机器”。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/DriveJEPA/">DriveJEPA</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/C_RADIOv4/">C_RADIOv4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/VLM4VLA/">VLM4VLA</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 P W Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>