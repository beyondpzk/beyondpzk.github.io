<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Gato | Tenacious life, proud journey. </title> <meta name="author" content="P W Name"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://beyondpzk.github.io/blog/2022/Gato/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Tenacious life, proud journey. </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Gato</h1> <p class="post-meta"> Created on May 12, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/category/vla"> <i class="fa-solid fa-tag fa-sm"></i> VLA</a>   <a href="/blog/category/embodiedai"> <i class="fa-solid fa-tag fa-sm"></i> EmbodiedAI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>[TOC]</p> <h1 id="gato-a-generalist-agent">Gato: A Generalist Agent</h1> <p><a href="https://arxiv.org/abs/2205.06175" rel="external nofollow noopener" target="_blank">paper link</a></p> <p>人工智能（AGI）发展史上具有里程碑意义的论文——由DeepMind团队在2022年发表的<strong>《A Generalist Agent》</strong>，也就是<strong>Gato</strong>。</p> <p>这篇论文的重要性在于，它挑战了过去人工智能领域“专才专用”的范式，提出了一个激进的假设：<strong>只要我们将不同模态的数据都序列化为Token，一个单一的Transformer网络就能同时学会玩Atari游戏、给图片写标题、甚至控制真实的机械臂。</strong></p> <hr> <h1 id="第一模块设计哲学与通用智能的愿景">第一模块：设计哲学与通用智能的愿景</h1> <h2 id="11-背景从专才到通才的范式转移">1.1 背景：从“专才”到“通才”的范式转移</h2> <p>在Gato之前，强化学习（RL）和机器人学（Robotics）领域的主流做法是为每个任务设计特定的架构。比如，处理图像我们用CNN，处理序列决策我们用LSTM或Transformer，处理控制我们设计特定的策略网络。虽然我们在单一任务上（如AlphaGo, OpenAI Five）取得了超越人类的成就，但这些系统是脆弱的，无法泛化。</p> <p>Gato的灵感直接来源于大型语言模型（LLM）的成功。既然GPT-3可以通过“下一个Token预测”来处理翻译、写作、编程等多种任务，我们能否将这种通用的序列建模能力扩展到文本之外的领域？Gato的核心论点是：<strong>如果我们将视觉、本体感觉（Proprioception）、关节力矩（Torques）都视为数据流，那么就可以训练一个单一的“通才”智能体。</strong> <alphaxiv-paper-citation title="Abstract" page="1" first="Inspired by progress" last="of text outputs."></alphaxiv-paper-citation></p> <h2 id="12-核心假设扩展定律在控制领域的应用">1.2 核心假设：扩展定律在控制领域的应用</h2> <p>这篇论文不仅是一个工程壮举，它实际上是在测试一个科学假设：<strong>通过扩大数据规模、计算量和模型参数，并持续拓宽训练分布，我们可以获得一个能够适应任何任务、行为和具身形式（Embodiment）的通用智能体。</strong></p> <p>这与Sutton提出的“苦涩的教训（The Bitter Lesson）”不谋而合——从历史来看，那些能利用算力扩张的通用方法，最终总会战胜利用特定领域知识的特化方法。Gato在当时（2022年）选择了一个约12亿（1.2B）参数的模型规模，这个规模是为了保证在控制真实机器人时能满足实时性（Real-time control）的要求。 <alphaxiv-paper-citation title="Scaling Hypothesis" page="2" first="We hypothesize that" last="behaviors."></alphaxiv-paper-citation></p> <h2 id="13-训练范式序列建模与监督学习">1.3 训练范式：序列建模与监督学习</h2> <p>与其说Gato是一个强化学习模型，不如说它是一个在大规模轨迹数据上进行<strong>监督学习（Supervised Learning）</strong>的序列模型。它并没有在训练中通过与环境交互来最大化奖励（那是RL做的事），而是利用现有的专家策略产生的数据进行“行为克隆（Behavior Cloning, BC）”。</p> <p>训练的目标非常单纯：给定一段历史上下文（Context），预测下一个Token。这个Token可能是一个文本词，可能是机器人的一个动作，也可能是一个离散的按键。这种统一的训练目标消除了针对特定领域设计归纳偏置（Inductive Biases）的需求。 <alphaxiv-paper-citation title="Training Paradigm" page="2" first="For simplicity Gato" last="supervised manner;"></alphaxiv-paper-citation></p> <hr> <h1 id="第二模块数据的统一表示与token化tokenization">第二模块：数据的统一表示与Token化（Tokenization）</h1> <p>这是理解Gato最关键的技术细节。要让一个网络处理截然不同的模态，必须解决如何让图像、文本和关节力矩讲同一种“语言”。Gato的答案是：<strong>一切皆为Token序列。</strong></p> <h2 id="21-文本与图像的token化">2.1 文本与图像的Token化</h2> <p>文本的处理是标准的。Gato使用了SentencePiece编码，词表大小为32000，整数范围在 $[0, 32000)$。</p> <p>对于图像，Gato借鉴了Vision Transformer (ViT) 的思路。图像被分割成互不重叠的 $16 \times 16$ 的图块（Patches）。这些图块按光栅顺序（Raster order）排列。每个图块内的像素值被归一化到 $[-1, 1]$ 并除以图块大小的平方根。 <alphaxiv-paper-citation title="Image Tokenization" page="3" first="Images are first" last="raster order,"></alphaxiv-paper-citation></p> <h2 id="22-连续值的离散化mu-law-编码">2.2 连续值的离散化：$\mu$-law 编码</h2> <p>机器人的本体感觉（如关节角度）和动作（如力矩）通常是连续浮点数。Transformer本质上是一个分类器，擅长处理离散符号。Gato如何处理连续值？</p> <p>它采用了一种在音频处理（如WaveNet）中常用的技术：<strong>$\mu$-law 压扩（Companding）</strong>。首先将数据展平，然后通过以下公式将数值非线性地映射到 $[-1, 1]$ 区间：</p> \[F(x) = \text{sgn}(x) \frac{\ln(|x|\mu + 1.0)}{\ln(M\mu + 1.0)}\] <p>其中 $\mu=100$，$M=256$。这种编码方式对接近0的小数值有更高的分辨率，而对大数值的分辨率较低，非常适合人类感知和控制信号的特性。映射后，数值被离散化为1024个均匀的箱（bins），并平移到 $[32000, 33024)$ 的整数范围内，以避免与文本Token冲突。 <alphaxiv-paper-citation title="Continuous Values" page="3" first="Continuous values, e.g." last="uniform bins."></alphaxiv-paper-citation></p> <h2 id="23-序列排序sequencing">2.3 序列排序（Sequencing）</h2> <p>有了Token，我们还需要定义它们的排列顺序。Gato定义了一个规范的序列顺序：</p> <ol> <li> <strong>文本 Token</strong>（按原始顺序）。</li> <li> <strong>图像 Patch Token</strong>（按光栅顺序）。</li> <li> <strong>张量（Tensors）</strong>（按行优先顺序）。</li> <li> <strong>智能体的时间步（Timesteps）</strong>：先是观测（Observation）Token，然后是一个分隔符（Separator）Token，最后是动作（Action）Token。</li> </ol> <p>这种严格的排序保证了模型能够学习到时间上的因果关系。 <alphaxiv-paper-citation title="Sequence Ordering" page="3" first="After converting data" last="action tokens."></alphaxiv-paper-citation></p> <hr> <h1 id="第三模块模型架构深度解析">第三模块：模型架构深度解析</h1> <p>Gato的骨架是一个<strong>仅解码器（Decoder-only）的Transformer</strong>。这意味着它和GPT系列一样，通过掩码注意力机制（Masked Attention）只能“看”到过去的信息。</p> <h2 id="31-总体架构参数">3.1 总体架构参数</h2> <p>论文中主要实验使用的是一个1.18B参数的模型。具体的超参数如下：</p> <ul> <li> <strong>层数（Layers）</strong>: 24层</li> <li> <strong>注意力头（Attention Heads）</strong>: 16个</li> <li> <strong>嵌入维度（Embedding Size）</strong>: 2048</li> <li> <strong>前馈网络隐藏层维度（FFN Hidden Size）</strong>: 8196</li> </ul> <p>这在LLM领域不算大，但在实时控制领域，这是一个相当庞大的网络，需要保证推理延迟足够低以便控制物理机器人。 <alphaxiv-paper-citation title="Model Size" page="4" first="Gato uses a" last="of 8196"></alphaxiv-paper-citation></p> <h2 id="32-嵌入函数embedding-function">3.2 嵌入函数（Embedding Function）</h2> <p>虽然主要架构是Transformer，但输入层并非全是简单的查表（Lookup Table）。为了高效处理多模态，$f(\cdot; \theta_e)$ 嵌入函数针对不同模态有不同的操作：</p> <ul> <li> <strong>文本、离散/连续观测值</strong>：使用学习到的向量嵌入空间（Lookup Table）。</li> <li> <strong>图像 Patch</strong>：这是关键点。图像Token不仅仅是查表，而是通过了一个<strong>ResNet Block</strong>。每个Patch通过这个ResNet块被编码成一个向量。这相当于在Transformer之前有一个轻量级的视觉编码器。 <alphaxiv-paper-citation title="Embedding Logic" page="4" first="Tokens belonging to" last="within-image position"></alphaxiv-paper-citation> </li> </ul> <p>具体到这个ResNet Block，它使用了V2架构，包含GroupNorm（32组）和GELU激活函数。 <alphaxiv-paper-citation title="ResNet Details" page="33" first="This block uses" last="activation functions."></alphaxiv-paper-citation></p> <h2 id="33-位置编码position-encodings">3.3 位置编码（Position Encodings）</h2> <p>对于序列模型，位置信息至关重要。Gato采用了两种位置编码策略：</p> <ol> <li> <strong>局部位置编码（Local Position Encodings）</strong>：针对同一时间步内的Token，根据它们在时间步内的局部位置添加编码。</li> <li> <strong>图像块位置编码</strong>：为了让模型理解图像的2D结构，模型通过Patch的归一化行、列坐标，分别查表得到行编码和列编码，并加到Patch的嵌入向量上。这是为了保留空间语义。 <alphaxiv-paper-citation title="Position Encodings" page="4" first="Learnable position encodings" last="encoding vector."></alphaxiv-paper-citation> </li> </ol> <h2 id="34-损失函数loss-function">3.4 损失函数（Loss Function）</h2> <p>训练的目标是最大化对数似然。对于一个批次 $B$，损失函数如下：</p> \[\mathcal{L}(\theta, B) = - \sum_{b=1}^{|B|} \sum_{l=1}^{L} m(b, l) \log p_\theta(s_l^{(b)} | s_1^{(b)}, \dots, s_{l-1}^{(b)})\] <p>这里引入了一个掩码函数 $m(b, l)$。这个掩码非常重要：<strong>并不是所有的Token都会产生损失。</strong> 只有文本Token和智能体的动作（Action）Token会被计算损失。图像Token和观测Token虽然作为上下文输入，但模型不需要预测它们（至少在这个版本的Gato中）。 <alphaxiv-paper-citation title="Training Loss" page="4" first="The training loss" last="and 0 otherwise."></alphaxiv-paper-citation></p> <hr> <h1 id="第四模块实验设置与结果分析">第四模块：实验设置与结果分析</h1> <p>Gato的训练数据规模庞大且多样，这决定了它的通用性。</p> <h2 id="41-数据集构成">4.1 数据集构成</h2> <p>Gato在约<strong>604个不同的任务</strong>上进行了训练。这些数据包括：</p> <ul> <li> <strong>模拟控制</strong>：Atari游戏、DeepMind Control Suite、Meta-World等。</li> <li> <strong>视觉与语言</strong>：MassiveText, ALIGN, MS-COCO等。</li> <li> <strong>机器人</strong>：RGB Stacking（堆叠积木）任务，包含仿真和真实机器人数据。</li> </ul> <p>特别值得注意的是，对于控制任务，Gato使用了过滤后的专家数据（Returns at least 80% of expert return），这保证了模型学到的是高质量的策略。 <alphaxiv-paper-citation title="Datasets" page="5" first="Gato was trained" last="specifications."></alphaxiv-paper-citation></p> <h2 id="42-模拟环境中的表现">4.2 模拟环境中的表现</h2> <p>结果令人印象深刻。在450个以上的任务中，Gato的性能超过了专家分数的50%。</p> <ul> <li> <strong>Atari</strong>：在23个游戏中达到或超过人类平均水平。</li> <li> <strong>BabyAI</strong>：在几乎所有关卡中达到专家分数的80%以上。</li> <li> <strong>Meta-World</strong>：在44/45个任务中超过50%的成功率。</li> </ul> <p>虽然在单任务比较上，专门训练的专家（Specialist）通常还是比Gato强，但Gato是用<strong>同一套权重</strong>解决了所有问题，这是质的飞跃。 <alphaxiv-paper-citation title="Simulated Performance" page="7" first="As shown in" last="expert score threshold."></alphaxiv-paper-citation></p> <h2 id="43-真实机器人控制real-world-robotics">4.3 真实机器人控制（Real-world Robotics）</h2> <p>这是最具挑战性的部分。Gato通过观察图像和本体感觉，直接输出关节力矩来控制一个Sawyer机械臂进行积木堆叠（RGB Stacking）。</p> <p>实验展示了Gato不仅在仿真中表现良好，还能直接迁移到真实机器人上。在“技能泛化（Skill Generalization）”测试中，Gato展现了与基线方法相当的能力。更重要的是，通过<strong>微调（Fine-tuning）</strong>，Gato可以迅速适应新的任务或从未见过的物体形状。 <alphaxiv-paper-citation title="Robotics Results" page="12" first="Gato, in both" last="the expert."></alphaxiv-paper-citation></p> <h2 id="44-提示prompting与上下文学习">4.4 提示（Prompting）与上下文学习</h2> <p>由于不同任务可能共享相同的观测空间（例如都是Atari画面），模型如何知道该玩哪个游戏？ Gato使用<strong>提示（Prompting）</strong>机制。在训练中，25%的序列会前置一段来自同一任务的演示（Demonstration）。在测试时，我们可以通过提供一段成功的演示作为Prompt，来告诉Gato我们想让它做什么。这类似于GPT-3的Few-shot prompting。 <alphaxiv-paper-citation title="Prompting" page="4" first="During evaluation, the" last="we present here."></alphaxiv-paper-citation></p> <hr> <h1 id="第五模块批判性分析与未来展望">第五模块：批判性分析与未来展望</h1> <h2 id="51-局限性">5.1 局限性</h2> <ol> <li> <strong>上下文长度限制</strong>：Gato的上下文窗口只有1024个Token。对于高频控制任务，这可能只覆盖了几秒钟的历史。这限制了它处理需要长期记忆的复杂任务的能力。</li> <li> <strong>遗忘灾难与容量</strong>：虽然Gato是一个通用智能体，但在特定任务（如Atari）上，它仍然不如专门训练的智能体。论文中提到，专门训练的Atari智能体在更多游戏中超越了人类。这表明单一网络的容量（1.18B）可能还不足以完美容纳所有领域的知识。 <alphaxiv-paper-citation title="Specialist Comparison" page="14" first="The specialist Atari" last="23 games."></alphaxiv-paper-citation> </li> <li> <strong>缺乏在线学习</strong>：Gato完全是离线训练（Offline Training）的。它没有在部署后通过试错来自我提升的机制。这意味着它无法应对训练分布之外的全新的、动态变化的环境，除非进行微调。</li> </ol> <h2 id="52-解释性分析">5.2 解释性分析</h2> <p>论文对模型的内部表示进行了可视化。</p> <ul> <li> <strong>注意力图（Attention Maps）</strong>：显示模型能够关注到图像中与任务相关的物体（如球拍、积木）。</li> <li> <strong>嵌入可视化（t-SNE）</strong>：不同任务的嵌入在空间中形成了清晰的聚类，甚至从未见过的任务（Hold-out tasks）也能被映射到语义相关的区域。这证明了模型学到了通用的表示，而不仅仅是死记硬背。 <alphaxiv-paper-citation title="Embeddings" page="15" first="Embeddings from the" last="to each other."></alphaxiv-paper-citation> </li> </ul> <h2 id="53-对未来的启示">5.3 对未来的启示</h2> <p>Gato证明了<strong>“大一统模型（The One Big Net）”</strong>在多模态控制领域的可行性。它通过序列建模统一了离散和连续、视觉和语言。</p> <p>未来的研究方向可能包括：</p> <ol> <li> <strong>扩大规模</strong>：随着硬件进步，更大的模型（如10B+）可能会展现出更强的涌现能力。</li> <li> <strong>统一训练目标</strong>：结合离线预训练和在线强化学习（RL），让模型能在部署中持续进化。</li> <li> <strong>世界模型</strong>：不仅预测动作，还预测下一个观测（Video Prediction），这将使智能体具备规划（Planning）能力。</li> </ol> <h2 id="一个具体的training-sample">一个具体的training sample</h2> <p>在 Gato 的眼中，无论是莎士比亚的诗句、Atari 游戏的像素画面，还是机械臂的关节力度，本质上<strong>没有任何区别</strong>——它们全都是一串<strong>整数（Integers）</strong>。</p> <p>为了直观理解，构造一个<strong>具体的训练样本（Training Sample）</strong>。以一个典型的<strong>机器人控制任务</strong>为例：<strong>“用机械臂将蓝色方块堆叠在绿色方块上（Stack blue on green）”</strong>。</p> <p>这一个时间步（Timestep）的数据，进入模型时会被转化为以下这样一个长序列：</p> <hr> <h3 id="1-概念层面的序列结构">1. 概念层面的序列结构</h3> <p>在这个样本中，模型接收的序列大致是这样的顺序： <code class="language-plaintext highlighter-rouge">[文本指令]</code> $\rightarrow$ <code class="language-plaintext highlighter-rouge">[图像 Patch]</code> $\rightarrow$ <code class="language-plaintext highlighter-rouge">[本体感觉状态]</code> $\rightarrow$ <code class="language-plaintext highlighter-rouge">[分隔符]</code> $\rightarrow$ <code class="language-plaintext highlighter-rouge">[动作]</code></p> <h3 id="2-具体的数据转化过程">2. 具体的数据转化过程</h3> <h4 id="a-文本部分-text-instruction">A. 文本部分 (Text Instruction)</h4> <ul> <li> <strong>原始输入</strong>：<code class="language-plaintext highlighter-rouge">"Stack blue on green"</code> </li> <li> <strong>处理方式</strong>：使用 SentencePiece 进行分词。</li> <li> <strong>数值范围</strong>：$[0, 32000)$</li> <li> <strong>Gato 看到的 Token</strong>： 假设词表映射如下：<code class="language-plaintext highlighter-rouge">Stack</code>$\rightarrow 540$, <code class="language-plaintext highlighter-rouge">blue</code>$\rightarrow 120$, <code class="language-plaintext highlighter-rouge">on</code>$\rightarrow 30$, <code class="language-plaintext highlighter-rouge">green</code>$\rightarrow 980$。 那么序列的开头就是： \([540, 120, 30, 980]\)</li> </ul> <alphaxiv-paper-citation title="Text Tokenization" page="3" first="Text is encoded" last="range [0, 32000)."></alphaxiv-paper-citation> <h4 id="b-图像部分-image-observations">B. 图像部分 (Image Observations)</h4> <ul> <li> <strong>原始输入</strong>：机械臂摄像头的画面（假设 $64 \times 64$ 像素）。</li> <li> <strong>处理方式</strong>： <ol> <li>切分为 $16 \times 16$ 的小方块（Patches）。这意味着一张图有 $(64/16) \times (64/16) = 16$ 个 Patch。</li> <li>按光栅顺序（从左到右，从上到下）排列。</li> <li>每个 Patch 通过 ResNet 编码成一个向量，再映射到嵌入空间。</li> </ol> </li> <li> <strong>Gato 看到的 Token</strong>： 虽然在内部它是向量，但在序列逻辑中，它占据了 16 个位置： \([\text{Patch}_1, \text{Patch}_2, \dots, \text{Patch}_{16}]\)</li> </ul> <alphaxiv-paper-citation title="Image Patching" page="3" first="Images are first" last="raster order,"></alphaxiv-paper-citation> <h4 id="c-本体感觉与状态-proprioception--continuous-observations">C. 本体感觉与状态 (Proprioception / Continuous Observations)</h4> <ul> <li> <strong>原始输入</strong>：机械臂当前的关节角度（例如：关节1是 $0.5$ 弧度，关节2是 $-1.2$ 弧度）。</li> <li> <strong>处理方式</strong>：这是最关键的一步。 <ol> <li> <strong>$\mu$-law 压扩</strong>：把连续浮点数映射到 $[-1, 1]$。</li> <li> <strong>离散化</strong>：把 $[-1, 1]$ 切分成 1024 个桶（bins）。</li> <li> <strong>平移</strong>：为了不和文本 ID 冲突，把这些 ID 加上 32000。</li> </ol> </li> <li> <strong>数值范围</strong>：$[32000, 33024)$</li> <li> <strong>Gato 看到的 Token</strong>： 假设 $0.5$ 对应桶 ID 500，$-1.2$ 对应桶 ID 100。 那么序列接着是： \([32500, 32100, \dots]\)</li> </ul> <alphaxiv-paper-citation title="Continuous Values" page="3" first="Continuous values are" last="integer token indices."></alphaxiv-paper-citation> <h4 id="d-分隔符-separator">D. 分隔符 (Separator)</h4> <ul> <li> <strong>作用</strong>：告诉模型，“观测”结束了，下面该预测“动作”了。</li> <li> <strong>Gato 看到的 Token</strong>： \([\text{SEPARATOR\_TOKEN}]\)</li> </ul> <h4 id="e-动作部分-action---训练目标">E. 动作部分 (Action - 训练目标)</h4> <ul> <li> <strong>原始输入</strong>：需要输出的关节力矩（Torques）或速度指令。</li> <li> <strong>处理方式</strong>：同样使用 $\mu$-law 编码离散化。</li> <li> <strong>Gato 看到的 Token</strong>： \([32800, 32650, 32900, \dots]\)</li> </ul> <hr> <h3 id="3-完整的单一序列视图">3. 完整的单一序列视图</h3> <p>把上面所有部分拼起来，Gato 在这一个时间步看到的<strong>完整输入张量</strong>就是这样一串整数：</p> \[[ \underbrace{540, 120, 30, 980}_{\text{Text}}, \underbrace{\text{P}_1, \dots, \text{P}_{16}}_{\text{Image}}, \underbrace{32500, \dots}_{\text{State}}, \text{SEP}, \underbrace{\mathbf{32800}, \mathbf{32650}, \dots}_{\text{Action (Target)}} ]\] <p>注意，如果是<strong>纯文本任务</strong>（比如聊天），就没有中间的图像和状态 Token；如果是<strong>玩 Atari 游戏</strong>，可能就没有开头的文本指令，直接从图像 Patch 开始。</p> <p>Gato 的强大之处就在于，它根本不在乎这些整数代表的是“单词 blue”还是“关节角度 0.5”，它只负责做一件事：<strong>根据前面的所有整数，预测下一个整数是什么。</strong></p> <alphaxiv-paper-citation title="Sequence Ordering" page="3" first="Tokens form the" last="action tokens."></alphaxiv-paper-citation> <h2 id="和后面的-rt-2-比较">和后面的 RT-2 比较</h2> <p><strong>Gato 不仅可以“看作”是 VLA（Vision-Language-Action）模型，它实际上是 VLA 概念的鼻祖和原型（Prototype）。</strong></p> <p>虽然“VLA”这个术语在后来 Google 发布 <strong>RT-2 (Robotic Transformer 2)</strong> 时才真正变得炙手可热，但在 2022 年，Gato 就已经完整地定义了 VLA 的范式。</p> <p>接下来对比下 <strong>Gato 与现代 VLA（如 RT-2）的异同</strong>，这对于理解具身智能的发展脉络至关重要。</p> <hr> <h3 id="1-为什么说-gato-是教科书级的-vla">1. 为什么说 Gato 是教科书级的 VLA？</h3> <p>VLA 的核心定义是：<strong>将视觉（Vision）、语言（Language）和动作（Action）统一在一个 Transformer 模型中，实现端到端的推理。</strong></p> <p>回顾我们在第二模块讲的内容，Gato 完美契合这三点：</p> <ul> <li> <strong>Vision</strong>: 它处理图像 Patch（来自 Atari 或 机器人摄像头）。</li> <li> <strong>Language</strong>: 它处理文本 Prompt（如“Stack blue block”）。</li> <li> <strong>Action</strong>: 它输出离散化的动作 Token（关节力矩或按键）。</li> </ul> <p>论文摘要的第一句就明确了这一点：“…works as a multi-modal, multi-task, multi-embodiment generalist policy.” 这本质上就是 VLA 的定义。 <alphaxiv-paper-citation title="Abstract" page="1" first="The agent, which" last="generalist policy."></alphaxiv-paper-citation></p> <hr> <h3 id="2-gato-2022-vs-rt-2-2023两条不同的技术路线">2. Gato (2022) vs. RT-2 (2023)：两条不同的技术路线</h3> <p>虽然都是 VLA，但 Gato 和后来的 RT-2 代表了两种截然不同的<strong>构建哲学</strong>。</p> <h4 id="a-训练策略从头培养-vs-半路出家">A. 训练策略：“从头培养” vs. “半路出家”</h4> <ul> <li> <strong>Gato (The Generalist Approach)</strong>: Gato 的策略是 <strong>“Train from Scratch”</strong>（或者说是混合训练）。它把控制数据（Control Data）、视觉文本数据（Vision-Language Data）和纯文本数据（Text Data）<strong>以 1:1:1 等比例混合</strong>，同时喂给一个随机初始化的（或部分预训练的）网络。 <ul> <li> <strong>优点</strong>：模型从一开始就学习如何平衡不同模态，动作和语言在特征空间是平等的。</li> <li> <strong>缺点</strong>：需要极大的数据平衡技巧，且很难直接利用 LLM 涌现出的超强推理能力（因为模型只有 1.2B）。</li> </ul> </li> <li> <strong>RT-2 (The VLM-Transfer Approach)</strong>: RT-2 的策略是 <strong>“Fine-tune a VLM”</strong>。它先拿一个已经在大规模互联网数据上训练好的超大 VLM（如 PaLI-X 或 PaLM-E），然后把机器人的动作数据混进去进行微调（Co-fine-tuning）。 <ul> <li> <strong>优点</strong>：直接继承了 LLM 的世界知识和逻辑推理能力（比如识别“超人公仔”或“灭绝的动物”）。</li> <li> <strong>缺点</strong>：动作只是被当作一种特殊的“语言”强行塞进去的，推理速度通常很慢。</li> </ul> </li> </ul> <h4 id="b-动作的-token-化专用-vs-通用">B. 动作的 Token 化：专用 vs. 通用</h4> <ul> <li> <strong>Gato</strong>: Gato 为动作设计了<strong>专用的数值编码区间</strong>（$[32000, \dots]$）。这意味着在 Gato 的词表中，“动作”和“单词”是物理隔离的。 <alphaxiv-paper-citation title="Action Tokenization" page="3" first="Action tokens are" last="integer token indices."></alphaxiv-paper-citation> </li> <li> <strong>RT-2</strong>: RT-2 通常直接使用<strong>文本 Token</strong> 来表示动作。例如，如果机器人的动作是 <code class="language-plaintext highlighter-rouge">128</code>，它就输出文本 <code class="language-plaintext highlighter-rouge">"1"</code>, <code class="language-plaintext highlighter-rouge">"2"</code>, <code class="language-plaintext highlighter-rouge">"8"</code>。这使得它能直接利用预训练 LLM 的输出头，不需要修改模型结构。</li> </ul> <h4 id="c-实时性real-time-inference">C. 实时性（Real-time Inference）</h4> <p>这是 Gato 最令人敬佩的工程考量。</p> <ul> <li> <strong>Gato</strong>: 只有 <strong>1.18B 参数</strong>。为什么？因为 DeepMind 必须保证它能以 <strong>20Hz</strong> 的频率控制真实的 Sawyer 机械臂。如果在做推理时延迟太高，机器人就会抖动甚至失控。</li> <li> <strong>现代 VLA</strong>: 往往参数量巨大（7B, 13B, 甚至 50B+）。它们通常无法直接进行高频控制，需要配合一个小的“策略网络”或者运行在极低的频率下。</li> </ul> <alphaxiv-paper-citation title="Real-time Control" page="4" first="Gato uses a" last="latency requirements."></alphaxiv-paper-citation> <hr> <h3 id="3-总结">3. 总结</h3> <p>Gato 是 VLA 的<strong>先驱（Pioneer）</strong>。</p> <ul> <li>如果说 GPT-3 证明了“语言是通用的接口”；</li> <li>那么 Gato 就证明了<strong>“序列（Sequence）是通用的接口”</strong>。</li> </ul> <p>它告诉我们：只要你能把物理世界的信号（力、光、声）变成 Token，Transformer 就能学会控制物理世界。这正是今天具身智能（Embodied AI）最核心的信条。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/DriveJEPA/">DriveJEPA</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/C_RADIOv4/">C_RADIOv4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/GeRo/">GeRo</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 P W Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>