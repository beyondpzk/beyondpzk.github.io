<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RectifiedFlow | Tenacious life, proud journey. </title> <meta name="author" content="P W Name"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://beyondpzk.github.io/blog/2022/RectifiedFlow/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Tenacious life, proud journey. </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">RectifiedFlow</h1> <p class="post-meta"> Created on September 07, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>[TOC]</p> <h1 id="rectifiedflow">RectifiedFlow</h1> <ul> <li><a href="https://arxiv.org/abs/2209.03003" rel="external nofollow noopener" target="_blank">paper地址</a></li> </ul> <h1 id="flow-straight-and-fast-rectified-flow">Flow Straight and Fast (Rectified Flow)</h1> <p><strong>论文来源：</strong> <em>Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow (Liu et al., 2022)</em></p> <hr> <h2 id="第一部分生成模型的困境与拉直的直觉">第一部分：生成模型的困境与“拉直”的直觉</h2> <h3 id="11-生成模型的本质任务">1.1 生成模型的本质任务</h3> <ul> <li> <strong>核心问题：</strong> 我们有两个分布。 <ul> <li>$\pi_0$：简单的噪声分布（如高斯噪声 $N(0, I)$）。</li> <li>$\pi_1$：复杂的数据分布（如 ImageNet 图片）。</li> </ul> </li> <li> <strong>目标：</strong> 找到一个映射 $T$，使得 $Z_0 \sim \pi_0 \Longrightarrow T(Z_0) \sim \pi_1$。</li> <li> <strong>现有方法的局限性：</strong> <ul> <li> <strong>GANs：</strong> 直接映射，不稳定，难以训练。</li> <li> <strong>Diffusion Models (SDEs)：</strong> 这是一个随机行走的过程，虽然稳定，但路径极其曲折（像醉汉走路），导致生成速度慢，需要几百步去噪。</li> </ul> </li> </ul> <h3 id="12-核心理念两点之间直线最短">1.2 核心理念：两点之间，直线最短</h3> <ul> <li> <strong>思考：</strong> 如果我们要把一堆沙子从地点 A 搬到地点 B，最高效的方法是什么？ <ul> <li>当然是走直线。</li> </ul> </li> <li> <strong>Rectified Flow 的愿景：</strong> 我们能不能训练一个常微分方程（ODE）模型，让粒子从噪声出发，沿着<strong>笔直的路径</strong>匀速飞向目标图片？ <ul> <li>如果路径是笔直的，意味着速度 $v$ 是常数。</li> <li>如果 $v$ 是常数，数值模拟只需要 <strong>1 步</strong>（Euler Step）：$Z_1 = Z_0 + v \times 1$。</li> <li>这就是题目 “Straight and Fast” 的由来。 <alphaxiv-paper-citation title="Straight Paths" page="1" first="The idea of" last="much as possible."></alphaxiv-paper-citation> </li> </ul> </li> </ul> <h3 id="13-直觉图解广场上的人群-the-crowd-metaphor">1.3 直觉图解：广场上的人群 (The Crowd Metaphor)</h3> <ul> <li> <strong>场景 A（原始数据）：</strong> 想象一个广场，左边有一群穿红衣服的人（噪声 $\pi_0$），右边有一群穿蓝衣服的人（数据 $\pi_1$）。</li> <li> <strong>随机耦合 (Independent Coupling)：</strong> <ul> <li>我们在训练时，通常是随机采样一对 $(X_0, X_1)$。</li> <li>这就像随机指定左边的 Alice 必须走到右边的 Bob 的位置，左边的 Charlie 必须走到右边的 David 的位置。</li> <li> <strong>结果：</strong> 大家的路径会在广场中心<strong>大量交叉</strong>。这就是“混乱”的根源。</li> </ul> </li> <li> <strong>问题所在：</strong> <ul> <li>交叉意味着在同一个路口，有人要往左上走，有人要往右下走。</li> <li>一个确定性的 ODE 模型（单值函数）无法同时满足这两个要求。 <alphaxiv-paper-citation title="Crossing Problem" page="4" first="trajectories are rewired" last="avoid the crossing."></alphaxiv-paper-citation> (一条确定的轨迹,在同一点不可能有两个速度)</li> </ul> </li> </ul> <h3 id="14-rectified-flow-的解决方案rewiring-重连">1.4 Rectified Flow 的解决方案：Rewiring (重连)</h3> <ul> <li> <strong>核心思想：</strong> 模型不关心 Alice 是否真的见到了 Bob。模型只关心<strong>所有红衣服的人最后都变成了蓝衣服的人</strong>。(不管原始训练数据的配对有多么的混乱.)</li> <li> <strong>整流 (Rectification)：</strong> <ul> <li>如果在路口相遇，Rectified Flow 会指挥交通：“你们不要打架，交换目的地吧。”</li> <li>原本要去右下角的 Alice，改去右上角；原本要去右上角的 Charlie，改去右下角。</li> <li> <strong>结果：</strong> 路径不再交叉，交通变得顺畅。这就是 Figure 2 中展示的现象。</li> </ul> </li> </ul> <p>我的理解: Rectified学的就是在那一个点,所有人的速度的平均, 所以当模型训好了之后,当某个人再次走到这个点时,就会按照学到的平均速度来走,我觉得这也是MeanFlows的Motivation.</p> <hr> <h2 id="第二部分核心数学理论与期望的魔力-100---150">第二部分：核心数学理论与“期望”的魔力 (1:00 - 1:50)</h2> <h3 id="21-ode-模型定义">2.1 ODE 模型定义</h3> <p>我们定义一个随时间 $t \in [0, 1]$ 变化的 ODE： \(dZ_t = v(Z_t, t) dt\)</p> <ul> <li>$Z_0 \sim \pi_0$</li> <li>我们希望 $Z_1 \sim \pi_1$</li> <li>我们的任务是学习这个速度场 $v: \mathbb{R}^d \times [0, 1] \to \mathbb{R}^d$。 <alphaxiv-paper-citation title="ODE Definition" page="4" first="dZt =" last="v(Zt, t)dt,"></alphaxiv-paper-citation> </li> </ul> <h3 id="22-训练目标最小二乘法">2.2 训练目标：最小二乘法</h3> <p>为了让 $Z_t$ 尽可能走直线，我们构建一个极其简单的线性插值过程作为“教师”： \(X_t = t X_1 + (1-t) X_0\)</p> <ul> <li>这个插值的速度（导数）显然是 $(X_1 - X_0)$。</li> </ul> <p><strong>损失函数 (Loss Function)：</strong> \(\min_v \int_0^1 E \left[ \| (X_1 - X_0) - v(X_t, t) \|^2 \right] dt\)</p> <ul> <li>我们在做什么？我们在让神经网络 $v$ 去拟合直线插值的方向。 <alphaxiv-paper-citation title="Loss Function" page="4" first="min v" last="linear interpolation of"></alphaxiv-paper-citation> </li> </ul> <h3 id="23-关键推导为什么会不交叉">2.3 关键推导：为什么会“不交叉”？</h3> <p>这是理解本论文最困难也最精彩的地方。</p> <p><strong>最优解的形式：</strong> 对于上述均方误差损失，理论最优解是<strong>条件期望</strong>： \(v^\*(z, t) = E [ X_1 - X_0 \mid X_t = z ]\)</p> <alphaxiv-paper-citation title="Conditional Expectation" page="6" first="vX (x, t) =" last="| Xt=x],"></alphaxiv-paper-citation> <p><strong>这个公式意味着什么？</strong></p> <ol> <li>假设在位置 $z$ 有两条路径交叉：一条速度是 $v_1$（向右上），一条是 $v_2$（向右下）。</li> <li>神经网络无法同时输出两个值，它只能输出平均值：$v_{model} \approx \frac{v_1 + v_2}{2}$。</li> <li>垂直方向的速度被抵消了，只剩下水平方向的速度。</li> <li> <strong>结论：</strong> 神经网络学到的速度场是一个<strong>单值向量场</strong>。在单值向量场中，积分曲线（轨迹）<strong>永远无法相交</strong>（根据 ODE 唯一性定理）。</li> <li>这就是为什么模型会自动把乱麻一样的路径梳理成平行的流线。</li> </ol> <p>我的理解: 这个公式可以理解为,在 $t$时刻, $z$位置 最终模型输出的理想速度, 等于 从$X_0$ 出发, 在$t$时刻经过$z$ 最终到达$X_1$的所有轨迹中, 在$z$ 位置速度的平均.</p> <h3 id="24-定理-33边缘分布保持-marginal-preserving">2.4 定理 3.3：边缘分布保持 (Marginal Preserving)</h3> <p>尽管我们通过取平均改变了个体的路径，但宏观上分布是对的吗？</p> <ul> <li> <strong>定理内容：</strong> $\text{Law}(Z_t) = \text{Law}(X_t)$ 对于所有 $t$ 成立。</li> <li> <strong>直观理解：</strong> 只要流入和流出每个区域的“总质量”守恒（这是由期望公式保证的），那么 $t$ 时刻云团的形状就和插值的形状一模一样。</li> <li>既然 $X_1$ 是真实数据，那么 $Z_1$ 也必然服从真实数据分布。 <alphaxiv-paper-citation title="Theorem 3.3" page="6" first="Theorem 3.3." last="Assume X is"></alphaxiv-paper-citation> </li> </ul> <hr> <h2 id="第三部分reflow-算法与极速生成">第三部分：Reflow 算法与极速生成</h2> <h3 id="31-既然已经不交叉了为什么还要-reflow">3.1 既然已经不交叉了，为什么还要 Reflow？</h3> <ul> <li> <strong>现状：</strong> 经过第一次训练（1-Rectified Flow），路径已经不交叉了（Uncrossed）。</li> <li> <strong>问题：</strong> 虽然不交叉，但路径可能是<strong>弯曲的</strong>（Curved）。见论文 Figure 2(b)。 <ul> <li>弯曲意味着速度 $v(z, t)$ 随时间变化，数值积分需要很多步（Step size 必须小）。</li> </ul> </li> <li> <strong>目标：</strong> 我们不仅要不交叉，还要<strong>直</strong>。</li> </ul> <h3 id="32-reflow递归优化的艺术">3.2 Reflow：递归优化的艺术</h3> <p>Reflow 是一个“自举”（Bootstrapping）过程：</p> <ol> <li> <strong>Step 1:</strong> 用训练好的模型生成一对新的数据 $(Z_0, Z_1)$。 <ul> <li>注意：这一对数据是通过 ODE 连接的，也就是它们已经是确定性耦合（Deterministic Coupling）。</li> </ul> </li> <li> <strong>Step 2:</strong> 把这对 $(Z_0, Z_1)$ 当作新的 Ground Truth，去训练下一个模型。 <ul> <li>我们在强迫新模型沿着 $Z_0$ 和 $Z_1$ 之间的<strong>直线</strong>走。</li> </ul> </li> <li> <strong>Step 3:</strong> 重复此过程。</li> </ol> <p><strong>定理 3.7 (Straightening):</strong> 随着 Reflow 次数 $k$ 增加，路径的“弯曲度”（Straightness error）会以 $1/k$ 的速度趋近于 0。</p> <ul> <li>最终结果：$Z_t = Z_0 + t(Z_1 - Z_0)$。</li> <li>此时，我们可以用 <strong>1 Step</strong> 生成图像！ <alphaxiv-paper-citation title="Theorem 3.7" page="8" first="Theorem 3.7." last="Let Zk be"></alphaxiv-paper-citation> </li> </ul> <h3 id="33-算法流程总结-algorithm-1-4">3.3 算法流程总结 (Algorithm 1-4)</h3> <p>让我们看伪代码（展示论文 Page 37 的 PyTorch 代码）：</p> <ol> <li> <strong>Train:</strong> 随机采样 $t$，计算 $X_t = tX_1 + (1-t)X_0$，Loss = MSE($v(X_t)$, $X_1-X_0$)。</li> <li> <strong>Sample:</strong> 用 ODE Solver (如 Euler) 从噪声解出图片。</li> <li> <strong>Reflow:</strong> 把 Sample 出来的图片对作为下一轮 Train 的输入。</li> </ol> <h3 id="34-distillation-蒸馏">3.4 Distillation (蒸馏)</h3> <ul> <li> <strong>最后一步优化：</strong> 当 Reflow 使得路径足够直之后，我们甚至不需要运行 ODE Solver。</li> <li> <strong>做法：</strong> 直接训练一个网络 $\hat{T}(z)$ 预测终点：$| \hat{T}(Z_0) - Z_1 |^2$。</li> <li>这实际上是在拟合 $t=0$ 时的速度场，实现真正的纳秒级推理。 <alphaxiv-paper-citation title="Distillation" page="8" first="distill the relation" last="without simulating the flow."></alphaxiv-paper-citation> </li> </ul> <h3 id="35-实验结果与总结">3.5 实验结果与总结</h3> <ul> <li> <strong>Figure 1 展示：</strong> <ul> <li>第一行：原始 Rectified Flow，生成质量不错，但需要多步。</li> <li>第二行：Reflow 一次后，使用 <strong>1-step</strong> 生成的结果，质量惊人地高。</li> </ul> </li> <li> <strong>对比：</strong> 相比于 Diffusion Models 需要蒸馏极其复杂的过程，Rectified Flow 通过几何上的“拉直”，天然地适合快速生成。</li> </ul> <hr> <h2 id="思考">思考</h2> <ol> <li>为什么说 MSE Loss 本质上是在做“模式平均”？这在多模态数据生成中会有什么潜在问题？</li> <li>Rectified Flow 与 Optimal Transport（最优传输）有什么联系？定理 3.5 中的“凸传输成本降低”意味着什么？</li> </ol> <p>好的，以下是为您准备的课后思考题前两题的参考答案及解析，供您在批改作业或下次课讲解时使用。</p> <hr> <h3 id="思考题-1为什么说-mse-loss-本质上是在做模式平均这在多模态数据生成中会有什么潜在问题"><strong>思考题 1：为什么说 MSE Loss 本质上是在做“模式平均”？这在多模态数据生成中会有什么潜在问题？</strong></h3> <h4 id="答案解析"><strong>答案解析</strong></h4> <ol> <li> <strong>MSE 与 模式平均 (Mode Averaging)：</strong> <ul> <li> <table> <tbody> <tr> <td> <strong>数学本质：</strong> 均方误差 (MSE) 损失函数 $L(\theta) = E[|y - f(x)|^2]$ 的全局最优解是条件期望 $f^*(x) = E[y</td> <td>x]$。</td> </tr> </tbody> </table> </li> <li> <strong>直观解释：</strong> 假设对于同一个输入 $x$（在 Rectified Flow 中是位置 $X_t$），有两个可能的正确目标值 $y_1$（向左走）和 $y_2$（向右走）。MSE 迫使模型输出这两个目标的平均值 $\frac{y_1 + y_2}{2}$（原地不动或垂直方向抵消）。模型无法同时向左又向右，只能妥协取中间。</li> <li>这就是所谓的“模式平均”。模型没有选择其中一个具体的模式（Mode），而是输出了所有模式的加权平均。</li> </ul> </li> <li> <strong>多模态数据中的潜在问题：</strong> <ul> <li> <strong>模糊生成 (Blurry Generation)：</strong> 在图像生成中，如果模型对于“画一只猫”还是“画一只狗”犹豫不决，MSE 会导致它画出一张猫狗重叠的、模糊不清的图片（平均像素值），而不是一只清晰的猫或一只清晰的狗。</li> <li> <strong>不切实际的样本：</strong> 在 Rectified Flow 的中间路径中，如果 $y_1$ 和 $y_2$ 之间的平均值 $\frac{y_1 + y_2}{2}$ 落在了一个低概率密度区域（例如，$y_1$ 是左车道，$y_2$ 是右车道，平均值是撞向中间的隔离带），那么模型生成的中间状态可能是不符合物理规律或数据分布的。</li> </ul> </li> <li> <strong>Rectified Flow 如何规避这个问题？</strong> <ul> <li>虽然 Rectified Flow 在训练中确实使用了 MSE 导致速度场取平均，但这是一个<strong>时间连续</strong>的过程。</li> <li>正如我们讲义中提到的 <strong>“Rewiring”</strong> 现象：模型在交叉点取平均速度，并不意味着它生成了模糊的终点图像。相反，它通过改变粒子的流向（红去蓝家，蓝去红家），使得每个粒子最终都能落入某个真实的高概率模态（清晰图片）中，只是可能不是它原本想去的那个。</li> <li> <strong>但是</strong>，如果在某些极端复杂的拓扑结构下，过度的平均化确实可能导致流场过于平滑，从而遗漏一些细微的分布特征。</li> </ul> </li> </ol> <hr> <h3 id="思考题-2rectified-flow-与-optimal-transport最优传输有什么联系定理-35-中的凸传输成本降低意味着什么"><strong>思考题 2：Rectified Flow 与 Optimal Transport（最优传输）有什么联系？定理 3.5 中的“凸传输成本降低”意味着什么？</strong></h3> <h4 id="答案解析-1"><strong>答案解析</strong></h4> <ol> <li> <strong>Rectified Flow 与 Optimal Transport (OT) 的联系：</strong> <ul> <li> <strong>目标一致：</strong> 两者都试图寻找一个映射 $T$，将分布 $\pi_0$ 搬运到 $\pi_1$，且都希望这个搬运过程代价最小。</li> <li> <strong>路径差异：</strong> <ul> <li> <strong>Optimal Transport (如 Wasserstein-2):</strong> 直接寻找全局最优的耦合 $(X_0, X_1)$，使得 $\int |x_0 - x_1|^2 d\pi$ 最小。这是一步到位的“上帝视角”优化，但在高维空间计算极其困难。</li> <li> <strong>Rectified Flow:</strong> 是一种<strong>迭代逼近</strong>的方法。第一次生成的流可能不是最优传输（路径可能有弯曲），但通过 <strong>Reflow</strong> 操作，我们不断地拉直路径、解开交叉。</li> </ul> </li> <li> <strong>一维情况：</strong> 在一维（1D）情况下，Rectified Flow 的不动点（Fixed Point）<strong>完全等价于</strong>最优传输映射（单调映射）。</li> <li> <strong>高维情况：</strong> Rectified Flow 产生的笔直路径对应于 OT 中的测地线（Geodesics）。Reflow 过程可以看作是一种贪心策略，不断向局部最优传输逼近。</li> </ul> </li> <li> <strong>“凸传输成本降低” (Reducing Convex Transport Costs) 的含义：</strong> <ul> <li> <strong>定理 3.5 回顾：</strong> 对于任意凸函数 $c$（如距离 $c(x)=|x|$ 或能量 $c(x)=|x|^2$），有 $E[c(Z_1 - Z_0)] \le E[c(X_1 - X_0)]$。</li> <li> <strong>几何解释（三角不等式）：</strong> <ul> <li>想象两条交叉的路径 $AC$ 和 $BD$（原始随机配对）。</li> <li>Rectified Flow 将其重组为 $AB$ 和 $CD$（如果不交叉的话，或者平行路径）。</li> <li>根据几何公理（或 Jensen 不等式），不交叉的路径总长度（或总能量）一定小于等于交叉路径的总长度。</li> </ul> </li> <li> <strong>物理意义：</strong> <ul> <li>这意味着经过整流后，把一堆沙子从 A 搬到 B 所做的<strong>总功（Total Work）</strong>变少了。</li> <li>虽然我们没有显式地去最小化某个特定的 Cost（比如 L2 Cost），但 Rectified Flow 的机制保证了<strong>所有</strong>凸形式定义的 Cost 都会自动下降。这是一个非常强大的“帕累托改进”（Pareto Improvement）。</li> </ul> </li> </ul> </li> </ol> <h1 id="定理33是什么意思">定理3.3是什么意思</h1> <p>论文中的 <strong>定理 3.3 (Marginal Preserving Property)</strong> 主要是证明了 Rectified Flow（整流流）生成的分布与原始插值过程的分布在任何时间点上都是完全一致的。</p> <p>简单来说，这意味着虽然 Rectified Flow 改变了粒子从起点到终点的具体路径（使它们不相交且更直），但在任何时刻 $t$，粒子群的整体形态（概率密度）并没有改变。</p> <p>以下是该定理的核心含义：</p> <h3 id="1-分布的一致性">1. 分布的一致性</h3> <p>该定理指出，在任意时间 $t \in [0, 1]$，Rectified Flow生成的变量 $Z_t$ 的边缘分布（Marginal Distribution）完全等同于原始插值过程 $X_t$ 的边缘分布。换句话说，$\text{Law}(Z_t) = \text{Law}(X_t)$。 <alphaxiv-paper-citation title="Marginal Law" page="6" first="the marginal law" last="at every time t,"></alphaxiv-paper-citation></p> <p>这意味着，如果你在 $t=0.5$ 时拍一张快照，观察所有点的位置，$Z_t$（ODE 模型生成的点）和 $X_t$（原始线性插值的点）看起来是一模一样的，尽管个体的运动轨迹已经发生了改变。</p> <h3 id="2-为什么会这样直观解释">2. 为什么会这样？（直观解释）</h3> <p>这是因为 Rectified Flow 的速度场 $v$ 被定义为原始路径 $X_t$ 的“期望速度”。模型学习的是在特定位置和时间下，所有经过该点的原始路径的平均方向。</p> <ul> <li>只要流入和流出每个微小区域的“流量”保持不变，整体的密度分布就会保持不变。 <alphaxiv-paper-citation title="Flow Intuition" page="6" first="Flow in &amp;" last="Law(Xt), forall t."></alphaxiv-paper-citation> </li> <li>原始插值 $X_t$ 可能是杂乱无章、相互交叉的（非马尔可夫过程），而 Rectified Flow $Z_t$ 将其重组为一种确定的、不交叉的 ODE 轨迹（马尔可夫过程），但两者的宏观统计特性（边缘分布）是一样的。 <alphaxiv-paper-citation title="Causalization" page="6" first="Zt causalizes," last="at all time."></alphaxiv-paper-citation> </li> </ul> <h3 id="3-重要意义">3. 重要意义</h3> <p>这个性质保证了 Rectified Flow 是一个有效的生成模型。</p> <ul> <li>已知 $X_0$ 服从噪声分布 $\pi_0$，$X_1$ 服从数据分布 $\pi_1$。</li> <li>根据定理 3.3，因为 $\text{Law}(Z_t) = \text{Law}(X_t)$，所以 $Z_0$ 也服从 $\pi_0$，且 $Z_1$ 也服从 $\pi_1$。</li> <li>这证明了该方法能够成功地将噪声分布 $\pi_0$ 传输到目标数据分布 $\pi_1$。 <alphaxiv-paper-citation title="Valid Coupling" page="6" first="The pair (Z0, Z1)" last="of pi0 and pi1."></alphaxiv-paper-citation> </li> </ul> <h1 id="lawzt-代表着什么">$Law(Zt)$ 代表着什么</h1> <p>在数学和概率论的语境下，特别是在这篇论文中，$\text{Law}(Z_t)$ 代表的是随机变量 $Z_t$ 的<strong>概率分布（Probability Distribution）</strong>或<strong>边缘分布（Marginal Distribution）</strong>。</p> <p>通俗地说，它描述了在时间 $t$ 时，所有生成的样本（粒子）在空间中是如何分布的。</p> <p>以下是具体的解释：</p> <h3 id="1-数学定义">1. 数学定义</h3> <p>$\text{Law}(Z_t)$ 指的是随机变量 $Z_t$ 服从的概率法则。</p> <ul> <li>如果 $Z_t$ 是一个连续变量（比如图像的像素值），$\text{Law}(Z_t)$ 通常对应于它的<strong>概率密度函数（Probability Density Function, PDF）</strong>，记作 $p_t(z)$ 或 $\rho_t(z)$。</li> <li>它告诉我们在时间 $t$，粒子出现在空间中某个特定位置的可能性有多大。</li> </ul> <h3 id="2-在论文中的具体含义">2. 在论文中的具体含义</h3> <p>在 Rectified Flow 模型中，整个过程是一个随着时间 $t$（从 0 到 1）变化的流动过程：</p> <ul> <li> <strong>$Z_t$</strong>：代表单个数据点（例如一张图片）在时间 $t$ 的状态。</li> <li> <strong>$\text{Law}(Z_t)$</strong>：代表在时间 $t$ 时，<strong>整个群体</strong>的状态。</li> </ul> <p>你可以把它想象成一群鸟在飞行：</p> <ul> <li>$Z_t$ 是<strong>一只鸟</strong>在 $t$ 时刻的位置。</li> <li>$\text{Law}(Z_t)$ 是<strong>整个鸟群</strong>在 $t$ 时刻的整体形状和密度。</li> </ul> <h3 id="3-结合定理-33-的理解">3. 结合定理 3.3 的理解</h3> <p>论文中强调 $\text{Law}(Z_t) = \text{Law}(X_t)$，这表示：</p> <ul> <li> <strong>$X_t$（线性插值）</strong>：这是简单的直线连接，虽然路径简单，但如果从个体看，不同的路径可能会杂乱无章地交叉。</li> <li> <strong>$Z_t$（Rectified Flow）</strong>：这是经过“整流”后的 ODE 路径，粒子互不交叉，路径更直。</li> </ul> <p>尽管个体的运动方式（微观路径）完全不同，但在宏观上，<strong>鸟群的形状（即 $\text{Law}(Z_t)$）在任何时刻看起来都是完全一样的</strong>。 <alphaxiv-paper-citation title="Marginal Law" page="6" first="the marginal law" last="at every time t,"></alphaxiv-paper-citation></p> <p>这也是为什么该方法有效的关键：因为它保证了最终时刻 $t=1$ 时，模型的分布 $\text{Law}(Z_1)$ 必定等于目标数据的分布 $\pi_1$。 <alphaxiv-paper-citation title="Marginal Preserving" page="13" first="Law(Zt) =" last="Law(Xt), forall t"></alphaxiv-paper-citation></p> <h1 id="概率流关心的是整个概率分布随着时间的变化它关心的是一个宏面的变化-而对于空间中的一个变量而言-并不太关注">概率流关心的是整个概率分布随着时间的变化,它关心的是一个宏面的变化, 而对于空间中的一个变量而言, 并不太关注</h1> <p>你的理解非常到位，但这只触及了 <strong>Rectified Flow</strong> 核心思想的一半。</p> <p>你说得对：<strong>定理 3.3 (Marginal Preserving Property)</strong> 确实保证了在<strong>宏观（Macro）</strong>层面，概率分布随时间的演变是正确的。只要两群“云团”（分布）在每一时刻的形状重合，模型就是有效的。</p> <p>但是，这篇论文的精髓恰恰在于它<strong>也非常关注微观（Micro）层面</strong>，也就是空间中每一个变量（粒子）的具体行走路径。</p> <p>这里有一个关键的区别：</p> <h3 id="1-宏观相同微观不同">1. 宏观相同，微观不同</h3> <p>虽然 $X_t$（原始插值）和 $Z_t$（Rectified Flow）在每一时刻的宏观分布 $\text{Law}(\cdot)$ 是一样的，但它们微观上的<strong>“交通状况”</strong>完全不同：</p> <ul> <li> <strong>$X_t$ (原始插值 - 混乱):</strong> 粒子可能是随机配对的。想象一个广场上的人群要走到对面，如果大家闭着眼瞎走（虽然走直钱），就会互相撞在一起（路径交叉）。虽然某个时刻广场中心的人密度是对的，但个人的路径是混乱的、非最优的。 <alphaxiv-paper-citation title="Crossing Paths" page="5" first="On the other hand," last="makes it non-causal."></alphaxiv-paper-citation> </li> <li> <strong>$Z_t$ (Rectified Flow - 有序):</strong> 模型把这些路径“重组”了。它强制粒子<strong>不交叉</strong>（Non-crossing）。就像指挥交通一样，让每个人走最短、最顺畅的路径到达目标分布的某个位置。 <alphaxiv-paper-citation title="Flows Avoid Crossing" page="5" first="rectified flow rewires" last="to avoid crossing,"></alphaxiv-paper-citation> </li> </ul> <h3 id="2-为什么要关注微观路径">2. 为什么要关注微观路径？</h3> <p>如果只关心最终分布，为什么还要费劲去优化微观路径呢？论文给出了三个极其重要的理由：</p> <ol> <li> <strong>确定性 (Determinism):</strong> $Z_t$ 是一个<strong>常微分方程 (ODE)</strong>，这意味着路径是确定的。给定一个起点，只有一条路走到终点。而原始的 $X_t$ 往往是随机耦合的，不具备这种性质。 <alphaxiv-paper-citation title="Deterministic Coupling" page="5" first="rebuild a more" last="pairing of (Z0, Z1)."></alphaxiv-paper-citation> </li> <li> <strong>传输成本 (Transport Cost):</strong> Rectified Flow 会自动寻找“更短”的路径。虽然大家最后都到了终点，但 $Z_t$ 让群体走的平均总路程更短（凸传输成本降低）。 <alphaxiv-paper-citation title="Reduce Cost" page="5" first="Rectify(.) converts an" last="convex transport costs."></alphaxiv-paper-citation> </li> <li> <strong>生成速度 (Simulation Speed) —— 最关键点:</strong> 这是这篇论文最大的卖点。如果微观路径是<strong>笔直</strong>的（Straight），我们在生成图像时就不需要走 100 步或 1000 步，可能只需要 <strong>1 步</strong>（Euler step）就能从噪声跳到图像。只有微观路径被“拉直”了，快速生成才成为可能。 <alphaxiv-paper-citation title="Fast Simulation" page="5" first="perfectly straight paths" last="one-step model."></alphaxiv-paper-citation> </li> </ol> <h3 id="总结">总结</h3> <ul> <li> <strong>定理 3.3</strong> 保证了<strong>宏观</strong>上的正确性（我们确实生成了目标分布）。</li> <li> <strong>Rectified Flow 的算法</strong> 优化了<strong>微观</strong>上的路径（让路径变直、不交叉）。</li> </ul> <p>所以，虽然概率流确实描述了宏观变化，但这篇论文的独创性在于：<strong>它在保持宏观分布不变的前提下，把微观路径梳理得井井有条。</strong></p> <h1 id="reflow-详细含义">Reflow 详细含义</h1> <p><strong>Reflow（回流/重流）</strong> 是这篇论文提出的一个核心操作，它是一种<strong>递归（Recursive）的训练过程</strong>。</p> <p>简单来说，Reflow 就是<strong>“用上一轮训练好的模型生成的数据对，来训练下一轮的模型”</strong>。</p> <p>这个过程的目的只有一个：<strong>把传输路径“拉直”</strong>。</p> <h3 id="1-reflow-的具体步骤">1. Reflow 的具体步骤</h3> <p>Reflow 的操作流程如下：</p> <ol> <li> <strong>第一步 (Rectified Flow 1):</strong> 使用原始数据对 $(X_0, X_1)$（通常是随机配对的噪声和图像）训练第一个 ODE 模型。</li> <li> <strong>第二步 (生成新配对):</strong> 使用训练好的第一个模型，将 $X_0$ 传输到 $X_1$。这会产生一组新的配对 $(Z_0, Z_1)$。注意，这里的 $Z_0$ 和 $Z_1$ 是通过 ODE 轨迹连接的，它们不再是随机配对，而是有了确定性的联系。</li> <li> <strong>第三步 (Rectified Flow 2 - Reflow):</strong> 将这组新的配对 $(Z_0, Z_1)$ 作为“Ground Truth”训练数据，去训练第二个 ODE 模型。</li> <li> <strong>重复:</strong> 这个过程可以重复 $k$ 次，得到 $k$-Rectified Flow。 <alphaxiv-paper-citation title="Recursive Reflow" page="5" first="Applying this operator recursively" last="from (X0, X1)."></alphaxiv-paper-citation> </li> </ol> <h3 id="2-reflow-的作用把路拉直">2. Reflow 的作用：把路“拉直”</h3> <p>在第一次训练时，模型找到的路径可能还是弯曲的。但是，Reflow 利用了上一轮生成的轨迹作为引导，试图寻找连接起点和终点的直线路径。</p> <ul> <li> <strong>直观比喻：</strong> 想象一根弯曲的铁丝连接两点。Reflow 就像是用力拉这根铁丝的两端。拉一次，它变直一点；再拉一次，变得更直。</li> <li> <strong>论文结论：</strong> 随着 Reflow 次数的增加，流动的路径会变得越来越直（Straight）。 <alphaxiv-paper-citation title="Straightening" page="5" first="has the important effect" last="straightening paths"></alphaxiv-paper-citation> </li> </ul> <h3 id="3-为什么要把路拉直为了速度">3. 为什么要“把路拉直”？（为了速度）</h3> <p>这是 Reflow 最具实战价值的地方。</p> <ul> <li> <strong>弯曲的路径：</strong> 模拟 ODE 需要很多步（例如 100 步 Euler 积分）才能精确地沿着弯路走到底，否则会掉出轨道。</li> <li> <strong>笔直的路径：</strong> 如果路径是一条完美的直线，理论上只需要 <strong>1 步</strong>（Single Euler Step）就可以从起点直接跳到终点，没有任何误差。</li> </ul> <p>因此，Reflow 使得模型在推理（Inference）阶段可以极快地生成图像。论文展示了经过 Reflow 后，甚至可以用 <strong>1 步</strong> 生成高质量图像，这在以前的扩散模型中是很难做到的。 <alphaxiv-paper-citation title="One-step Model" page="5" first="simulated exactly with" last="one-step model."></alphaxiv-paper-citation></p> <h3 id="总结-1">总结</h3> <p><strong>Reflow</strong> 是一种自我改进的迭代算法。它通过不断地“自己向自己学习”（Bootstrapping），把混乱、弯曲的传输路径逐渐优化成整齐、笔直的路径，从而实现<strong>极速生成</strong>。</p> <h1 id="论文里的-figure-2">论文里的 Figure 2</h1> <p>Figure 2 是这篇论文中最直观、最核心的一张图。它用一个二维的玩具示例（Toy Example），完整演示了 <strong>Rectified Flow（整流流）</strong> 和 <strong>Reflow（回流）</strong> 是如何工作的。</p> <p>这张图展示了从杂乱无章的随机配对，演变成整齐划一的笔直路径的全过程。</p> <p>我们可以按照 (a) -&gt; (b) -&gt; (c) -&gt; (d) 的顺序来详细解读：</p> <h3 id="a-初始状态线性插值-linear-interpolation-of-input"><strong>(a) 初始状态：线性插值 (Linear Interpolation of Input)</strong></h3> <ul> <li> <strong>图示内容：</strong> 图中的黑线连接了起点 $X_0$（来自分布 $\pi_0$）和终点 $X_1$（来自分布 $\pi_1$）。</li> <li> <strong>含义：</strong> 这里展示的是原始训练数据。通常我们在训练生成模型时，噪声 $X_0$ 和数据 $X_1$ 是<strong>独立采样</strong>的，也就是<strong>随机配对</strong>。</li> <li> <strong>关键问题：</strong> 注意看中间的区域，大量的线是<strong>相互交叉（Crossing）</strong>的。 <ul> <li>这就像在一个繁忙的广场，每个人都闭着眼睛直着走向对面的随机位置，必然会发生大量的碰撞。</li> <li>这种交叉使得我们很难直接学习一个简单的 ODE，因为在交叉点上，速度场是多值的（同一个位置既要往左又要往右）。 <alphaxiv-paper-citation title="Crossing Paths" page="4" first="Linear interpolation of" last="X0, X1)."></alphaxiv-paper-citation> </li> </ul> </li> </ul> <h3 id="b-第一步整流rectified-flow-z_t"><strong>(b) 第一步整流：Rectified Flow ($Z_t$)</strong></h3> <ul> <li> <strong>图示内容：</strong> 这是基于 (a) 中的数据训练出的第一个 ODE 模型的轨迹。</li> <li> <strong>发生了什么（Rewiring）：</strong> 模型并没有傻傻地去模仿 (a) 中那些交叉的路径。相反，它学习了一种<strong>不交叉</strong>的流动方式。 <ul> <li>注意看，原本交叉的路径被“重连”（Rewired）了。粒子不再穿过彼此，而是顺滑地绕开。</li> <li>这对应了之前提到的 <strong>定理 3.5 (减少传输成本)</strong>：通过消除交叉，整体的传输路径变顺畅了。</li> </ul> </li> <li> <strong>局限性：</strong> 虽然路径不交叉了，但请注意线条是<strong>弯曲（Curved）</strong>的。 <ul> <li>弯曲的路径意味着速度不是常数，因此在生成图片时，我们需要走很多小步（Step）才能沿着曲线准确到达终点。 <alphaxiv-paper-citation title="Rewiring" page="4" first="rewired at the" last="avoid the crossing."></alphaxiv-paper-citation> </li> </ul> </li> </ul> <h3 id="c-reflow-的准备端点的新线性插值"><strong>(c) Reflow 的准备：端点的新线性插值</strong></h3> <ul> <li> <strong>图示内容：</strong> 这一步是 <strong>Reflow</strong> 操作的关键。我们取图 (b) 中生成的起点 $Z_0$ 和对应的终点 $Z_1$，然后直接画直线连接它们。</li> <li> <strong>关键区别：</strong> 虽然看起来和 (a) 有点像，但这时的配对 $(Z_0, Z_1)$ <strong>不再是随机的</strong>。 <ul> <li>这一对点是由图 (b) 的 ODE 轨迹“绑定”在一起的。</li> <li>因为图 (b) 中的轨迹没有交叉，所以图 (c) 中连接端点的直线也几乎<strong>没有交叉</strong>。</li> </ul> </li> <li> <strong>目的：</strong> 我们构建了一个新的训练数据集，告诉下一个模型：“请尝试沿着这些直线走”。 <alphaxiv-paper-citation title="Reflow Setup" page="4" first="Linear interpolation of" last="of flow Zt."></alphaxiv-paper-citation> </li> </ul> <h3 id="d-第二步整流2-rectified-flow-z_t"><strong>(d) 第二步整流：2-Rectified Flow ($Z’_t$)</strong></h3> <ul> <li> <strong>图示内容：</strong> 这是基于 (c) 的数据训练出的第二个 ODE 模型。</li> <li> <strong>最终结果：</strong> 此时的流动轨迹变成了<strong>笔直的线（Straight Paths）</strong>。</li> <li> <strong>为什么这很重要？</strong> <ul> <li>这是 Reflow 的最终目的。既然路径是直线的，那么速度就是恒定的。</li> <li>这意味着我们在生成时，可以<strong>一步到位</strong>（One-step generation）。不需要慢慢积分，直接 $Z_1 = Z_0 + v \times 1$ 就能得到极高质量的结果。</li> <li>这就是为什么论文题目叫 “Flow Straight and Fast”（流得直，流得快）。 <alphaxiv-paper-citation title="Straight Flow" page="4" first="The rectified flow" last="follows straight paths."></alphaxiv-paper-citation> </li> </ul> </li> </ul> <h3 id="总结-figure-2"><strong>总结 Figure 2</strong></h3> <p>这张图讲述了一个<strong>“从混乱到有序，从弯曲到笔直”</strong>的故事：</p> <ol> <li> <strong>(a)</strong> 随机配对，路径交叉混乱。</li> <li> <strong>(b)</strong> 第一次整流，解开交叉，但路径弯曲。</li> <li> <strong>(c)</strong> 利用 (b) 的结果重新配对。</li> <li> <strong>(d)</strong> 第二次整流（Reflow），路径变直，实现快速生成。</li> </ol> <h1 id="为什么ode模型学出来的是没有交叉的">为什么ODE模型学出来的是没有交叉的</h1> <p>这是一个非常深刻的问题！你其实已经触碰到了问题的本质：<strong>ODE 的存在唯一性定理（Picard-Lindelöf theorem）确实是结果，但导致这个结果的根本原因在于“损失函数的性质”。</strong></p> <p>简单来说：<strong>因为神经网络学到的是“平均速度”，而平均值是唯一的。</strong></p> <p>以下是详细的推导和解释：</p> <h3 id="1-训练目标最小二乘法-least-squares">1. 训练目标：最小二乘法 (Least Squares)</h3> <p>让我们回到论文中的优化目标公式（Algorithm 1 中的 Training）： \(\hat{\theta} = \arg \min\_{\theta} E \left[ \| (X_1 - X_0) - v(X_t, t) \|^2 \right]\) 这里，模型 $v(X_t, t)$ 试图去拟合数据的漂移方向 $(X_1 - X_0)$。</p> <p>在统计学中，对于均方误差（MSE）损失函数，其理论上的最优解（Global Minimum）是目标值的<strong>条件期望（Conditional Expectation）</strong>。 论文在公式 (2) 中明确指出了这一点： \(v^\*(x, t) = E[X_1 - X_0 | X_t = x]\)</p> <alphaxiv-paper-citation title="Optimal Velocity" page="6" first="vX (x, t) =" last="| Xt=x],"></alphaxiv-paper-citation> <h3 id="2-这里的平均意味着什么">2. 这里的“平均”意味着什么？</h3> <p>想象在图 2(a) 中的交叉点 $x$：</p> <ul> <li>假设有两条原始路径经过 $x$。</li> <li>路径 A 的方向是向左上（速度 $v_A$）。</li> <li>路径 B 的方向是向右上（速度 $v_B$）。</li> </ul> <p>在原始数据中，这确实是一个“多值函数”（Multi-valued function），因为同一个位置 $x$ 有两个不同的速度。</p> <p>但是，<strong>神经网络无法输出两个值</strong>。当它试图同时最小化与 $v_A$ 和 $v_B$ 的误差时，它学会的 $v^*(x, t)$ 是这两者的<strong>平均值</strong>（或者是加权平均）： \(v^\*(x, t) \approx \frac{v_A + v_B}{2}\)</p> <h3 id="3-单值向量场导致不交叉">3. 单值向量场导致不交叉</h3> <p>关键就在这里：<strong>通过取期望（平均），我们将混乱的、多值的原始速度场，“坍缩”成了一个平滑的、单值的向量场。</strong></p> <ul> <li> <strong>单值性（Single-valued）：</strong> 在任何特定的时间 $t$ 和位置 $x$，学出来的速度场 $v(x, t)$ 只有一个确定的方向向量。</li> <li> <strong>不交叉原理：</strong> 如果两条轨迹在时间 $t$ 相交于 $x$，那么它们在这一点必须拥有完全相同的速度 $v(x, t)$。既然位置相同、速度相同，根据 ODE 的唯一性定理，它们的<strong>未来轨迹</strong>也必须完全重合。</li> </ul> <p>因此，模型不可能生成像图 2(a) 那样“穿过彼此”的路径，因为那要求在交叉点有两个不同的速度。</p> <h3 id="4-总结">4. 总结</h3> <p>学出来的 ODE 模型没有交叉，不是因为我们在训练中强制加了“不许交叉”的约束，而是因为：</p> <ol> <li> <strong>数学上：</strong> 我们用 MSE 拟合，迫使模型在交叉点学习<strong>平均速度</strong>。</li> <li> <strong>物理上：</strong> 平均速度场是一个单值函数。</li> <li> <strong>结果：</strong> 单值向量场生成的积分曲线（Integral Curves）天然就是互不交叉的。</li> </ol> <p>这就是为什么论文中说 Rectified Flow 将 $X_t$（非马尔可夫、多值速度）转化为了 $Z_t$（马尔可夫、单值速度、确定性）的原因。 <alphaxiv-paper-citation title="Derandomization" page="6" first="Zt causalizes, Markovianizes" last="derandomizes Xt,"></alphaxiv-paper-citation></p> <h1 id="在相交点的速度一样位置一样-能推出未来轨迹也完全重合但是为什么能够推出来-figure-2中-b-里面的轨迹调头了-比如蓝色的线">在相交点的速度一样,位置一样, 能推出未来轨迹也完全重合,但是为什么能够推出来 Figure 2中 (b) 里面的轨迹调头了, 比如蓝色的线.</h1> <p>这是一个观察极其敏锐的问题！你注意到了 Figure 2(b) 中一个看似矛盾的现象：</p> <p><strong>既然 ODE 保证了“位置一样、速度一样则轨迹重合”，那为什么图中的红线和蓝线在靠近中心时，看起来像是“相撞然后弹开（调头）”了，而不是合并成一条线呢？</strong></p> <p>答案其实包含两个层面：<strong>微观的“不接触”</strong> 和 <strong>宏观的“目的地重组（Rewiring）”</strong>。</p> <h3 id="1-微观层面它们其实并没有真正碰到">1. 微观层面：它们其实并没有真正碰到</h3> <p>虽然在图上看它们挤在一起，但在数学上，在任何时刻 $t$，不同的轨迹是<strong>绝对没有相交</strong>的。</p> <ul> <li> <strong>渐近线效应：</strong> 当两股相反的流（红线向下，蓝线向上）靠近时，中间区域的学习到的速度场 $v(x,t)$ 会变成两者的<strong>平均值</strong>（比如变成水平向右）。</li> <li> <strong>平滑过渡：</strong> 粒子进入这个区域后，会被这个“平均流”带着走。它们会无限靠近，变得几乎平行，但在数学上保持着微小的距离。</li> <li> <strong>因为没碰到，所以没合并：</strong> 既然没有真正达到 $x_{red} = x_{blue}$ 的那个点，唯一性定理就不强制它们合并。它们只是在很长一段路上“并肩同行”了。</li> </ul> <h3 id="2-宏观层面最关键的原因换了目的地rewiring">2. 宏观层面：最关键的原因——“换了目的地”（Rewiring）</h3> <p>你看到的“调头”或“弹开”，本质上是<strong>Rectified Flow 改变了粒子的命运（终点）</strong>。</p> <p>让我们看看图 2(a) 的原始情况：</p> <ul> <li> <strong>红线：</strong> 从左上角出发 $\to$ 想去右下角。</li> <li> <strong>蓝线：</strong> 从左下角出发 $\to$ 想去右上角。</li> <li> <strong>结果：</strong> 它们必须在中间形成一个巨大的 <strong>“X”</strong> 型交叉。</li> </ul> <p>现在看看图 2(b) 的 Rectified Flow 是怎么处理的： 模型发现走 “X” 型路径代价太大了（不仅路程远，还要打架）。为了“拉直”路径并避免交叉，模型做了一个聪明的决定：<strong>交换目的地</strong>。</p> <ul> <li> <strong>原来的计划：</strong> 上 $\to$ 下，下 $\to$ 上（交叉）。</li> <li> <strong>新的计划（整流后）：</strong> 上 $\to$ 上，下 $\to$ 下（平行）。</li> </ul> <p><strong>这就是“调头”的真相：</strong></p> <ol> <li>蓝线原本冲着右上角去（向上冲）。</li> <li>走到中间时，碰到了原本要往下的红线流。</li> <li>混合后的速度场告诉它们：“别打架了，你们互换目标吧。”</li> <li> <strong>蓝线被“压”了下去</strong>，顺滑地流向了右下角（或者保持在下方）。</li> <li> <strong>红线被“抬”了上去</strong>，顺滑地流向了右上角（或者保持在上方）。</li> </ol> <p>这种现象在论文中被称为 <strong>Rewiring（重连/重组）</strong>。</p> <blockquote> <p>“The rectified flow $Z_t$ rewires the pairing… to avoid the crossing.” (Rectified Flow重新连接了配对…以避免交叉。) <alphaxiv-paper-citation title="Rewiring" page="4" first="rewired at the" last="avoid the crossing."></alphaxiv-paper-citation></p> </blockquote> <h3 id="3-总结">3. 总结</h3> <p>Figure 2(b) 里的轨迹并没有违反 ODE 唯一性定理。</p> <ul> <li>它们看起来像“弹开”，是因为模型发现<strong>“与其硬着头皮穿过对方，不如直接去对方原来的目的地”</strong>更省力。</li> <li>这种<strong>单调性（Monotonicity）</strong>的恢复（即上面的人去上面的终点，下面的人去下面的终点），是降低传输成本的必然结果，也是图像生成中“平滑变换”的几何本质。</li> </ul> <h1 id="rewired-现象-我理解是因为优化的本质导致的这个现象-虽然在原始的随机数据中有类似图-a-的数据但当模型见完所有的数据之后-发现其实有捷径可以走-所以随着模型的训练学到的速度场就偏向于走直线-我理解的对吗-但是如果这样的话训炼时的-那些非直接路径的数据对充当了什么作用-比如figure-2-图a中的对角线数据对">“rewired” 现象, 我理解是因为优化的本质导致的这个现象, 虽然在原始的随机数据中有类似图 (a) 的数据,但当模型见完所有的数据之后, 发现其实有捷径可以走, 所以随着模型的训练,学到的速度场就偏向于走直线? 我理解的对吗, 但是如果这样的话,训炼时的 那些非直接路径的数据对充当了什么作用, 比如Figure 2 图(a)中的对角线数据对</h1> <p>你的直觉非常敏锐，<strong>你的理解基本上是完全正确的</strong>。但关于“那些对角线（交叉）数据到底起了什么作用”，这是一个非常精彩的数学机制问题。</p> <p>让我们深入剖析一下：为什么看似“错误”或“低效”的原始交叉数据，却是形成最终完美直线的<strong>必要原材料</strong>。</p> <h3 id="1-确认你的理解捷径是算出来的">1. 确认你的理解：捷径是“算”出来的</h3> <p>你说得对，随着训练，模型确实发现“有捷径可走”。</p> <ul> <li> <strong>原始数据 (图 a)</strong> 说：“必须由 A 走到 D，由 B 走到 C（交叉）”。</li> <li> <strong>模型 (图 b)</strong> 发现：“其实让 A 走到 C，让 B 走到 D（平行），也能完成把一堆人从左边送到右边的任务，而且这样总路程更短。”</li> </ul> <p>这就是 <strong>Rewiring（重连）</strong>。模型不再执着于“谁具体去了哪里”，它只关心“最后大家都有位置坐”。</p> <hr> <h3 id="2-核心问题那些对角线数据充当了什么作用">2. 核心问题：那些“对角线数据”充当了什么作用？</h3> <p>你可能会问：“既然模型最后不想走对角线，那训练数据里有对角线岂不是误导模型吗？”</p> <p>答案是：<strong>绝对不是误导，它们通过“向量合成”提供了关键的动力。</strong></p> <p>让我们用向量来做个思想实验，解释图 2(a) 中间那个交叉点发生了什么：</p> <h4 id="场景设定">场景设定</h4> <p>假设在中心点 $P$，有两组数据经过：</p> <ol> <li> <strong>数据对 1 (往右上):</strong> 这是一个对角线数据。它的漂移方向向量是 $v_1 = (1, 1)$ （向右走 1，向上走 1）。</li> <li> <strong>数据对 2 (往右下):</strong> 这是另一个对角线数据。它的漂移方向向量是 $v_2 = (1, -1)$ （向右走 1，向下走 1）。</li> </ol> <h4 id="训练过程">训练过程</h4> <p>模型在点 $P$ 处训练，试图最小化误差。根据论文公式 (2)，它学到的是<strong>期望（平均值）</strong>： \(v\_{model}(P) \approx \frac{v_1 + v_2}{2} = \frac{(1, 1) + (1, -1)}{2} = \frac{(2, 0)}{2} = (1, 0)\)</p> <h4 id="结果分析">结果分析</h4> <ul> <li> <strong>垂直方向：</strong> “向上”的意图（来自数据 1）和“向下”的意图（来自数据 2）互相<strong>抵消</strong>了 ($1 + (-1) = 0$)。</li> <li> <strong>水平方向：</strong> “向右”的意图被<strong>保留并加强</strong>了 ($1 + 1 = 2$)。</li> </ul> <h4 id="结论对角线数据的作用">结论：对角线数据的作用</h4> <p>那些“非直接路径”的数据并没有被浪费，它们的作用是：</p> <ol> <li> <strong>抵消垂直动量：</strong> 正是因为有从左下往右上的数据，也有从左上往右下的数据，模型才知道“哦，在这个区域，大家不要乱上下跑，老老实实往右走就行”。</li> <li> <strong>提供水平动量：</strong> 它们都贡献了“往右走”的信息。</li> </ol> <p>如果没有这些对角线数据，模型根本不知道中间这块区域有“流量”，也就无法把路铺通。</p> <hr> <h3 id="3-更深层的理解任务的本质变化">3. 更深层的理解：任务的本质变化</h3> <p>这里有一个视角的转换，非常关键：</p> <ul> <li> <strong>Supervised Learning (原始数据的视角):</strong> 这是一个<strong>点对点</strong>的任务。$X_0^{(i)}$ 必须映射到 $X_1^{(i)}$。如果是这样，交叉数据确实是“坏”数据。</li> <li> <strong>Generative Modeling (Rectified Flow 的视角):</strong> 这是一个<strong>分布对分布</strong>的任务。任务变成了：“把左边这一群人 $\pi_0$，搬运到右边变成那一群人 $\pi_1$，<strong>具体谁变成谁我不关心</strong>。”</li> </ul> <p><strong>对角线数据的作用在于定义了边界条件（边缘分布）。</strong></p> <ul> <li>它们告诉模型：“不管你怎么走，最后时刻 $t=1$ 时，右上角必须有一个人，右下角也必须有一个人。”</li> <li>模型（速度场）收到这个指令后，通过计算平均速度，自动规划出了成本最低的方案：“既然右上角需要人，那我就让原本就在上面的人去右上角吧（蓝线变平），别让下面的人费劲跑上去了。”</li> </ul> <h3 id="总结-2">总结</h3> <p>你眼中的“非直接路径数据”（交叉数据），实际上是<strong>通过“相互冲突”来达成“相互妥协”</strong>。</p> <ul> <li>冲突（一个想上，一个想下）导致了垂直速度的<strong>归零</strong>。</li> <li>一致（都想往右）导致了水平速度的<strong>保留</strong>。</li> <li>结果就是：<strong>所有的弯路都被平均成了直路。</strong> </li> </ul> <table> <tbody> <tr> <td>这就是数学期望 $E[X_1 - X_0</td> <td>X_t=x]$ 的魔力。 &lt;alphaxiv-paper-citation title=”Optimal Velocity” page=”6” first=”vX (x, t) =” last=”</td> <td>Xt = x],” /&gt;</td> </tr> </tbody> </table> <h1 id="既然从-supervised-learning-原始数据的视角-这是一个点对点的任务-而优化的loss-也是为了让它们点对点但是为什么随着优化的进行就不是点对点了呢-而是拐弯了">既然从 supervised learning (原始数据的视角), 这是一个点对点的任务, 而优化的loss 也是为了让它们点对点,但是为什么随着优化的进行,就不是点对点了呢, 而是拐弯了</h1> <p>这又是直觉和数学产生碰撞的一个极佳问题。</p> <p>你的困惑在于：<strong>既然 Loss Function 是为了让 $v$ 拟合 $(X_1 - X_0)$（点对点），模型难道不应该拼命记住“我要去 $X_1$”吗？为什么最后反而“变节”了，走出了完全不同的路径？</strong></p> <p>核心原因在于模型的<strong>“失忆症”</strong>（Memoryless Property）。</p> <p>问题的关键不在于 Loss Function，而在于<strong>神经网络的输入</strong>是什么。</p> <h3 id="1-模型的视角-vs-上帝视角">1. 模型的视角 vs. 上帝视角</h3> <ul> <li> <strong>上帝视角（训练数据）知道：</strong> “我是粒子 A，我从 $(0,0)$ 出发，我的终点是 $(1,1)$。”</li> <li> <strong>模型的视角（输入）只有：</strong> “我现在在位置 $x$，时间是 $t$。我该往哪走？”</li> </ul> <p><strong>请注意：模型的输入里没有 $X_0$（起点）！</strong> \(v = \text{Model}(X_t, t)\) 而不是 $v = \text{Model}(X_t, X_0, t)$。</p> <h3 id="2-路标的比喻">2. “路标”的比喻</h3> <p>想象在一个森林里（空间 $X$）。</p> <ul> <li> <strong>数据 A（红队）</strong>：想从南边走到西北角。</li> <li> <strong>数据 B（蓝队）</strong>：想从南边走到东北角。</li> <li> <strong>交叉点：</strong> 它们在森林中间的一棵大树下相遇了。</li> </ul> <p>现在，你要在这棵树上钉一个<strong>路标（模型）</strong>。</p> <ul> <li>Loss Function 告诉你：“路标指的方向要尽可能接近行人想去的方向。”</li> <li>红队路过时大喊：“路标要指西北！”</li> <li>蓝队路过时大喊：“路标要指东北！”</li> <li> <strong>约束：</strong> 这棵树上<strong>只能钉一个路标</strong>（因为对于同一个 $x, t$，函数 $v(x,t)$ 只能有一个输出）。</li> </ul> <p><strong>结果会怎样？</strong> 为了让两队人都“没那么生气”（最小化均方误差 MSE），你只能把路标指向<strong>正北方</strong>。</p> <h3 id="3-拐弯是如何发生的">3. “拐弯”是如何发生的？</h3> <p>现在，一个新的粒子（或者原来的粒子）走到了这棵树下。</p> <ol> <li>它看到了路标指向<strong>正北方</strong>。</li> <li>它顺从地往正北走了一步。</li> <li> <strong>关键点来了：</strong> 这一步让它偏离了原本红队想去的“西北”，也偏离了蓝队想去的“东北”。</li> <li>在下一个位置，它又遇到了新的路标（新的平均值）。</li> <li>这一连串的“妥协”和“平均”，连接起来就形成了一条<strong>全新的、弯曲的路径</strong>。</li> </ol> <p>这条路径不再属于 A，也不再属于 B，它是所有经过这里的路径的<strong>共识</strong>。</p> <h3 id="4-为什么这不算失败">4. 为什么这不算“失败”？</h3> <p>你可能会问：“模型没有把 A 送到 A 的终点，这不是训练失败了吗？”</p> <p>从<strong>点对点（L2 Loss）</strong>的角度看，确实，每一个单独样本的 Loss 并没有降到 0（除非数据本来就不交叉）。 但是，从<strong>生成模型</strong>的角度看，这是巨大的成功：</p> <ol> <li> <strong>宏观守恒：</strong> 虽然 A 没去 A 的家，B 没去 B 的家。但是 A 去了 B 的家，B 去了 A 的家（或者类似的附近区域）。对于观察者来说，<strong>终点区域的人数总量是对的</strong>。</li> <li> <strong>物理平滑：</strong> 模型无法处理“精神分裂”的指令（同一个点既往左又往右），它通过“平滑化”创造了一个物理上可行的流场。</li> </ol> <h3 id="总结-3">总结</h3> <p>优化的 Loss 确实想让模型做“点对点”的映射，但是<strong>模型的输入（只有当前位置，没有身份ID）</strong>限制了它无法区分个体。</p> <p>这种<strong>“信息的瓶颈”</strong>（Information Bottleneck）迫使模型必须在交叉点做<strong>平均</strong>。</p> <ul> <li>正是这个“被迫的平均”，导致了原本直线的意图被扭曲成了曲线。</li> <li>正是这个“被迫的平均”，解决了交叉问题，实现了 Rewiring。</li> </ul> <p>所以，是<strong>有限的模型能力（单值函数）</strong>与<strong>复杂的训练数据（多值目标）</strong>之间的博弈，造就了 Rectified Flow 独特的整流效果。</p> <h1 id="因为训练的整个过程是一个在所有数据上的过程它站在了整体数据的视角来看的-并不是单独某一个单点数据">因为训练的整个过程,是一个在所有数据上的过程,它站在了整体数据的视角来看的, 并不是单独某一个单点数据,</h1> <p>太棒了，你这句话一针见血！<strong>“站在了整体数据的视角来看”</strong> 正是解开所有困惑的最终钥匙。</p> <p>你完全抓住了本质：</p> <ul> <li> <strong>微观训练</strong>（Batch 中的每一条数据）看似是在教模型“点对点”走路。</li> <li>但<strong>宏观结果</strong>（Loss 求和/求期望）却是在强迫模型构建一个<strong>全局最优的交通网络</strong>。</li> </ul> <h3 id="总结你的领悟">总结你的领悟：</h3> <ol> <li> <strong>全局视角 vs. 局部视角：</strong> 优化过程 $E[\dots]$ 是对<strong>所有</strong>数据的期望。模型不能只讨好某一个数据点（因为那样会得罪另一个数据点）。它必须找到一个让<strong>整体</strong>误差最小的方案。</li> <li> <strong>个体的牺牲为了集体的最优：</strong> 为了让整体的“交通系统”不堵塞（不交叉、单值化），模型“牺牲”了个体的原始意愿（比如让想去左上角的去了左下角）。</li> <li> <strong>涌现（Emergence）：</strong> “Rewiring”（重连）和“不交叉”并不是我们在代码里显式写出来的规则，而是当我们强迫一个单值函数去拟合全局数据流时，<strong>自然涌现</strong>出来的数学特性。</li> </ol> <p>这也正是这篇论文题目 <em>Rectified Flow</em>（整流流）中 <strong>“Rectified”（整流/矫正）</strong> 的深意： 它像一个整流器一样，把原本杂乱无章、四处乱窜的原始信号（数据），梳理成了整齐划一、井然有序的全局流场。</p> <p>你的理解非常深刻，已经完全透彻了这篇论文最核心的机制！</p> <h1 id="distillation主要讲的内容">Distillation主要讲的内容</h1> <p>论文中的 <strong>Distillation（蒸馏）</strong> 是在 Rectified Flow 训练完成之后的“最后一步优化”，目的是为了<strong>进一步提升推理速度</strong>。</p> <p>简单来说，它的核心思想是：<strong>既然现在的流已经很直了，我们能不能干脆连 ODE 都不走了，直接训练一个神经网络一步算出终点？</strong></p> <h3 id="1-为什么需要-distillation">1. 为什么需要 Distillation？</h3> <p>虽然通过 $k$-Rectified Flow（比如 2-Rectified Flow）我们已经得到了非常直的路径，理论上可以用 1 步 Euler 法生成很好的图像。但是：</p> <ul> <li>即便是最直的流，用 Euler 法（$z_1 = z_0 + v(z_0)$）仍然是一个近似。</li> <li>如果我们想要极致的速度（毫秒级响应），同时又要保证结果尽可能精确，我们可以用蒸馏来直接拟合那个从 $Z_0$ 到 $Z_1$ 的映射关系。</li> </ul> <h3 id="2-distillation-怎么做">2. Distillation 怎么做？</h3> <p>假设我们要对第 $k$ 次训练好的 Rectified Flow 模型 $Z_k$ 进行蒸馏：</p> <ol> <li> <strong>数据生成：</strong> 用训练好的 ODE 模型生成成对的数据 $(Z_0, Z_1)$。</li> <li> <strong>拟合映射：</strong> 训练一个新的神经网络 $\hat{T}(z)$，让它直接预测 $Z_1$。 <ul> <li>目标函数是：最小化 $|\hat{T}(Z_0) - Z_1|^2$。</li> <li>通常我们让 $\hat{T}(z_0) = z_0 + v_{new}(z_0)$，这本质上就是拟合 $t=0$ 时的速度场。 <alphaxiv-paper-citation title="Distillation Loss" page="8" first="loss function for" last="when t=0."></alphaxiv-paper-citation> </li> </ul> </li> </ol> <h3 id="3-distillation-和-rectification-reflow-的区别">3. Distillation 和 Rectification (Reflow) 的区别</h3> <p>这是最容易混淆的地方，论文特别强调了这一点：</p> <ul> <li> <strong>Rectification (Reflow):</strong> 改变了<strong>耦合关系（Coupling）</strong>。它会寻找新的、路径更直的配对方式。它的目的是<strong>优化路径质量</strong>。</li> <li> <strong>Distillation:</strong> <strong>不改变</strong>耦合关系。它只是尝试去<strong>逼近（Approximate）</strong>当前已经存在的那个耦合关系 $(Z_k^0, Z_k^1)$。它的目的是<strong>加速计算</strong>。</li> </ul> <p><strong>论文建议：</strong> 先做 Reflow 把路拉直，最后再做 Distillation 把这个直路“固化”下来。因为如果路还是弯的，Distillation 很难用简单的网络去拟合；但如果路已经是直的，Distillation 就非常容易且精准。 <alphaxiv-paper-citation title="Distillation Difference" page="8" first="distillation attempts to" last="coupling (Zk+1,0, Zk+1,1)"></alphaxiv-paper-citation></p> <h3 id="总结-4">总结</h3> <p>Distillation 是 Rectified Flow 流程中的<strong>终极加速器</strong>。</p> <ul> <li> <strong>Reflow</strong> 负责把复杂的曲线变成简单的直线。</li> <li> <strong>Distillation</strong> 负责把走这条直线的任务“硬编码”进一个前馈网络里，实现真正的实时生成。</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/DriveJEPA/">DriveJEPA</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/C_RADIOv4/">C_RADIOv4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/VLM4VLA/">VLM4VLA</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 P W Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>