<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RT1 | Tenacious life, proud journey. </title> <meta name="author" content="P W Name"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://beyondpzk.github.io/blog/2022/RT1/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Tenacious life, proud journey. </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">RT1</h1> <p class="post-meta"> Created on December 13, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/category/vla"> <i class="fa-solid fa-tag fa-sm"></i> VLA</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>[TOC]</p> <h1 id="rt1-robotics-transformer-for-real-world-control-at-scale">RT1: Robotics Transformer for Real-World Control at Scale</h1> <p><a href="https://arxiv.org/abs/2212.06817" rel="external nofollow noopener" target="_blank">paper link</a></p> <p>这篇论文由 Google Robotics（包括现在的 Google DeepMind）团队发表。</p> <p><strong>背景</strong> 在计算机视觉（CV）和自然语言处理（NLP）领域，我们已经见证了从“专用小模型”向“通用大模型”的范式转变。GPT 系列和 CLIP 等模型的成功告诉我们：<strong>开放式的任务无关训练（Open-ended task-agnostic training）配合高容量的架构（High-capacity architectures），能够通过吸收海量数据产生涌现能力。</strong></p> <p>然而，在机器人领域，这个故事讲得并不顺利。机器人数据的获取极其昂贵，物理世界的交互极其复杂。今天的核心问题是：<strong>我们能否在机器人领域复刻 NLP 的成功？能否训练一个单一的、通用的多任务骨干模型（Backbone），让它展现出对新任务、新环境和新物体的零样本（Zero-shot）泛化能力？</strong></p> <p>RT-1 就是对这一问题给出的有力回答。</p> <hr> <h1 id="第一部分引言与核心理念-introduction--motivation">第一部分：引言与核心理念 (Introduction &amp; Motivation)</h1> <h3 id="11-机器人学习的现状与挑战">1.1 机器人学习的现状与挑战</h3> <p>传统的机器人学习（无论是模仿学习还是强化学习）通常遵循“孤岛式”流程：</p> <ol> <li>定义一个特定任务（例如：抓取苹果）。</li> <li>收集该特定任务的数据。</li> <li>训练一个专用模型。</li> </ol> <p>这种方法导致模型无法在任务之间共享知识。虽然近期出现了一些多任务策略（如 Gato, BC-Z），但它们要么在真实世界任务的广度上受限，要么在未见过的任务上泛化性能不佳。</p> <h3 id="12-rt-1-的核心假设">1.2 RT-1 的核心假设</h3> <p>论文提出了一个核心论点：机器人通用模型的成功关键在于两点：</p> <ol> <li> <strong>大规模、多样化的数据集</strong>：能够覆盖广泛的现实世界场景。</li> <li> <strong>高容量的模型架构</strong>：能够“像海绵一样”吸收这些数据中的所有异构经验。</li> </ol> <h3 id="13-路线图">1.3 路线图</h3> <p>接下来，我们将从三个维度解构 RT-1：</p> <ul> <li> <strong>数据（The Data）</strong>：如何构建通过 13 台机器人历时 17 个月收集的大规模数据集。</li> <li> <strong>模型（The Model）</strong>：RT-1 的架构设计，特别是如何平衡高容量 Transformer 与实时控制（Real-time Control）的需求。</li> <li> <strong>实验（The Experiments）</strong>：如何科学地评估泛化能力，以及跨形态（Cross-embodiment）数据传输的实验结果。</li> </ul> <hr> <h1 id="第二部分rt-1-模型架构详解-model-architecture">第二部分：RT-1 模型架构详解 (Model Architecture)</h1> <h3 id="21-总体设计思路">2.1 总体设计思路</h3> <p>RT-1 是一个基于 Transformer 的模型，它接收<strong>图像序列</strong>和<strong>自然语言指令</strong>作为输入，直接输出<strong>离散化的动作 Token</strong>。</p> <ul> <li> <strong>输入</strong>：$T$ 个时刻的图像历史 + 文本指令。</li> <li> <strong>输出</strong>：当前时刻的动作 $a_t$。</li> <li> <strong>控制频率</strong>：3 Hz。这是一个硬性约束，意味着模型推理加上系统延迟必须控制在 100ms 以内。这对庞大的 Transformer 来说是一个巨大的挑战。</li> </ul> <h3 id="22-详细组件分析-参考论文-figure-3">2.2 详细组件分析 (参考论文 Figure 3)</h3> <p>我们将模型拆解为三个阶段：<strong>编码（Tokenizer）</strong> -&gt; <strong>压缩（TokenLearner）</strong> -&gt; <strong>序列建模（Transformer Backbone）</strong>。</p> <h4 id="a-图像与语言的联合-tokenization-film-efficientnet">A. 图像与语言的联合 Tokenization (FiLM-EfficientNet)</h4> <p>为了将高维图像和文本转化为 Transformer 可以处理的 Token，RT-1 并没有简单地将图像切片（Patchify，如 ViT 的做法），而是采用了更高效的卷积网络提取特征。</p> <ol> <li> <strong>文本编码</strong>：使用通用句子编码器（Universal Sentence Encoder, USE）将自然语言指令嵌入为向量。</li> <li> <strong>图像主干</strong>：使用在 ImageNet 上预训练的 <strong>EfficientNet-B3</strong>。 <ul> <li> <em>输入</em>：6 张 300x300 的历史图像。</li> <li> <em>输出</em>：9x9x512 的特征图（Feature Map）。</li> </ul> </li> <li> <strong>多模态融合（FiLM 层）</strong>：这是关键点。 <ul> <li>RT-1 使用 <strong>FiLM (Feature-wise Linear Modulation)</strong> 层将文本嵌入注入到 EfficientNet 中。</li> <li> <em>机制</em>：文本嵌入经过全连接层生成仿射变换参数（$\gamma, \beta$），对图像特征图进行逐通道的缩放和平移：$Feature_{new} = \gamma \cdot Feature_{old} + \beta$。</li> <li> <em>技巧</em>：为了不破坏 EfficientNet 的预训练权重，FiLM 层初始化为恒等变换（Identity-initialized），即初始化 $\gamma=1, \beta=0$。这使得视觉特征在训练初期得以保留，从而加速收敛。</li> <li>最终，EfficientNet 输出被展平为 <strong>81 个视觉 Token</strong>。</li> </ul> </li> </ol> <h4 id="b-token-压缩-tokenlearner">B. Token 压缩 (TokenLearner)</h4> <p>如果直接将 6 张图像 * 81 个 Token 输入 Transformer，序列长度将达到 486，这对实时性是致命的（Transformer 的计算复杂度随长度呈二次方增长）。</p> <p>RT-1 引入了 <strong>TokenLearner (Ryoo et al., 2021)</strong>：</p> <ul> <li> <strong>作用</strong>：它学习一组空间注意力掩码（Spatial Attention Masks），从 81 个原始 Token 中筛选并聚合出信息量最大的特征。</li> <li> <strong>结果</strong>：每张图像的 Token 数从 81 被压缩到了 <strong>8</strong> 个。</li> <li> <strong>优势</strong>：这种压缩使得推理速度提升了约 2.4 倍，是 RT-1 能在真机上跑通 3Hz 控制循环的关键。</li> </ul> <h4 id="c-transformer-主干-backbone">C. Transformer 主干 (Backbone)</h4> <ul> <li> <strong>结构</strong>：Decoder-only Transformer。</li> <li> <strong>参数</strong>：8 层 Self-attention，总参数量约 35M（整个模型总参数 76M）。</li> <li> <strong>输入序列</strong>：6 张历史图像 * 8 个 Token = 48 个 Token。加上位置编码。</li> </ul> <h4 id="d-动作输出-action-tokenization">D. 动作输出 (Action Tokenization)</h4> <p>RT-1 将连续的机器人控制问题转化为离散的分类问题：</p> <ul> <li> <strong>动作空间</strong>： <ul> <li>7 个手臂维度（x, y, z, roll, pitch, yaw, 夹爪开合）。</li> <li>3 个底座维度（x, y, yaw）。</li> <li>1 个模式维度（控制手臂 / 控制底座 / 终止任务）。</li> </ul> </li> <li> <strong>离散化</strong>：每个维度被离散化为 <strong>256 个 Bin</strong>（区间）。</li> <li> <strong>损失函数</strong>：标准的分类交叉熵损失（Categorical Cross-entropy）。</li> </ul> <p>这种“图像+语言 -&gt; 离散动作”的设计，本质上是将机器人控制看作是一个序列建模问题（Sequence Modeling）。这与 GPT 预测下一个单词在数学形式上是完全一致的。</p> <hr> <h1 id="第三部分大规模数据构建-data-at-scale">第三部分：大规模数据构建 (Data at Scale)</h1> <p>在深度学习时代，数据就是护城河。RT-1 的强大很大程度上归功于其背后的数据工程。</p> <h3 id="31-数据集规模">3.1 数据集规模</h3> <ul> <li> <strong>总演示数</strong>：约 130,000 条（Episodes）。</li> <li> <strong>采集方式</strong>：人类通过 VR 设别进行远程遥操作（Teleoperation）。这是高质量的专家数据。</li> <li> <strong>采集周期</strong>：17 个月。</li> <li> <strong>硬件规模</strong>：13 台 Everyday Robots (EDR) 移动操作机器人。</li> </ul> <h3 id="32-任务与指令的多样性">3.2 任务与指令的多样性</h3> <p>RT-1 定义了 <strong>700 多条</strong> 独特的指令。这些指令由动词（Skill）和名词（Object）组合而成。</p> <ul> <li> <strong>技能（Skills）</strong>：抓取（Pick）、放置（Place）、打开抽屉（Open drawer）、扶正物体（Place upright）、推倒物体（Knock over）等。</li> <li> <strong>物体</strong>：涵盖了各种各样的厨房用品、零食、文具。</li> <li> <strong>环境</strong>：主要是在办公楼的厨房场景中采集。分为“训练厨房”和“真实厨房”。</li> </ul> <h3 id="33-数据分布的重要性">3.3 数据分布的重要性</h3> <p>论文中特别提到，为了让“抓取（Pick）”这个技能具备泛化性，他们特意引入了大量多样化的物体进行抓取训练。而对于其他技能，物体集合相对固定。这种非均匀的数据分布是机器人学习中的常态。</p> <hr> <h1 id="第四部分实验结果与分析-experiments">第四部分：实验结果与分析 (Experiments)</h1> <p>我们要如何科学地评估一个通用机器人模型？仅仅看训练集上的成功率是毫无意义的。RT-1 的评估体系涵盖了四个层级：</p> <h3 id="41-评估基准与设置">4.1 评估基准与设置</h3> <ul> <li> <strong>基准模型（Baselines）</strong>： <ul> <li> <strong>Gato</strong>：DeepMind 的多模态通用模型。</li> <li> <strong>BC-Z</strong>：之前的 SOTA 模仿学习方法，使用 ResNet + RNN。</li> </ul> </li> <li> <strong>评估指标</strong>：任务成功率（Success Rate）。</li> </ul> <h3 id="42-核心实验结果">4.2 核心实验结果</h3> <p>我们关注三个维度的泛化能力：</p> <ol> <li> <strong>已知任务（Seen Tasks）</strong>： <ul> <li>在训练过的任务上，RT-1 达到了 <strong>97%</strong> 的惊人成功率。</li> <li>相比之下，Gato 只有 51%，BC-Z 只有 72%。这说明高容量架构在拟合大规模数据方面具有绝对优势。</li> </ul> </li> <li> <strong>未见任务（Unseen Tasks）</strong>： <ul> <li>这是真正的考验。任务指令从未在训练集中出现，但其组成的技能和物体是见过的。</li> <li>RT-1 成功率 <strong>76%</strong>。这表明模型学会了组合性（Compositionality），比如它学过“抓苹果”和“放盘子里”，就能执行“抓苹果放盘子里”。</li> </ul> </li> <li> <strong>抗干扰与背景鲁棒性（Distractors &amp; Backgrounds）</strong>： <ul> <li> <strong>L1（新环境）</strong>：换个光照、换个桌子，RT-1 依然稳健。</li> <li> <strong>L2（干扰物）</strong>：桌上放满没见过的杂物。</li> <li> <strong>L3（新场景）</strong>：完全不同的真实厨房。</li> <li>在最具挑战性的 L3 场景中，RT-1 的表现明显优于基准模型，泛化性能提升极大。</li> </ul> </li> </ol> <blockquote> <p><strong>关键图表解读</strong>：论文中的 <strong>Figure 4</strong>。随着数据量的增加（从 10% 到 100%），RT-1 的性能持续上升，没有饱和迹象。而 ResNet 架构的 BC-Z 在数据量达到一定程度后性能趋于平缓。这完美验证了“苦涩的教训（The Bitter Lesson）”——<strong>计算量和模型容量必须与数据量相匹配</strong>。</p> </blockquote> <h3 id="43-异构数据融合heterogeneous-data">4.3 异构数据融合（Heterogeneous Data）</h3> <p>这是 RT-1 最令人兴奋的发现之一。</p> <ul> <li> <strong>仿真数据（Sim-to-Real）</strong>： <ul> <li>如果我们在仿真器中生成大量数据（比如抓取一些现实中没有的物体），混合到真实数据中训练，会发生什么？</li> <li>结果：RT-1 在<strong>未见物体</strong>上的抓取成功率从 78% 提升到了 <strong>87%</strong>。这证明了仿真数据可以作为真实数据的有效补充，提升视觉泛化性。</li> </ul> </li> <li> <strong>跨机器人形态（Cross-embodiment）</strong>： <ul> <li>Google 还有一种叫 Kuka 的工业机械臂，主要做分拣（Bin-picking）。Kuka 的外观、视角、动作空间与 EDR 移动机器人完全不同。</li> <li>实验：将 Kuka 的数据混入 RT-1 训练。</li> <li>结果：EDR 机器人在执行类似 Kuka 的分拣任务时，成功率翻倍（22% -&gt; <strong>39%</strong>）。</li> <li> <strong>结论</strong>：RT-1 展示了惊人的数据吸收能力，它似乎能够忽略机器人形态的差异，提取出通用的“物体交互物理规律”。</li> </ul> </li> </ul> <h3 id="44-长程任务saycan-集成">4.4 长程任务：SayCan 集成</h3> <p>RT-1 并非只能做短时任务。将其作为底层执行器嵌入到 <strong>SayCan</strong> 框架中（上层由 LLM 进行任务规划），RT-1 能够连续执行多达 50 个步骤的长程任务。</p> <ul> <li>在真实厨房 1 中，执行成功率达到 67%。</li> <li>在完全未见过的厨房 2 中，由于 RT-1 强大的底层泛化能力，整个系统依然能保持运转。</li> </ul> <hr> <h1 id="第五部分深度研讨与批判性思维-discussion--critique">第五部分：深度研讨与批判性思维 (Discussion &amp; Critique)</h1> <h3 id="51-rt-1-的局限性">5.1 RT-1 的局限性</h3> <ol> <li> <strong>数据依赖</strong>：尽管 RT-1 泛化性好，但它依然依赖于极其昂贵的 130k 条专家演示数据。这对于普通实验室是不可复制的。</li> <li> <strong>动作空间</strong>：目前的离散化动作空间虽然有效，但在处理需要极高精度的精细操作（如穿针引线）时可能会受限。</li> <li> <strong>开环 vs 闭环</strong>：虽然是 3Hz 控制，但本质上它主要是基于视觉反馈的反应式策略，缺乏显式的记忆模块（虽然输入了历史图像，但 Transformer 的上下文窗口有限）。</li> </ol> <h3 id="52-启示与未来方向">5.2 启示与未来方向</h3> <ul> <li> <strong>RT-2 及之后</strong>：RT-1 之后出现了 RT-2（VLAD），直接将 VLM（视觉语言大模型）微调用于输出动作，进一步利用了互联网级别的语义知识。RT-1 是这个方向的基石。</li> <li> <strong>数据飞轮</strong>：RT-1 证明了“数据规模 + Transformer”在机器人领域是行得通的。未来的重点将是如何更廉价地获取数据（如视频生成数据、Sim2Real、被动视频观察）。</li> </ul> <hr> <h1 id="总结-conclusion">总结 (Conclusion)</h1> <p>RT-1 是机器人学习领域的一个转折点。它不仅是一个模型，更是一个关于<strong>Scaling Law（缩放定律）</strong>在实体人工智能中是否适用的验证实验。</p> <p>它告诉我们：</p> <ol> <li> <strong>Transformer 可以实时控制机器人。</strong>（通过 TokenLearner 等架构创新）</li> <li><strong>大规模真实世界数据是不可替代的。</strong></li> <li><strong>异构数据（仿真、不同机器人）是可以被统一模型吸收利用的。</strong></li> </ol> <h2 id="动作的预测">动作的预测</h2> <ol> <li> <strong>维度数量</strong>：实际是 <strong>11 个维度</strong>，不是 10 个。</li> <li> <strong>输出机制</strong>：模型确实是<strong>一次性并行输出</strong>这 11 个维度的分布（即 Logits），形状确实是 <code class="language-plaintext highlighter-rouge">(11, 256)</code>。</li> </ol> <p>我们来详细拆解一下 RT-1 的动作输出头（Action Head）。</p> <h3 id="1-为什么是-11-个维度">1. 为什么是 11 个维度？</h3> <p>虽然我们常说 7 自由度机械臂 + 3 自由度底座 = 10 个维度，但 RT-1 增加了一个非常重要的“控制位”。具体的 11 个维度如下：</p> <ul> <li> <strong>机械臂（Arm）- 7 维</strong>： <ul> <li>末端执行器位置：$x, y, z$</li> <li>末端执行器姿态：$roll, pitch, yaw$</li> <li>夹爪状态：$gripper_opening$</li> </ul> </li> <li> <strong>移动底座（Base）- 3 维</strong>： <ul> <li>移动指令：$x, y, yaw$（底座的前进、横移和旋转）</li> </ul> </li> <li> <strong>模式切换（Mode）- 1 维</strong>： <ul> <li>这是一个离散的指令，用于告诉机器人当前应该主要控制什么，或者是否结束任务。</li> <li>例如：<code class="language-plaintext highlighter-rouge">0</code>=控制手臂，<code class="language-plaintext highlighter-rouge">1</code>=控制底座，<code class="language-plaintext highlighter-rouge">2</code>=任务结束（Terminate Episode）。</li> </ul> </li> </ul> <p>所以，总共是 <strong>7 + 3 + 1 = 11 个变量</strong>。</p> <h3 id="2-输出形状与并行生成关键点">2. 输出形状与并行生成（关键点！）</h3> <p>每个维度都被离散化成了 <strong>256 个 Bin</strong>（区间）。</p> <p>在 Transformer 的最后一层输出之后，RT-1 并不是像 GPT 生成文本那样“一个字接一个字”地生成动作（即 $x \to y \to z \dots$），而是<strong>并行生成</strong>。</p> <ul> <li> <strong>输入给 Action Head 的</strong>：是 Transformer 输出序列中的最后一个 Token 的嵌入向量（Embedding），假设维度是 $H$（Hidden Size）。</li> <li> <strong>Action Head 的结构</strong>：通常是一个简单的全连接层（Linear Layer），将 $H$ 映射到 $11 \times 256$。</li> <li> <strong>输出张量形状</strong>： \(\text{Logits} \in \mathbb{R}^{11 \times 256}\)</li> </ul> <p><strong>这意味着：</strong> 模型在同一个时间步 $t$，<strong>同时</strong>预测手臂的 $x$ 坐标、底座的 $y$ 速度、夹爪的开合等等。它认为在当前图像和指令的条件下，这 11 个维度的动作是<strong>条件独立</strong>的。</p> <h3 id="3-为什么不使用自回归auto-regressive">3. 为什么不使用自回归（Auto-regressive）？</h3> <p>在早期的一些机器人模型（甚至 Gato 的某些设置）中，确实尝试过把动作展平，按顺序生成： \(P(a_x, a_y, \dots) = P(a_x) \cdot P(a_y | a_x) \cdot P(a_z | a_x, a_y) \dots\)</p> <p>但在 RT-1 的论文中，作者做了一个非常重要的消融实验（Ablation Study）：</p> <ul> <li> <strong>实验</strong>：对比“并行输出”与“自回归输出”。</li> <li> <strong>发现</strong>：自回归输出虽然理论上能捕捉动作维度间的相关性（比如 $x$ 变了 $y$ 也要变），但在实际测试中，<strong>并没有带来明显的性能提升，反而让推理速度慢了 2 倍以上</strong>。</li> <li> <strong>决策</strong>：为了保证 <strong>3 Hz</strong> 的实时控制频率，RT-1 最终选择了<strong>非自回归（并行）</strong>的方式。</li> </ul> <h3 id="总结">总结</h3> <p>RT-1 的动作输出流程是这样的：</p> <ol> <li> <strong>Transformer 输出</strong>：拿到最后一个时刻的特征向量。(最后一个token的Embedding)</li> <li> <strong>映射</strong>：通过线性层得到形状为 <code class="language-plaintext highlighter-rouge">(11, 256)</code> 的 Logits。</li> <li> <strong>采样/Argmax</strong>：对这 11 个维度分别做 <code class="language-plaintext highlighter-rouge">argmax</code> 或者按概率采样，得到 11 个整数索引（范围 0-255）。</li> <li> <strong>去离散化（De-tokenization）</strong>：将这 11 个整数映射回连续的物理数值（比如 [-1.0, 1.0] 之间），发送给机器人控制器。</li> </ol> <p>这个 <code class="language-plaintext highlighter-rouge">(11, 256)</code> 的张量就是 RT-1 在每一个时间步的最终产出。</p> <h2 id="rt1训完后不具备vlm的能力">RT1训完后不具备VLM的能力</h2> <p>RT-1 是 VLA（Vision-Language-Action）模型的雏形，且它在训练完成后，确实完全不具备我们现在所说的通用 VLM（如 GPT-4V 或 Gemini）的图文对话或推理能力。</p> <p>我们从架构设计的底层逻辑，将 RT-1 与后来的 RT-2（真正的 VLA）做一个本质上的区分。</p> <h3 id="1-为什么说它是-vla-的雏形">1. 为什么说它是 VLA 的雏形？</h3> <p><strong>VLA 的定义</strong>：接受视觉（Vision）和语言（Language）输入，输出动作（Action）。 从这个定义上看，RT-1 完美符合。它确实是迈向“通才机器人”的第一步，试图用一个统一的模型（Transformer）来解决多模态输入到控制信号的映射问题。</p> <h3 id="2-为什么它不具备-vlm-能力核心原因">2. 为什么它不具备 VLM 能力？（核心原因）</h3> <p>RT-1 从架构设计之初，就没有被赋予生成语言的能力。 并不是“遗忘”了，而是“从未拥有”。</p> <p>RT-1 的几个关键设计：</p> <h4 id="a-语言编码器是死的-frozen--discriminative">A. 语言编码器是“死的” (Frozen &amp; Discriminative)</h4> <ul> <li> <strong>RT-1 的做法</strong>：它使用的是 <strong>Universal Sentence Encoder (USE)</strong>。</li> <li> <strong>本质</strong>：USE 是一个<strong>判别式</strong>的嵌入模型（Embedding Model），它的作用是把一句话（比如“拿起苹果”）变成一个固定的向量。</li> <li> <strong>局限</strong>：它只能“读懂”语义并将其压缩成向量，无法“生成”下一个词。它不是 GPT 那样的生成式模型（Generative Model）。</li> </ul> <h4 id="b-输出头被锁死在动作空间-hard-coded-action-heads">B. 输出头被“锁死”在动作空间 (Hard-coded Action Heads)</h4> <ul> <li> <strong>RT-1 的做法</strong>：RT-1 的输出层是专门设计的 <strong>Action Head</strong>，输出的是 <code class="language-plaintext highlighter-rouge">(11, 256)</code> 的动作分类分布。</li> <li> <strong>VLM 的做法</strong>：真正的 VLM（如 RT-2 使用的 PaLI-X 或 PaLM-E），其输出层是 <strong>Text Head</strong>（词表，Vocabulary Size 通常是几万到几十万）。</li> <li> <strong>结果</strong>：RT-1 的物理结构决定了它只能输出“底座向前 0.5m”，而绝对无法输出“这是一个苹果”这样的文本。它的输出空间里根本没有“单词”这个概念。</li> </ul> <h4 id="c-transformer-主干的训练目标">C. Transformer 主干的训练目标</h4> <ul> <li> <strong>RT-1</strong>：Transformer 的训练目标是最小化动作的分类误差（Behavior Cloning）。</li> <li> <strong>VLM</strong>：训练目标是 Next Token Prediction（预测下一个文本 Token）。</li> </ul> <h3 id="3-rt-1-vs-rt-2从专用架构到通用大模型的飞跃">3. RT-1 vs. RT-2：从“专用架构”到“通用大模型”的飞跃</h3> <table> <thead> <tr> <th style="text-align: left">特性</th> <th style="text-align: left">RT-1 (本论文)</th> <th style="text-align: left">RT-2 (Vision-Language-Action-Model)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>基础架构</strong></td> <td style="text-align: left"> <strong>专用设计</strong>：EfficientNet + TokenLearner + Transformer</td> <td style="text-align: left"> <strong>通用 VLM</strong>：直接使用 PaLI-X 或 PaLM-E</td> </tr> <tr> <td style="text-align: left"><strong>语言能力</strong></td> <td style="text-align: left"> <strong>仅理解</strong>：通过 USE 嵌入向量</td> <td style="text-align: left"> <strong>理解+生成</strong>：具备完整的 LLM 能力</td> </tr> <tr> <td style="text-align: left"><strong>动作输出</strong></td> <td style="text-align: left"> <strong>专用 Head</strong>：输出离散动作 Bin</td> <td style="text-align: left"> <strong>Token 化</strong>：将动作映射为文本 Token（如把动作”128”视为文本”128”）</td> </tr> <tr> <td style="text-align: left"><strong>训练后能力</strong></td> <td style="text-align: left"> <strong>仅控制</strong>：只会做机器人动作，无法对话</td> <td style="text-align: left"> <strong>多模态</strong>：既能做动作，又能回答“图中有什么？”</td> </tr> <tr> <td style="text-align: left"><strong>知识迁移</strong></td> <td style="text-align: left"> <strong>有限</strong>：仅限于ImageNet的视觉特征</td> <td style="text-align: left"> <strong>无限</strong>：继承了互联网级别的语义知识和推理能力</td> </tr> </tbody> </table> <h3 id="4-总结">4. 总结</h3> <ul> <li> <strong>RT-1 是“专才”</strong>：它借用了 Transformer 的架构优势（长序列建模、注意力机制）来处理机器人数据，但它本质上还是一个<strong>模仿学习策略网络（Policy Network）</strong>。它的“脑子”（Transformer）是专门为了输出动作而从头训练（或微调）的，不具备通用的语言生成逻辑。</li> <li> <strong>RT-2 才是真正的“通才” VLA</strong>：RT-2 才是你想象中那种“既能写诗，又能通过推理决定去拿苹果”的模型。RT-2 证明了我们可以把“动作”伪装成“语言”，强行塞进一个已经训练好的 VLM 里，从而继承 VLM 的强大能力。</li> </ul> <p>所以，RT-1 是 <strong>Robotics Transformer</strong>（用于机器人的 Transformer），而 RT-2 才是 <strong>Vision-Language-Action Model</strong>（具备视觉语言能力的动作模型）。RT-1 是从 0 到 1 的探索，证明了 Transformer 可以控制机器人；RT-2 则是从 1 到 100，证明了机器人模型可以继承互联网大模型的智慧。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/DriveJEPA/">DriveJEPA</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/C_RADIOv4/">C_RADIOv4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/VLM4VLA/">VLM4VLA</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 P W Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>