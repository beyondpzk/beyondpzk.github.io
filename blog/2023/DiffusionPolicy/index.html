<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> DiffusionPolicy | Tenacious life, proud journey. </title> <meta name="author" content="P W Name"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://beyondpzk.github.io/blog/2023/DiffusionPolicy/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Tenacious life, proud journey. </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">DiffusionPolicy</h1> <p class="post-meta"> Created on March 07, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/vla"> <i class="fa-solid fa-tag fa-sm"></i> VLA</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>[TOC]</p> <h1 id="diffusion-policy-visuomotor-policy-learning-via-action-diffusion">Diffusion Policy: Visuomotor Policy Learning via Action Diffusion</h1> <p><a href="https://arxiv.org/abs/2303.04137" rel="external nofollow noopener" target="_blank">paper link</a></p> <p>今天要深入探讨的是一篇在机器人学习（Robot Learning）领域具有里程碑意义的论文：<strong>《Diffusion Policy: Visuomotor Policy Learning via Action Diffusion》</strong>。这篇论文由哥伦比亚大学、MIT和丰田研究院（TRI）的研究人员共同完成。</p> <p>在过去的一两年里，生成式模型（Generative Models）特别是扩散模型（Diffusion Models）彻底改变了图像生成领域（如DALL-E 2, Stable Diffusion）。而这篇论文，则是将这股浪潮成功引入机器人操作（Manipulation）领域的代表作。它不仅在15个基准任务上取得了平均46.9%的性能提升，更重要的是，它提出了一种全新的视角来看待“策略（Policy）”的表示问题。</p> <p>我将分为四个核心模块：</p> <ol> <li> <strong>背景与动机</strong>：为什么传统的行为克隆（Behavior Cloning）方法不够好？</li> <li> <strong>核心理论</strong>：Diffusion Policy的数学形式化与推断机制。</li> <li> <strong>关键技术实现</strong>：从网络架构到预测视界（Receding Horizon）的设计。</li> <li> <strong>实验分析与讨论</strong>：从仿真到真机实验的洞察。</li> </ol> <hr> <h1 id="第一部分背景与动机多模态分布的挑战">第一部分：背景与动机——多模态分布的挑战</h1> <h2 id="11-什么是视觉运动策略visuomotor-policy">1.1 什么是视觉运动策略（Visuomotor Policy）？</h2> <p>在机器人学习中，特别是模仿学习（Imitation Learning）的范畴下，我们的目标是学习一个函数（策略）$\pi$，它接收当前的视觉观测 $O_t$（Observations），并输出动作 $A_t$（Actions）。 \(A_t = \pi(O_t)\) 这看起来像是一个标准的监督回归（Supervised Regression）问题。然而，机器人操作数据的独特性使得这不是一个简单的回归任务。</p> <h2 id="12-核心挑战多模态分布multimodal-distributions">1.2 核心挑战：多模态分布（Multimodal Distributions）</h2> <p>想象一个机器人需要绕过桌子上的一个障碍物去抓取物体。它既可以从左边绕过去，也可以从右边绕过去。这两种轨迹在演示数据（Demonstrations）中都存在。</p> <ul> <li>如果我们使用标准的均方误差（MSE）回归（即显式策略 Explicit Policy），网络倾向于输出这两种模式的<strong>平均值</strong>。</li> <li>“左”和“右”的平均值是什么？是直接撞向中间的障碍物。</li> </ul> <p>这就是<strong>多模态分布</strong>问题。</p> <h2 id="13-现有方法的局限性">1.3 现有方法的局限性</h2> <p>为了解决这个问题，学术界之前尝试了多种方案：</p> <ol> <li> <strong>混合高斯模型（GMMs, e.g., LSTM-GMM）</strong>： <ul> <li> <em>原理</em>：预测多个高斯分布的加权和。</li> <li> <em>局限</em>：训练不稳定，对参数敏感，且高斯核的数量限制了表达能力的上限。在许多高精度任务中，它难以拟合尖峰分布。</li> </ul> </li> <li> <strong>分类/离散化（Categorical）</strong>： <ul> <li> <em>原理</em>：将连续动作空间划分为网格（Bins）。</li> <li> <em>局限</em>：维数灾难。对于高维动作空间（如7自由度机械臂），网格数量呈指数级增长，导致计算不可行或精度极其低下。</li> </ul> </li> <li> <strong>隐式策略（Implicit Policies, e.g., IBC - Implicit Behavior Cloning）</strong>： <ul> <li> <em>原理</em>：学习一个能量函数 $E(a, o)$，通过优化 $\text{argmin}_a E(a, o)$ 来找动作。</li> <li> <em>局限</em>：训练极其困难。通常需要负采样（Negative Sampling）来估计配分函数（Partition Function），这导致了著名的训练不稳定性。论文中提到，IBC在许多复杂任务上的表现并不理想（如Lift, Can等任务）。</li> </ul> </li> </ol> <p><strong>Diffusion Policy 的出现，正是为了解决上述所有痛点：它既能完美表达多模态分布，又能保持训练的极度稳定性，同时支持高维动作空间。</strong> <alphaxiv-paper-citation title="Introduction" page="1" first="This formulation allows" last="significantly improving performance."></alphaxiv-paper-citation></p> <hr> <h1 id="第二部分diffusion-policy-理论形式化">第二部分：Diffusion Policy 理论形式化</h1> <h2 id="21-从ddpm到机器人策略">2.1 从DDPM到机器人策略</h2> <p>去噪扩散概率模型（DDPM）通常我们用它来生成图像，即从高斯噪声中恢复出图像。</p> <p>在这篇论文中，作者做了一个巧妙的转换：<strong>将“动作序列”视为一张“图像”来进行生成。</strong></p> <p>Diffusion Policy 将策略建模为一个条件去噪过程。给定观测 $O_t$，我们通过 $K$ 次迭代去噪，生成动作 $A_t$。</p> <h3 id="22-数学表达">2.2 数学表达</h3> <p>标准的DDPM逆向过程（生成过程）如下： \(x^{k-1} = \alpha (x^k - \gamma \epsilon_\theta(x^k, k) + \mathcal{N}(0, \sigma^2 I))\) 其中 $x^k$ 是第 $k$ 步的带噪样本，$\epsilon_\theta$ 是噪声预测网络。</p> <p><strong>对于 Diffusion Policy，我们需要做两点关键修改：</strong></p> <ol> <li> <strong>输出对象</strong>：$x$ 变成了机器人的动作序列 $A_t$。</li> <li> <strong>条件生成</strong>：去噪过程必须以当前的观测 $O_t$ 为条件。</li> </ol> <p>修改后的公式为（论文 Eq 4）： \(A_t^{k-1} = \alpha (A_t^k - \gamma \epsilon_\theta(O_t, A_t^k, k) + \mathcal{N}(0, \sigma^2 I))\)</p> <table> <tbody> <tr> <td>这里非常关键的一点是：**$\epsilon_\theta(O_t, A_t^k, k)$ 实际上是在预测分数函数的梯度 $\nabla \log p(A_t</td> <td>O_t)$。** (根据Score Matching的理论.)</td> </tr> </tbody> </table> <h2 id="23-为什么这比-ebm能量模型-更好">2.3 为什么这比 EBM（能量模型） 更好？</h2> <p>隐式策略（Implicit Policy/EBM）试图直接学习能量函数 $E(x)$。这需要计算配分函数（积分），非常难。 Diffusion Policy 学习的是能量函数的<strong>梯度</strong> $\nabla E(x)$。</p> <ul> <li>学习梯度不需要计算归一化常数（因为常数的导数为0）。</li> <li>这就是为什么 Diffusion Policy 的训练比 IBC 稳定得多的根本数学原因。</li> </ul> <hr> <h1 id="第三部分关键技术实现与工程设计">第三部分：关键技术实现与工程设计</h1> <p>理论很美，但要让它在物理机器人上工作，需要一系列精妙的工程设计。这也是这篇论文不仅是“Idea paper”更是“System paper”的原因。</p> <h2 id="31-闭环动作序列预测closed-loop-action-sequences">3.1 闭环动作序列预测（Closed-loop Action Sequences）</h2> <p>这是一个极其重要的设计细节。</p> <p>传统的策略通常是 $O_t \to a_t$（单步预测）。但 Diffusion Policy 预测的是一个<strong>动作序列</strong> $A_t$。(Trunk)</p> <ul> <li> <strong>输入</strong>：过去 $T_o$ 步的观测 $O_t$。</li> <li> <strong>输出</strong>：未来 $T_p$ 步的动作序列。</li> </ul> <p><strong>为什么要预测序列？</strong></p> <ol> <li> <strong>时间一致性（Temporal Consistency）</strong>：单步策略容易出现抖动，预测序列能保证动作的连贯和平滑。</li> <li> <strong>避免短视（Avoiding Myopic Planning）</strong>：模型被迫考虑未来的轨迹，这就隐式地包含了规划（Planning）的能力。</li> </ol> <p><strong>但是，我们如何执行这个序列？</strong> 这就引入了<strong>后退视界控制（Receding Horizon Control, RHC）</strong>的概念。 假设我们在时刻 $t$ 预测了未来16步的动作。我们<strong>不会</strong>把这16步全部执行完再重新预测。相反，我们只执行前 $T_a$ 步（比如前8步），然后立刻在时刻 $t+T_a$ 重新进行预测。 这种机制既保证了动作的长程平滑性，又赋予了机器人对环境干扰的快速响应能力。 <alphaxiv-paper-citation title="Action Sequences" page="2" first="This design allows" last="and responsiveness."></alphaxiv-paper-citation></p> <h2 id="32-视觉条件注入visual-conditioning">3.2 视觉条件注入（Visual Conditioning）</h2> <p>网络 $\epsilon_\theta$ 需要同时处理高维的图像输入和低维的动作输入。如何融合？</p> <p>论文提出了两种架构变体：</p> <ol> <li> <strong>CNN-based (1D Temporal CNN)</strong>: <ul> <li>基于 Janner et al. 的架构。</li> <li> <strong>融合方式</strong>：FiLM (Feature-wise Linear Modulation)。简单来说，就是用图像特征去仿射变换（缩放和平移）动作处理网络的特征图。</li> <li> <em>特点</em>：擅长低频、平滑的控制任务。</li> </ul> </li> <li> <strong>Transformer-based (Time-series Diffusion Transformer)</strong>: <ul> <li>基于 MinGPT。</li> <li> <strong>融合方式</strong>：Cross-Attention。将动作嵌入作为 Query，图像嵌入作为 Key 和 Value。</li> <li> <em>特点</em>：对于动作变化剧烈、高频震荡的任务（如 Push-T 任务中的快速调整），Transformer 表现更好，因为它减少了 CNN 带来的过度平滑（Over-smoothing）效应。 <alphaxiv-paper-citation title="Architecture" page="2" first="We propose a" last="velocity control."></alphaxiv-paper-citation> </li> </ul> </li> </ol> <hr> <h1 id="第四部分实验分析与讨论">第四部分：实验分析与讨论</h1> <h2 id="41-仿真实验结果">4.1 仿真实验结果</h2> <p>论文在4个不同的基准测试（RoboMimic, Kitchen, etc.）共15个任务上进行了评估。 结果是压倒性的：<strong>平均性能提升 46.9%</strong>。</p> <ul> <li>在复杂的 <strong>Transport</strong> 任务中，LSTM-GMM 的成功率是 62%，IBC 是 0%，而 Diffusion Policy 达到了 94% 以上。</li> <li>注意 <strong>IBC 的崩溃</strong>。在很多任务中 IBC 成功率为 0。这验证了我们之前的理论分析：EBM 极难训练，经常陷入局部极小值或模式坍塌。</li> </ul> <h2 id="42-真机实验real-world-experiments">4.2 真机实验（Real-world Experiments）</h2> <p>这部分的实验设计非常精彩，展示了 Diffusion Policy 的鲁棒性。</p> <ol> <li> <strong>Push-T 任务</strong>：推一个T型的木块。这是一个典型的多模态任务（可以推T的左边、右边或顶端）。Diffusion Policy 展现了极其精确的接触控制。</li> <li> <strong>Mug Flip（翻转马克杯）</strong>：这是一个6自由度的高难度任务。机器人需要拿起任意摆放的杯子，把它翻转并挂在架子上。这涉及复杂的重抓取（Regrasp）策略。 <ul> <li> <em>观察</em>：LSTM-GMM 完全失败。Diffusion Policy 能够处理多种抓取姿势（正手、反手）。 <alphaxiv-paper-citation title="Mug Flip" page="10" first="Although never demonstrated" last="when necessary."></alphaxiv-paper-citation> </li> </ul> </li> <li> <strong>Sauce Pouring（倒酱汁）</strong>：处理流体和非刚性物体。</li> <li> <strong>双臂协调（Bimanual Tasks）</strong>：如叠衣服（Shirt Folding）。</li> </ol> <h2 id="43-关键消融实验ablation-studies">4.3 关键消融实验（Ablation Studies）</h2> <p>我们需要关注几个工程参数的影响：</p> <ul> <li> <strong>预测视界 ($T_p$)</strong>：预测太短，动作不平滑；预测太长，计算量大且容易累积误差。</li> <li> <strong>执行视界 ($T_a$)</strong>：这是 Receding Horizon 的核心。$T_a &lt; T_p$ 至关重要。如果 $T_a = T_p$（开环执行），由于误差累积，成功率会大幅下降。这就证明了<strong>闭环控制</strong>的必要性。</li> </ul> <hr> <h1 id="第五部分总结与思考">第五部分：总结与思考</h1> <p>为什么 <strong>Diffusion Policy</strong> 会成为当前的主流范式。</p> <ol> <li> <strong>数学上的优雅与稳定</strong>：通过学习 Score Function 的梯度，避开了 EBM 训练中的配分函数难题，实现了稳定的训练。</li> <li> <strong>对多模态的天然适应</strong>：扩散过程本质上是从分布中采样，这使得它能够自然地处理多解问题，而不需要像 GMM 那样预设模式数量，也不像回归那样取平均。</li> <li> <strong>序列预测与时空一致性</strong>：将动作视为序列（图片），利用了扩散模型在图像生成中被验证过的强大的结构化生成能力。</li> </ol> <p><strong>遗留问题与未来方向</strong>：</p> <ul> <li> <strong>推断速度</strong>：扩散模型需要多次迭代（如 100 步或 16 步），这导致推理速度较慢（尽管论文中优化到了 10-20Hz）。如何进一步加速（如 Consistency Models, Distillation）是当前的研究热点。</li> <li> <strong>数据依赖</strong>：虽然比 IBC 好，但本质上还是 BC，对数据质量有要求。如何结合强化学习（RL）进行微调？</li> </ul> <p><strong>思考</strong>： 如果我们将 Diffusion Policy 用于导航任务（Navigation），观测空间和动作空间会有什么变化？Receding Horizon 的策略是否需要调整？</p> <h1 id="后面与rl的结合">后面与RL的结合</h1> <p>在 Diffusion Policy 提出（2023年）之后，学术界立刻意识到了它的一个核心局限：<strong>它本质上还是行为克隆（BC）</strong>。也就是说，它只能“模仿”示教者。如果示教者做得不够好，或者环境发生了未见过的变化，它无法像强化学习（RL）那样去“探索”出更优的解。</p> <p>因此，<strong>Diffusion + RL</strong> 成为了过去两年（2023-2025）最火热的研究方向之一。主要结合方式可以归纳为以下三类流派：</p> <h3 id="1-离线强化学习diffusion-作为演员policy-head">1. 离线强化学习：Diffusion 作为“演员”（Policy Head）</h3> <p>这是最早期的结合方式。传统的 RL（如 SAC, PPO）通常假设策略是高斯分布（单模态）。但正如我们课上讲的，机器人动作往往是多模态的。</p> <ul> <li> <strong>核心思想</strong>：我们用 Diffusion Model 来替代传统 RL 中的高斯网络（Gaussian Policy）作为 Actor，用来拟合复杂的动作分布。</li> <li> <strong>代表论文</strong>： <ul> <li> <strong>IDQL (Implicit Diffusion Q-Learning)</strong> <a href="https://arxiv.org/abs/2304.10573" rel="external nofollow noopener" target="_blank">arXiv:2304.10573</a>：这篇论文非常经典。它结合了 IQL（Implicit Q-Learning）和 Diffusion。Diffusion Model 负责从离线数据中生成“候选动作”（拟合数据分布），然后训练一个 Q-function（Critic）来评估这些动作的好坏，最后在推理时通过拒绝采样（Rejection Sampling）或者梯度引导选出 Q 值最高的动作。</li> <li> <strong>优势</strong>：既能处理多模态数据，又能通过 Q 值找到比示教数据更好的动作。</li> </ul> </li> </ul> <h3 id="2-在线微调先模仿后强化rl-fine-tuning">2. 在线微调：先模仿，后强化（RL Fine-tuning）</h3> <p>这是目前最直接的思路。先用 Diffusion Policy 进行模仿学习（预训练），得到一个还不错的策略，然后把它放到环境里，用 RL 接着训练，让它根据奖励（Reward）自我进化。</p> <ul> <li> <strong>核心挑战</strong>：Diffusion 的生成过程是一个多步的去噪链（比如100步）。要计算 RL 的梯度（Policy Gradient）并反向传播穿过这一百步，显存消耗巨大且梯度极不稳定。</li> <li> <strong>代表论文</strong>： <ul> <li> <strong>DPPO (Diffusion Policy Policy Optimization)</strong> <a href="https://arxiv.org/abs/2409.00588" rel="external nofollow noopener" target="_blank">arXiv:2409.00588</a>：这是2024年的一篇重磅工作，专门解决上述问题。它提出了一套完整的框架，使得我们可以用类似 PPO 的算法来微调 Diffusion Policy。实验表明，经过微调后，机器人不仅动作更流畅，而且完成任务的成功率远超原始的示教者。</li> <li> <strong>方法</strong>：它不需要反向传播穿过整个去噪链，而是巧妙地利用了 Score Function 的性质来估计梯度，使得微调变得可行且高效。</li> </ul> </li> </ul> <h3 id="3-奖励引导生成reward-guided-generation">3. 奖励引导生成（Reward-Guided Generation）</h3> <p>这种方法不改变 Diffusion Policy 的权重，而是在推理（Inference）阶段“外挂”一个导航员。</p> <ul> <li> <strong>核心思想</strong>：训练一个独立的价值函数 $V(s, a)$ 或分类器。在 Diffusion 去噪的每一步，我们计算 $\nabla_a V(s, a)$，用这个梯度去“推”生成的动作，让它向高价值区域偏移。</li> <li> <strong>类比</strong>：就像你在画画（Diffusion 生成），旁边站着一个老师（RL Critic）。你每画一笔，老师就提醒你“往左一点更好”，最后画出来的结果就会既像原来的风格，又符合老师的要求。</li> <li> <strong>相关工作</strong>：这种思想最早见于 <strong>Decision Diffuser</strong>，虽然它更多用于规划，但其核心逻辑被广泛用于机器人控制中。</li> </ul> <h3 id="总结">总结</h3> <p>如果用一句话概括 Diffusion Policy 和 RL 的关系：</p> <ul> <li> <strong>Diffusion Policy</strong> 提供了 <strong>“像人”</strong>的先验（平滑、拟人、多模态）。</li> <li> <strong>RL</strong> 提供了 <strong>“成功”</strong>的导向（最大化奖励、适应新环境）。</li> </ul> <p>未来的趋势一定是二者的深度融合：<strong>用 Diffusion 保证动作不发生灾难性的变形，用 RL 提升任务的极限性能。</strong></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/DriveJEPA/">DriveJEPA</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/C_RADIOv4/">C_RADIOv4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/VLM4VLA/">VLM4VLA</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 P W Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>