<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RT2 | Tenacious life, proud journey. </title> <meta name="author" content="P W Name"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://beyondpzk.github.io/blog/2023/RT2/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Tenacious life, proud journey. </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">RT2</h1> <p class="post-meta"> Created on July 28, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/vla"> <i class="fa-solid fa-tag fa-sm"></i> VLA</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>[TOC]</p> <h1 id="rt2-vision-language-action-models-transfer-web-knowledge-to-robotic-control">RT2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</h1> <p>研讨的这篇论文——<strong>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</strong>，是Google DeepMind在2023年发布的一项里程碑式工作。这篇论文的核心贡献在于，它打破了传统机器人学习与互联网大规模预训练模型之间的界限，提出了一种被称为<strong>VLA（Vision-Language-Action）</strong>的新范式。</p> <hr> <h3 id="第一部分研究背景与核心动机-context--motivation">第一部分：研究背景与核心动机 (Context &amp; Motivation)</h3> <p>在深入技术细节之前，我们需要理解为什么DeepMind要在这个时间点提出RT-2。</p> <h4 id="1-机器人学习的困境莫拉维克悖论的现代演绎">1. 机器人学习的困境：莫拉维克悖论的现代演绎</h4> <p>长期以来，通用机器人的构建面临一个巨大的数据瓶颈。我们在互联网上拥有数十亿的文本和图像数据，这使得GPT-4或PaLM这样的模型能够展现出惊人的推理能力。然而，机器人数据是非常昂贵的。收集真实的物理交互数据需要时间、硬件磨损和人工监督。</p> <p>这就导致了一个现状：</p> <ul> <li> <strong>大模型（Web-scale Models）：</strong> 拥有极其丰富的语义知识（知道什么是“麦当赫”，知道“海绵”是软的），但缺乏物理世界的具身控制能力。</li> <li> <strong>传统机器人策略（Robot Policies）：</strong> 能够执行精准的动作（如抓取），但语义理解能力极差，往往只能在封闭环境下处理预定义的物体。</li> </ul> <h4 id="2-从rt-1到rt-2的跨越">2. 从RT-1到RT-2的跨越</h4> <p>DeepMind之前的作品RT-1（Robotics Transformer 1）已经证明了基于Transformer架构进行端到端（End-to-End）控制的可行性。RT-1虽然成功，但它并没有真正利用大模型在互联网数据上学到的通用知识。它更像是一个在大规模机器人数据上训练出来的“专家”。</p> <p>RT-2的核心假设非常大胆且优雅：<strong>如果我们能让大模型直接“讲”机器人的语言，是否就能直接继承大模型在互联网上学到的常识、推理和泛化能力？</strong></p> <p>RT-2不仅是学习如何映射观察到动作，更是要享受大规模预训练带来的红利。 <alphaxiv-paper-citation title="Goal" page="1" first="Our goal is" last="vision-language data from the web."></alphaxiv-paper-citation></p> <h4 id="3-vla视觉-语言-动作模型">3. VLA：视觉-语言-动作模型</h4> <p>论文提出了一个新的概念类别：VLA。这不仅仅是VLM（视觉语言模型），而是将“动作（Action）”作为与“文本（Language）”同等地位的一种模态。在RT-2中，机器人动作被处理为一种特殊的“语言”token，直接嵌入到模型的输出空间中。</p> <hr> <h3 id="第二部分模型架构与实现细节-model-architecture--methodology">第二部分：模型架构与实现细节 (Model Architecture &amp; Methodology)</h3> <p>RT-2并不是从零开始训练的，而是基于已有的强大VLM进行改造。</p> <h4 id="1-基础模型骨架-backbone">1. 基础模型骨架 (Backbone)</h4> <p>RT-2探索了两种主要的基础架构，均由Google开发：</p> <ul> <li> <strong>PaLI-X (5B &amp; 55B参数)：</strong> 这是一个典型的Encoder-Decoder架构。它使用ViT-22B处理图像，然后通过一个类似于UL2的Encoder-Decoder主干网络处理文本和图像嵌入。</li> <li> <strong>PaLM-E (12B参数)：</strong> 这是一个Decoder-only的架构（类似GPT）。它将图像投影到语言embedding空间，然后作为一个统一的多模态序列输入给LLM。</li> </ul> <p>这两种架构代表了当前VLM的两个主流方向，RT-2证明了这套方法对二者都有效。</p> <h4 id="2-动作的token化-action-tokenization">2. 动作的Token化 (Action Tokenization)</h4> <p>这是RT-2最关键的技术细节：<strong>如何让一个只会输出文本的模型输出电压或位置指令？</strong></p> <p>RT-2继承了RT-1的离散化动作空间设计。</p> <ul> <li> <strong>动作空间：</strong> 包括6个自由度的末端执行器位姿（x, y, z, roll, pitch, yaw），1个夹爪开合程度，以及1个终止指令（terminate）。共8个维度。</li> <li> <strong>离散化：</strong> 每个连续维度被均匀离散化为256个区间（bins）。</li> <li> <strong>字符串化：</strong> 一个完整的机器人动作被转换为一个由空格分隔的数字字符串。例如：“1 128 91 241 5 101 127”。</li> </ul> <p>为了将自然语言回答和机器人动作适配到同一种格式中，作者将动作表达为文本token。 <alphaxiv-paper-citation title="Format" page="1" first="in order to fit" last="natural language tokens."></alphaxiv-paper-citation></p> <p><strong>具体的Token映射策略：</strong> 由于不同的基础模型使用不同的分词器（Tokenizer），RT-2采用了不同的策略：</p> <ul> <li> <strong>对于PaLI-X：</strong> 因为其词表中本就包含代表整数（如”1”, “2”… “1000”）的token，所以直接将动作bin的数值映射到对应的整数token上。</li> <li> <strong>对于PaLM-E：</strong> 其词表可能没有这种直接对应。RT-2采取了一种“覆写（Overwriting）”策略，选取词表中256个<strong>使用频率最低</strong>的token，强制赋予它们代表动作bin的含义。这实际上是一种<strong>符号微调（Symbol Tuning）</strong>。</li> </ul> <h4 id="3-输入与输出流-io-flow">3. 输入与输出流 (I/O Flow)</h4> <ul> <li> <strong>输入：</strong> 机器人摄像头图像 + 文本指令（例如：”Q: what action should the robot take to [task instruction]? A:”）。</li> <li> <strong>输出：</strong> 一串代表动作的token序列。</li> </ul> <p>在推理过程中，模型会像生成文本一样，自回归地生成这些动作token，然后将其“反Token化（De-Tokenize）”回连续的控制信号发送给机器人。</p> <hr> <h3 id="第三部分训练策略联合微调-co-fine-tuning">第三部分：训练策略——联合微调 (Co-Fine-Tuning)</h3> <p>如果我们只拿机器人数据去微调（Fine-tune）一个预训练好的VLM，会发生什么？答案是<strong>灾难性遗忘（Catastrophic Forgetting）</strong>。模型会迅速学会控制机器人，但会忘记它在互联网数据上学到的“什么是泰勒·斯威夫特”或者“什么是能量饮料”。</p> <p>为了解决这个问题，RT-2提出了<strong>联合微调（Co-Fine-Tuning）</strong>策略。</p> <h4 id="1-数据混合-data-mixture">1. 数据混合 (Data Mixture)</h4> <p>训练数据包含两部分：</p> <ol> <li> <strong>机器人轨迹数据（Robot Data）：</strong> 来自RT-1的数据集，包含图像、指令和动作对。</li> <li> <strong>互联网视觉语言数据（Web Data）：</strong> 原始VLM预训练任务的数据，如视觉问答（VQA）、图像描述（Captioning）等。</li> </ol> <h4 id="2-训练平衡">2. 训练平衡</h4> <p>在每一个训练Batch中，必须同时包含这两类数据。这是一个精细的平衡过程。</p> <ul> <li>如果机器人数据太少，控制策略学不好。</li> <li>如果Web数据太少，语义概念会丢失。</li> <li>实验表明，在联合微调中，需要适当增加机器人数据的采样权重。例如，在RT-2-PaLM-E中，机器人数据的权重约为66%。</li> </ul> <p>这种联合训练使得模型既能接触到抽象的视觉概念，又能学习低层的机器人动作。 <alphaxiv-paper-citation title="Co-Fine-Tuning" page="6" first="We notice that" last="just robot actions."></alphaxiv-paper-citation></p> <h4 id="3-输出约束-output-constraint">3. 输出约束 (Output Constraint)</h4> <p>在推理（Inference）阶段，如果任务是机器人控制，我们必须保证模型输出的是合法的动作token，而不是突然开始写诗。因此，RT-2在解码时会限制采样范围，只允许模型在动作token集合中进行采样。</p> <hr> <h3 id="第四部分实验结果与涌现能力-experiments--emergent-capabilities">第四部分：实验结果与涌现能力 (Experiments &amp; Emergent Capabilities)</h3> <p>这是验证RT-2是否成功的关键。实验使用了6000次真实的机器人评估试验。</p> <h4 id="1-泛化能力的提升-generalization">1. 泛化能力的提升 (Generalization)</h4> <p>与RT-1（基线模型）相比，RT-2在见过的任务（Seen Tasks）上表现相当，但在<strong>未见过的任务（Unseen Tasks）</strong>上展现了惊人的泛化能力。</p> <ul> <li> <strong>未见过的物体（Unseen Objects）：</strong> 比如训练时没见过“海绵”，但测试时让它“拿起海绵”。RT-2可以直接利用VLM的知识识别出海绵并执行抓取。</li> <li> <strong>未见过的背景与环境：</strong> RT-2对光照、背景杂乱程度的鲁棒性远超RT-1。</li> </ul> <p>数据表明，RT-2在未见过的类别上性能提升了约3倍（从RT-1的约30%提升到RT-2的60%以上）。这证明了视觉-语言-动作模型能够将通用的语义理解迁移到机器人控制中。 <alphaxiv-paper-citation title="Generalization" page="2" first="Besides the expected" last="varied instructions,"></alphaxiv-paper-citation></p> <h4 id="2-涌现能力-emergent-capabilities">2. 涌现能力 (Emergent Capabilities)</h4> <p>这是RT-2最令人兴奋的部分。由于它保留了Web知识，它展现出了机器人数据中根本不存在的能力。作者将其分为三类：</p> <ul> <li> <strong>符号理解 (Symbol Understanding)：</strong> <ul> <li>指令：“把可乐放到数字3那里”。</li> <li>机器人数据中从未有过数字标签。但VLM认识数字，RT-2成功将物体放到了写有“3”的卡片上。</li> </ul> </li> <li> <strong>推理 (Reasoning)：</strong> <ul> <li>指令：“把那个适合疲劳的人喝的饮料拿起来”。</li> <li>机器人不知道什么是“疲劳”或“能量饮料”。但VLM知道“红牛”适合疲劳的人。RT-2成功抓取了红牛，而不是旁边的水。</li> <li>指令：“拿起那个可以用来当锤子的东西”。</li> <li>RT-2选择了一块石头。这种<strong>语义推理到物理动作的映射</strong>是前所未有的。</li> </ul> </li> <li> <strong>人类识别 (Human Recognition)：</strong> <ul> <li>指令：“把可乐给戴眼镜的人”。</li> <li>模型成功识别了戴眼镜的人并规划了动作。</li> </ul> </li> </ul> <p>这表明，RT-2不仅仅是在做模式匹配，它实际上是在利用其庞大的知识库来解释指令，并将其转化为物理动作。 <alphaxiv-paper-citation title="Capabilities" page="3" first="This includes significantly" last="another object)."></alphaxiv-paper-citation></p> <h4 id="3-思维链推理-chain-of-thought-cot">3. 思维链推理 (Chain-of-Thought, CoT)</h4> <p>受LLM中CoT技术的启发，作者还尝试让RT-2先生成“计划”，再生成动作。</p> <ul> <li>输入：图像 + 指令“我饿了”。</li> <li>输出：Plan: “pick rxbar chocolate.” -&gt; Action: “1 128…” 这进一步增强了处理复杂多步推理任务的能力。 <alphaxiv-paper-citation title="CoT" page="3" first="We further show" last="energy drink)."></alphaxiv-paper-citation> </li> </ul> <hr> <h3 id="第五部分局限性与讨论-limitations--discussion">第五部分：局限性与讨论 (Limitations &amp; Discussion)</h3> <ol> <li> <strong>物理技能的边界 (Physical Skills Limitation)：</strong> RT-2虽然聪明，但它的<strong>动作库</strong>受限于训练数据。它不能无中生有地学会一个新的物理动作（比如“后空翻”或“精细的插拔”），如果这些动作没有在RT-1的数据集中出现过。它只是学会了在新的场景下<strong>调用</strong>已有的动作技能。 <alphaxiv-paper-citation title="Limitation" page="11" first="the robot does" last="additional experience."></alphaxiv-paper-citation> </li> <li> <strong>推理成本 (Inference Cost)：</strong> 运行一个55B参数的模型进行实时控制是非常昂贵的。虽然作者通过多TPU云服务实现了1-3Hz的控制频率，但这对于高频动态任务（如接球）来说是远远不够的。</li> <li> <strong>闭源与复现：</strong> PaLI-X和PaLM-E是Google的闭源模型，且需要巨大的算力，这使得学术界复现RT-2极其困难。</li> </ol> <h3 id="总结-conclusion">总结 (Conclusion)</h3> <p>RT-2不仅是一个工程上的胜利，它验证了一个重要的假设：<strong>如果我们将动作视为一种语言，那么通用的多模态大模型就可以直接转化为通用的机器人大脑。</strong> 它通过联合微调，成功地将互联网规模的语义知识“蒸馏”到了机器人控制策略中，实现了前所未有的泛化能力和涌现出的推理能力。</p> <h2 id="如何实现对输出做截断-即只输出action">如何实现对输出做截断, 即只输出action</h2> <p>RT-2本质上是一个大语言模型（LLM/VLM），它的默认行为是在整个词表（Vocabulary，通常有几万到几十万个词）中预测下一个词。如果我们不做任何限制，模型完全可能在输出动作指令时突然“走神”，输出“I think the robot should…”这样的文本，这对于实时的机器人控制是致命的。</p> <p>“输出约束”（Output Constraint）并不是一种魔法，而是在<strong>推理（Inference）阶段</strong>通过对模型输出概率分布（Logits）进行干预来实现的。我们可以将其称为<strong>词表掩码（Vocabulary Masking）</strong>或<strong>Logit Masking</strong>。</p> <p>下面我将从原理到实现步骤，详细拆解这个过程：</p> <h3 id="1-前置概念动作token集合-the-valid-action-set">1. 前置概念：动作Token集合 (The Valid Action Set)</h3> <p>首先，我们需要定义什么是“合法的输出”。 在RT-2中，机器人的动作空间被离散化为256个区间（bins）。</p> <ul> <li> <strong>对于PaLI-X版本：</strong> 它是直接使用词表中代表数字的Token（例如 “1”, “2”, …, “256”）。</li> <li> <strong>对于PaLM-E版本：</strong> 它是覆盖了词表中频率最低的256个Token。</li> </ul> <p>无论哪种情况，我们都能确切地知道，只有这<strong>256个特定的Token</strong>（加上一个代表“终止”的Token）是合法的动作指令。我们将这个集合称为 $V_{action}$。</p> <h3 id="2-核心机制logit-masking-logit掩码">2. 核心机制：Logit Masking (Logit掩码)</h3> <p>这是实现“只输出动作”的关键一步。让我们看看在模型推理的每一步发生了什么：</p> <ol> <li> <p><strong>计算Logits：</strong> 模型接收当前的图像和历史文本，经过几十层的Transformer计算，在最后一层输出一个向量。这个向量的维度等于整个词表的大小（例如 $|V| = 250,000$）。向量中的每一个数值叫做 <strong>Logit</strong>，代表模型认为下一个词是该词的“原始得分”。</p> </li> <li> <p><strong>施加约束（The Constraint Step）：</strong> 在将这些Logits送入Softmax层转化为概率之前，我们手动介入。 我们创建一个掩码（Mask），对于所有<strong>不属于</strong>动作集合 $V_{action}$ 的词，我们将它们的Logit值设为一个极小的负数（例如 $-\infty$ 或 $-10^9$）。</p> \[\text{Logit}[i] = \begin{cases} \text{Original\_Logit}[i] &amp; \text{if } \text{Token}_i \in V_{action} \\ -\infty &amp; \text{if } \text{Token}_i \notin V_{action} \end{cases}\] </li> <li> <p><strong>Softmax归一化：</strong> 当我们对处理后的Logits进行Softmax操作时： \(P(x_i) = \frac{e^{\text{Logit}[i]}}{\sum_j e^{\text{Logit}[j]}}\) 由于 $e^{-\infty} \approx 0$，所有非动作Token的概率都会变成严格的 <strong>0</strong>。</p> </li> <li> <p><strong>采样（Sampling）：</strong> 现在，无论模型原本多么想说一句闲话，它的概率都被强制归零了。模型只能在合法的动作Token中选择概率最大的那个（Greedy Search）或者根据概率分布采样（Sampling）。</p> </li> </ol> <p>通过这种方式，我们从数学上强制模型<strong>只能</strong>输出动作指令。 <alphaxiv-paper-citation title="Constraint" page="5" first="The action space consists" last="256 bins uniformly."></alphaxiv-paper-citation></p> <h3 id="3-为什么模型不会感到困惑-role-of-training">3. 为什么模型不会“感到困惑”？ (Role of Training)</h3> <p><em>“强行禁止模型说其他话，模型会不会不知道该选哪个动作了？”</em></p> <p>这就是<strong>联合微调（Co-Fine-Tuning）</strong>的功劳。 在训练阶段，我们已经通过大量的数据（Prompt: “Action: …” -&gt; Label: “1 128 91…“）教会了模型：当看到特定的Prompt（如“Action:”）时，它的概率分布本能地就会向这些数字Token倾斜。</p> <p><strong>Logit Masking</strong> 只是一个“双保险”。实际上，经过训练的RT-2在绝大多数情况下，其原始Logit中最高的几个值本身就是动作Token。Masking主要是为了防止极其罕见的“幻觉”或者在长序列生成中的漂移。</p> <h3 id="4-格式的严格性">4. 格式的严格性</h3> <p>RT-2的输出不仅仅是单个Token，而是一串Token（代表x, y, z, roll, pitch, yaw, gripper）。 通常在工程实现中，这种约束可以更加精细：</p> <ul> <li> <strong>固定长度解码：</strong> 我们知道一个动作包含8个Token。我们可以强制模型只解码8步，然后强制截断。</li> <li> <strong>位置相关约束：</strong> 如果不同维度的动作使用了不同的Token区间（RT-2中通常是统一的256个，但有些变体不同），我们甚至可以在第1步只允许x轴的Token，第2步只允许y轴的Token。</li> </ul> <h3 id="总结">总结</h3> <p>“输出约束”并不是模型“自己”控制的，而是我们在模型大脑（神经网络）和嘴巴（输出）之间加了一个<strong>过滤器</strong>。</p> <ol> <li> <strong>定义</strong>合法的动作Token集合。</li> <li>在推理时，将所有非法Token的得分为<strong>负无穷</strong>。</li> <li>迫使模型只能在动作空间内进行概率选择。</li> </ol> <p>这样，我们就把一个只会“说话”的AI，变成了一个只会“行动”的机器人大脑。</p> <h2 id="vlm能力的保持">VLM能力的保持</h2> <p>这是一个极其重要的问题，因为它触及了<strong>多任务学习（Multi-task Learning）</strong>和<strong>持续学习（Continual Learning）</strong>的核心痛点。</p> <p>简单直接的回答是：<strong>RT-2 的 VLM 能力保持得非常出色，甚至可以说，它不仅保留了原有的视觉语言能力，还将这些能力成功“接地”（Grounding）到了物理世界中。</strong></p> <p>但这并非自然发生的，而是精心设计的训练策略的结果。如果处理不当，模型会迅速“变笨”。让我们从三个层面来详细分析这个问题：</p> <h3 id="1-灾难性遗忘的威胁-the-risk-of-catastrophic-forgetting">1. “灾难性遗忘”的威胁 (The Risk of Catastrophic Forgetting)</h3> <p>在早期的尝试中，研究人员发现，如果你拿一个预训练好的大模型（比如 PaLI-X），只用机器人数据（Robot Data）去微调它，会发生什么？</p> <ul> <li> <strong>现象：</strong> 模型会迅速学会控制机器人，但在几天甚至几小时内，它就会忘记“什么是蒙娜丽莎”、“什么是红色”、“什么是大象”。</li> <li> <strong>后果：</strong> 它的泛化能力会退化到和 RT-1（从头训练的专家模型）差不多的水平。它变成了一个“熟练工”，但不再是一个“通才”。</li> </ul> <p>这在深度学习中被称为<strong>灾难性遗忘</strong>。如果 RT-2 失去了 VLM 能力，那么我们做这所有的一切（引入大模型）就失去了意义。</p> <h3 id="2-rt-2-的解决方案联合微调-co-fine-tuning">2. RT-2 的解决方案：联合微调 (Co-Fine-Tuning)</h3> <p>为了保住 VLM 的能力，RT-2 并没有选择“先训练 VLM，再微调 Robot”，而是选择了<strong>混合训练</strong>。</p> <ul> <li> <strong>数据配比：</strong> 在训练 RT-2 的每一个 Batch 中，既包含机器人轨迹数据，也包含原始的互联网视觉语言数据（Web Data，如 VQA、Captioning）。</li> <li> <strong>机制：</strong> 这迫使模型在学习输出“动作 Token”的同时，必须继续复习如何输出“自然语言 Token”。</li> <li> <strong>结果：</strong> 论文中的消融实验（Ablation Study）明确指出，<strong>Co-fine-tuning（联合微调）</strong> 是 RT-2 能够泛化到未见物体和指令的关键。</li> </ul> <alphaxiv-paper-citation title="Co-Fine-Tuning Importance" page="10" first="We attribute this to" last="the VLM training."></alphaxiv-paper-citation> <p>论文中写道：“We attribute this to the fact that keeping the original data around the fine-tuning part of training, allows the model to not forget its previous concepts learned during the VLM training.”（我们将此归因于在微调过程中保留原始数据，使得模型不会忘记在 VLM 训练期间学到的先前概念。）</p> <h3 id="3-证据涌现能力的证明-evidence-from-emergent-capabilities">3. 证据：涌现能力的证明 (Evidence from Emergent Capabilities)</h3> <p>虽然论文主要展示的是机器人的成功率，但这些成功率本身就是 VLM 能力得以保持的<strong>铁证</strong>。我们可以通过 RT-2 能完成的任务反推它的 VLM 能力：</p> <ul> <li> <strong>证据一：数学与逻辑推理</strong> <ul> <li>任务：“将香蕉放到 2 + 1 的和那里。”（Move banana near the sum of two plus one）</li> <li>分析：机器人数据里绝对没有数学题。如果 RT-2 忘记了 VLM 的数学能力，它根本无法完成这个任务。它能做，说明它保留了 PaLM/PaLI 的数学推理能力。</li> </ul> </li> <li> <strong>证据二：名人与知识识别</strong> <ul> <li>任务：“把可乐移给泰勒·斯威夫特（Taylor Swift）的照片。”</li> <li>分析：机器人从未见过泰勒·斯威夫特。模型能认出来，说明它保留了互联网预训练中关于名人的视觉知识。</li> </ul> </li> <li> <strong>证据三：多语言能力</strong> <ul> <li>任务：使用西班牙语或德语指令控制机器人。</li> <li>分析：RT-2 依然能听懂。这说明它的多语言编码器没有被破坏。</li> </ul> </li> </ul> <alphaxiv-paper-citation title="Reasoning" page="22" first="move banna near the" last="of two plus one"></alphaxiv-paper-citation> <h3 id="4-总结与教学延伸">4. 总结与教学延伸</h3> <p>同学们，这里有一个深刻的洞察：在 RT-2 中，<strong>“动作”被视为一种特殊的“语言”</strong>。</p> <p>这就好比一个原本精通英语、法语、中文的人（VLM），现在我们要教它一门新语言——“机器人语”（Robot Action Tokens）。</p> <ul> <li>如果我们只强迫它说“机器人语”，不准它说别的，时间久了它就会忘记英语。</li> <li>RT-2 的做法是：让它在学“机器人语”的同时，继续练习英语和法语。</li> </ul> <p>最终，RT-2 成为了一位<strong>四语通（English, French, Chinese, Robot-ese）</strong>。它不仅保留了原有的语言能力，还能用原本的语言能力（比如用英语思考逻辑）来指导新的语言（用机器人语行动）。</p> <p>所以，RT-2 证明了：只要通过<strong>联合微调（Co-fine-tuning）</strong>并保持适当的数据配比，VLM 的通用能力不仅可以保持，还能成为机器人策略泛化的基石。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/DriveJEPA/">DriveJEPA</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/C_RADIOv4/">C_RADIOv4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/GeRo/">GeRo</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 P W Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>