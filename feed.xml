<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://beyondpzk.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://beyondpzk.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-07T13:14:40+00:00</updated><id>https://beyondpzk.github.io/feed.xml</id><title type="html">Tenacious life, proud journey.</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">DriveJEPA</title><link href="https://beyondpzk.github.io/blog/2026/DriveJEPA/" rel="alternate" type="text/html" title="DriveJEPA"/><published>2026-01-29T00:00:00+00:00</published><updated>2026-01-29T00:00:00+00:00</updated><id>https://beyondpzk.github.io/blog/2026/DriveJEPA</id><content type="html" xml:base="https://beyondpzk.github.io/blog/2026/DriveJEPA/"><![CDATA[<p>[TOC]</p> <h1 id="drivejepa">DriveJEPA</h1> <p><a href="https://arxiv.org/abs/2601.22032">论文链接</a></p> <h1 id="drive-jepa--视频联合嵌入预测架构与多模态轨迹蒸馏在端到端驾驶中的应用">Drive-JEPA —— 视频联合嵌入预测架构与多模态轨迹蒸馏在端到端驾驶中的应用</h1> <p><strong>论文题目：</strong> Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving</p> <hr/> <h2 id="第一部分背景与动机">第一部分：背景与动机</h2> <h3 id="11-端到端自动驾驶的现状与瓶颈">1.1 端到端自动驾驶的现状与瓶颈</h3> <p>在深入 Drive-JEPA 之前，我们需要回顾端到端自动驾驶（End-to-End Autonomous Driving）的核心理念及其当前面临的两大主要挑战。</p> <ul> <li><strong>从模块化到端到端：</strong> 传统自动驾驶采用模块化管道（感知 $\to$ 预测 $\to$ 规划），虽然可解释性强，但存在累积误差和信息丢失。端到端方法试图通过统一的神经网络，直接将原始传感器数据映射到驾驶行为，旨在减少信息丢失并利用大规模数据。 <alphaxiv-paper-citation title="Introduction" page="1" first="End-to-end autonomous driving" last="neural model."></alphaxiv-paper-citation></li> <li><strong>挑战一：表征学习的局限性（Representation Bottleneck）</strong> 目前端到端模型通常依赖视频预训练来理解场景。主流的“世界模型”（World Models）方法分为两类： <ol> <li><strong>视频生成式（Video-generative）：</strong> 试图重建或生成像素级视频。这计算量巨大，且过于关注视觉细节（如树叶的纹理），而这些细节对驾驶决策往往无关紧要。</li> <li><strong>潜空间动力学（Latent World Models）：</strong> 预测特征的演变。但这通常只作为辅助目标，并未展示出随着预训练规模扩大而带来的显著性能提升。 <alphaxiv-paper-citation title="World Models" page="1" first="However, pretraining video" last="limited improvements."></alphaxiv-paper-citation></li> </ol> </li> <li><strong>挑战二：多模态行为的监管缺失（Supervision Bottleneck）</strong> 驾驶本质上是多模态的（Multimodal）。在一个路口，左转、直行或右转可能都是合法的。然而，人类驾驶数据集通常每一种场景只提供<strong>一条</strong>轨迹（Ground Truth）。如果我们只用这一条轨迹做监督，模型会丢失其他可行解的多样性，导致在未见过的场景中泛化能力差。 <alphaxiv-paper-citation title="Ambiguity" page="1" first="This limitation is" last="multimodal behaviors."></alphaxiv-paper-citation></li> </ul> <h3 id="12-核心解决方案概览">1.2 核心解决方案概览</h3> <p>Drive-JEPA 针对上述两个痛点提出了针对性的解决方案：</p> <ol> <li><strong>针对表征学习：</strong> 引入 <strong>V-JEPA (Video Joint-Embedding Predictive Architecture)</strong> 进行预训练。它不重建像素，而是在特征空间预测未来，从而高效地学习对规划有用的语义特征。</li> <li><strong>针对多模态监管：</strong> 提出 <strong>多模态轨迹蒸馏 (Multimodal Trajectory Distillation)</strong>。利用仿真器（Simulator）生成多条安全轨迹作为“伪教师”，补充单一的人类数据。</li> </ol> <hr/> <h2 id="第二部分drive-jepa-方法论详解">第二部分：Drive-JEPA 方法论详解</h2> <h3 id="21-架构总览-figure-2-解析">2.1 架构总览 (Figure 2 解析)</h3> <p>Drive-JEPA 的框架包含三个核心组件：</p> <ol> <li><strong>驾驶视频预训练 (Driving Video Pretraining)</strong>：使用 V-JEPA 学习视觉编码器。</li> <li><strong>基于锚点的提案生成 (Waypoint-anchored Proposal Generation)</strong>：生成候选轨迹。</li> <li><strong>多模态轨迹蒸馏与选择 (Distillation &amp; Selection)</strong>：利用仿真器数据优化轨迹分布，并选择最佳路径。 <alphaxiv-paper-citation title="Framework" page="2" first="Specifically, our framework" last="Trajectory Selection."></alphaxiv-paper-citation></li> </ol> <h3 id="22-核心组件一v-jepa-驾驶视频预训练">2.2 核心组件一：V-JEPA 驾驶视频预训练</h3> <p>这是该论文的一大亮点，它将 LeCun 提出的 JEPA 架构成功适配到了驾驶领域。</p> <ul> <li><strong>原理：</strong> V-JEPA 不同于生成式模型（如 MAE 或 VideoMAE），它不预测被遮挡的像素，而是预测被遮挡区域的<strong>潜在特征（Latent Representation）</strong>。</li> <li><strong>流程：</strong> <ol> <li><strong>输入：</strong> 连续的驾驶视频帧。</li> <li><strong>掩码策略：</strong> 随机遮挡视频的时空块。</li> <li><strong>目标：</strong> 编码器提取可见部分的特征，预测器（Predictor）根据这些特征预测被遮挡部分的特征表示。</li> </ol> </li> <li><strong>优势：</strong> 这种方法避免了像素级重建的高昂计算成本，专注于学习场景的高层语义（如物体运动、道路拓扑），这与规划任务（Planning）更加对齐。</li> <li><strong>成果：</strong> 作者在 208 小时的视频数据上进行了预训练，相比之前的像素重建方法，计算效率更高。 <alphaxiv-paper-citation title="V-JEPA" page="2" first="In the first" last="collapse prevention."></alphaxiv-paper-citation></li> </ul> <h3 id="23-核心组件二基于锚点的提案生成">2.3 核心组件二：基于锚点的提案生成</h3> <p>有了强大的特征提取器后，如何生成规划轨迹？Drive-JEPA 采用了一种“提案-选择”（Proposal-Selection）的范式，但这与传统的固定词表不同。</p> <ul> <li><strong>动态提案：</strong> 模型不是从固定的轨迹库中选，而是动态生成提案。</li> <li><strong>机制：</strong> <ol> <li><strong>查询初始化：</strong> 使用自车状态（Ego Status）初始化一组可学习的查询向量（Queries）。</li> <li><strong>迭代优化：</strong> 使用 Deformable Attention 机制，这些查询向量在 BEV（鸟瞰图）特征图上聚合信息，并迭代地修正轨迹锚点（Anchors）。</li> <li><strong>输出：</strong> 输出 $N$ 条候选轨迹，每条轨迹由一系列时空路点 $(x, y, \text{heading})$ 组成。 <alphaxiv-paper-citation title="Proposal Gen" page="2" first="In the second" last="refine proposals iteratively."></alphaxiv-paper-citation></li> </ol> </li> </ul> <h3 id="24-核心组件三多模态轨迹蒸馏-核心创新">2.4 核心组件三：多模态轨迹蒸馏 (核心创新)</h3> <p>这是解决“单一人类轨迹监管”问题的关键。</p> <ul> <li><strong>问题：</strong> 如果只用人类的一条轨迹做 Loss（如 L2 距离），模型会倾向于坍缩到单一模态，或者输出多模态的平均值（这是不安全的）。</li> <li><strong>解决方案：</strong> 引入仿真器作为“老师”。 <ol> <li><strong>仿真生成：</strong> 在训练阶段，利用仿真器（Simulator）基于当前场景生成大量随机轨迹。</li> <li><strong>筛选：</strong> 筛选出那些符合动力学约束且无碰撞的“高质量”轨迹。</li> <li><strong>蒸馏：</strong> 将这些轨迹作为额外的监督信号。模型生成的 $N$ 条提案不仅要逼近人类轨迹，还要覆盖仿真器生成的这些合法的多模态轨迹。</li> </ol> </li> <li><strong>意义：</strong> 这极大地丰富了训练信号，教会模型“除了人类这样做，那样做也是安全的”。 <alphaxiv-paper-citation title="Distillation" page="2" first="proposals are supervised" last="from the simulator."></alphaxiv-paper-citation></li> </ul> <h3 id="25-动量感知轨迹选择-momentum-aware-selection">2.5 动量感知轨迹选择 (Momentum-aware Selection)</h3> <p>生成了多条轨迹后，如何选择最终执行的那一条？</p> <ul> <li><strong>评分器：</strong> 模型预测每条轨迹的安全性（碰撞风险）、舒适度和交通规则符合度。</li> <li><strong>动量机制：</strong> 为了防止控制信号在帧与帧之间剧烈跳变（造成“画龙”现象），引入了动量感知惩罚。当前选择的轨迹应与上一帧规划的轨迹保持一定的一致性。</li> <li><strong>公式逻辑：</strong> 最终得分 = 预测质量得分 - 轨迹形变惩罚。 <alphaxiv-paper-citation title="Selection" page="2" first="incorporates a momentum-aware" last="trajectory distortion."></alphaxiv-paper-citation></li> </ul> <hr/> <h2 id="第三部分实验结果与讨论">第三部分：实验结果与讨论</h2> <h3 id="31-实验设置">3.1 实验设置</h3> <ul> <li><strong>数据集：</strong> NAVSIM v1 和 NAVSIM v2（基于 nuPlan 的大规模闭环仿真评测基准），以及 Bench2Drive。</li> <li><strong>评估指标：</strong> PDMS (Predictive Driving Model Score)。这是一个综合指标，不仅仅看轨迹与人类的重合度（L2 error），更看重闭环仿真中的安全性、舒适度和进度。 <alphaxiv-paper-citation title="Evaluation" page="2" first="We validate Drive-JEPA" last="Jia et al., 2024)."></alphaxiv-paper-citation></li> </ul> <h3 id="32-核心结果分析">3.2 核心结果分析</h3> <ol> <li><strong>State-of-the-Art (SOTA) 表现：</strong> <ul> <li>在 NAVSIM v1 上，Drive-JEPA 达到了 <strong>93.3 PDMS</strong>。</li> <li>在 NAVSIM v2 上，达到了 <strong>87.8 EPDMS</strong>。</li> <li><strong>结论：</strong> 这刷新了目前的最佳成绩，证明了该框架的有效性。 <alphaxiv-paper-citation title="Results" page="1" first="The complete Drive-JEPA" last="state-of-the-art."></alphaxiv-paper-citation></li> </ul> </li> <li><strong>V-JEPA 的有效性（Perception-Free Setting）：</strong> <ul> <li>为了验证 V-JEPA 预训练是否真的有用，作者在一个纯感知无关（Perception-Free）的设置下进行了测试（即不使用检测框等中间任务，纯端到端）。</li> <li><strong>结果：</strong> 仅使用 V-JEPA 预训练的 ViT 编码器配合简单的 Transformer 解码器，就比之前的方法高出 <strong>3 PDMS</strong>。</li> <li><strong>解读：</strong> 这有力地证明了 V-JEPA 学到了对规划极其关键的特征，而不仅仅是视觉特征。 <alphaxiv-paper-citation title="Perception-Free" page="1" first="outperforms prior methods" last="perception-free setting."></alphaxiv-paper-citation></li> </ul> </li> <li><strong>多模态蒸馏的贡献：</strong> <ul> <li>消融实验显示，引入多模态轨迹蒸馏显著提升了驾驶质量。尤其是在 Bench2Drive 这种复杂场景较多的测试中，模型能够处理更多样化的路况，避免了单一模仿人类可能导致的死板或危险行为。 <alphaxiv-paper-citation title="Ablation" page="2" first="Multimodal Trajectory Distillation" last="multimodal trajectories."></alphaxiv-paper-citation></li> </ul> </li> </ol> <h3 id="33-总结与讨论">3.3 总结与讨论</h3> <p><strong>总结：</strong> Drive-JEPA 成功地将非生成式视频预训练（V-JEPA）与基于仿真器的知识蒸馏结合起来。它不仅提升了特征的鲁棒性，还解决了端到端学习中数据分布稀疏的问题。</p> <p><strong>思考题：</strong></p> <ol> <li><strong>V-JEPA vs. 生成式模型：</strong> 既然生成式模型（如 Sora）展现了强大的物理世界理解能力，为什么 Drive-JEPA 认为像素级生成对驾驶是不必要的？未来随着算力提升，这一观点会改变吗？</li> <li><strong>仿真器的依赖：</strong> 该方法依赖仿真器生成“伪教师”轨迹。如果仿真器本身的物理模型不准确（Sim-to-Real Gap），会对实车部署造成什么影响？如何缓解？</li> <li><strong>安全性保障：</strong> 虽然使用了多模态蒸馏，但端到端模型仍然是个“黑盒”。在实际部署中，我们如何为这种基于神经网络的规划器加上确定性的安全围栏（Safety Guardrails）？</li> </ol> <h1 id="思考题解答">思考题解答</h1> <p>好的，这是为您准备的针对那三个讨论题的详细回答。这些回答结合了 <strong>Drive-JEPA</strong> 论文的具体内容以及更广泛的自动驾驶领域知识，旨在帮助学生深入理解背后的工程权衡和理论基础。</p> <hr/> <h3 id="讨论题-1v-jepa-vs-生成式模型如-soravideomae">讨论题 1：V-JEPA vs. 生成式模型（如 Sora/VideoMAE）</h3> <p><strong>问题回顾：</strong> 既然生成式模型展现了强大的物理世界理解能力，为什么 Drive-JEPA 认为像素级生成对驾驶是不必要的？未来随着算力提升，这一观点会改变吗？</p> <p><strong>详细解答：</strong></p> <ol> <li><strong>信息密度与相关性的权衡（Signal-to-Noise Ratio）：</strong> <ul> <li><strong>核心观点：</strong> Drive-JEPA 的核心论点是“驾驶决策不需要像素级的完美”。在驾驶场景中，像素空间包含了大量与规划无关的高频信息（例如：树叶随风摆动的纹理、路边广告牌的具体内容、云层的形状）。生成式模型（Generative Models）强迫网络去学习并重建这些细节，这不仅浪费了巨大的计算资源，还可能导致模型“过拟合”到视觉细节上，而忽略了物体间的相对运动、遮挡关系等对驾驶至关重要的<strong>语义信息</strong>。</li> <li><strong>论文佐证：</strong> 作者明确指出，像素级目标（Pixel-level objective）会带来沉重的计算负担，并且可能过分强调与决策无关的视觉细节。 <alphaxiv-paper-citation title="Generative Limitations" page="1" first="pixel-level objective incurs" last="to decision making."></alphaxiv-paper-citation></li> <li><strong>V-JEPA 的优势：</strong> V-JEPA 在 <strong>特征空间（Latent Space)</strong>进行预测。它实际上是在学习一种“抽象”，即只保留那些在时间上具有预测性的信息（通常是物体的位置、类别、运动状态），而丢弃不可预测的噪音（光照的随机闪烁、纹理细节）。这种抽象恰恰是规划模块（Planner）最需要的。(我觉得这是做和规划相关的worldmodel要一直铭记的初衷,即到底什么样的信息是planner模块需要的。)</li> </ul> </li> <li><strong>计算效率与实时性（Efficiency）：</strong> <ul> <li>端到端自动驾驶模型需要在车端芯片上实时运行。生成式模型的解码器（Decoder）通常非常庞大，推理延迟高。而 V-JEPA 的预测头（Predictor）是轻量级的，且在推理时甚至不需要运行预测头（只用 Encoder），这使得它在部署时极其高效。</li> </ul> </li> <li><strong>未来的展望（Future Perspective）：</strong> <ul> <li><strong>算力不是唯一瓶颈：</strong> 即使未来算力无限，<strong>Yann LeCun</strong>（JEPA 的提出者）的理论认为，<strong>在不确定性极高的世界中进行像素级预测在数学上是不适定（Ill-posed）的</strong>。例如，预测一辆车“可能会左转”是容易的，但预测这辆车左转时每一个像素的 RGB 值是非常难且没必要的。</li> <li><strong>可能的融合：</strong> 未来更有可能出现的趋势是<strong>混合架构</strong>。即底层使用 V-JEPA 学习物理常识和动力学，上层使用轻量级的生成模块用于“可解释性可视化”（例如生成未来场景给人类安全员看，而不是给控制算法看）。只要驾驶的本质是“做决策”而非“画图”，特征空间学习（Latent Learning）大概率仍是主流。</li> </ul> </li> </ol> <hr/> <h3 id="讨论题-2仿真器的依赖与-sim-to-real-gap">讨论题 2：仿真器的依赖与 Sim-to-Real Gap</h3> <p><strong>问题回顾：</strong> 该方法依赖仿真器生成“伪教师”轨迹。如果仿真器本身的物理模型不准确，会对实车部署造成什么影响？如何缓解？</p> <p><strong>详细解答：</strong></p> <ol> <li><strong>Drive-JEPA 中仿真器的角色：</strong> <ul> <li>首先需要明确，Drive-JEPA 使用仿真器并不是为了生成<strong>图像（Sensor Data）</strong>，而是为了生成<strong>轨迹（Future Trajectories）</strong>。这是一个关键的区别。</li> <li>论文提到，他们使用仿真器来生成“多样化的轨迹”，并根据动力学约束筛选出无碰撞的路径作为额外的监督信号。 <alphaxiv-paper-citation title="Sim Distillation" page="1" first="distills diverse" last="human trajectories."></alphaxiv-paper-citation></li> </ul> </li> <li><strong>Sim-to-Real Gap 的具体表现：</strong> <ul> <li><strong>动力学差异（Dynamics Gap）：</strong> 仿真器通常使用简化的车辆动力学模型（如单车模型 Bicycle Model）。然而，真实车辆在高速转弯、湿滑路面或轮胎磨损情况下的响应是高度非线性的。如果模型在训练时认为“以 80km/h 速度急转弯是安全的（因为仿真器没报错）”，在实车上可能会导致侧滑或失控。</li> <li><strong>行为逻辑差异（Behavioral Gap）：</strong> 仿真器中的其他车辆（NPC）通常遵循规则（Rule-based），行为比较死板。这可能导致模型学不到如何处理真实世界中人类驾驶员的博弈、犹豫或违规行为。</li> </ul> </li> <li><strong>缓解策略：</strong> <ul> <li><strong>保守性筛选（Conservative Filtering）：</strong> 在生成蒸馏数据时，必须设置比真实物理极限更严格的阈值。例如，如果物理极限侧向加速度是 $0.8g$，仿真筛选时可能限制在 $0.5g$。</li> <li><strong>闭环微调（Closed-loop Finetuning）：</strong> 在仿真训练后，必须在真实数据上进行少量的微调，或者使用<strong>逆强化学习（Inverse Reinforcement Learning）</strong>，让模型重新对齐人类的驾驶风格。</li> <li><strong>混合监督（Hybrid Supervision）：</strong> Drive-JEPA 并没有完全抛弃人类数据，而是将“仿真轨迹”与“人类轨迹”结合。人类轨迹保证了风格像人（拟人化），仿真轨迹提供了多样性和安全性边界（鲁棒性）。这种组合本身就是一种缓解 Sim-to-Real Gap 的手段。 <alphaxiv-paper-citation title="Selection Mechanism" page="1" first="momentum-aware selection" last="safe behavior."></alphaxiv-paper-citation></li> </ul> </li> </ol> <hr/> <h3 id="讨论题-3安全性保障与安全围栏safety-guardrails">讨论题 3：安全性保障与安全围栏（Safety Guardrails）</h3> <p><strong>问题回顾：</strong> 虽然使用了多模态蒸馏，但端到端模型仍然是个“黑盒”。在实际部署中，我们如何为这种基于神经网络的规划器加上确定性的安全围栏？</p> <p><strong>详细解答：</strong></p> <ol> <li><strong>神经网络的概率本质 vs. 驾驶的确定性要求：</strong> <ul> <li>Drive-JEPA 输出的是轨迹的概率分布或评分。神经网络本质上是基于统计的，它无法保证 100% 不犯错（例如在长尾分布的场景下）。</li> </ul> </li> <li><strong>Drive-JEPA 内部的软约束（Soft Guardrails）：</strong> <ul> <li>论文中提到的 <strong>“动量感知选择机制”（Momentum-aware selection mechanism）</strong> 就是一种内置的软约束。它强迫模型输出的轨迹在时间上是连续的，防止出现“上一帧左转，下一帧突然右转”的危险抖动。 <alphaxiv-paper-citation title="Momentum Selection" page="1" first="incorporates a momentum-aware" last="trajectory distortion."></alphaxiv-paper-citation></li> </ul> </li> <li><strong>外部的硬约束（Hard Guardrails）—— 工业界标准做法：</strong> 在实际部署中，通常采用 <strong>“规划-校验-回退”（Plan-Check-Fallback）</strong> 架构： <ul> <li><strong>第一层：模型规划（Planner）：</strong> Drive-JEPA 生成 Top-K 条候选轨迹。</li> <li><strong>第二层：规则校验（Rule-based Checker）：</strong> 这是一个轻量级、确定性的代码模块。它利用高精地图和感知边界框，对候选轨迹进行物理校验。 <ul> <li><em>碰撞检测：</em> 轨迹是否与障碍物重叠？</li> <li><em>动力学检测：</em> 曲率是否过大？加速度是否超标？</li> <li><em>交通规则：</em> 是否闯红灯或逆行？</li> </ul> </li> <li><strong>第三层：轨迹截断或回退（Fallback）：</strong> <ul> <li>如果 Drive-JEPA 的最优轨迹未通过校验，则顺次检查第二、第三优轨迹。</li> <li>如果所有轨迹都失败，系统将触发<strong>最小风险策略（Minimum Risk Maneuver, MRM）</strong>，通常是紧急制动或缓慢靠边停车。</li> </ul> </li> </ul> </li> <li><strong>总结：</strong> Drive-JEPA 负责“聪明地驾驶”（舒适、高效、拟人），而外部的规则系统负责“不撞车”。</li> </ol> <h1 id="其他">其他</h1> <ol> <li>在做Drive-JEPA的pre-training的时候, 是在独立的视频片段上面进行的; 但是在下游规划任务中, 模型的输入是 “Multi-View” 的图像序列.</li> <li>Vision Encoder 用V-JEPA训完之后, 会进行微调, 即只是作为初始化的weights. 预训练的目的是为了让编码器先”看懂”视频中的物理规律(即获得更好的Representation). 论文中的 Table 7 对比了不同的vision pretrain model 的效果.</li> </ol>]]></content><author><name></name></author><category term="WorldModel &amp; E2E"/><summary type="html"><![CDATA[[TOC] DriveJEPA]]></summary></entry><entry><title type="html">C_RADIOv4</title><link href="https://beyondpzk.github.io/blog/2026/C_RADIOv4/" rel="alternate" type="text/html" title="C_RADIOv4"/><published>2026-01-24T00:00:00+00:00</published><updated>2026-01-24T00:00:00+00:00</updated><id>https://beyondpzk.github.io/blog/2026/C_RADIOv4</id><content type="html" xml:base="https://beyondpzk.github.io/blog/2026/C_RADIOv4/"><![CDATA[<p>[TOC]</p> <h1 id="c_radiov4">C_RADIOv4</h1> <p><a href="https://www.arxiv.org/abs/2601.17237">论文链接</a></p> <hr/> <h1 id="模组一聚合模型范式与教师阵容演变">模组一：聚合模型范式与教师阵容演变</h1> <p><strong>目标</strong>：理解“聚合(Agglomeration)”的数学定义，分析为什么选择这些特定的教师模型，以及从 AM-RADIO 到 v4 的架构演进逻辑。</p> <h2 id="1-课程概览与学习目标">1: 课程概览与学习目标</h2> <ul> <li><strong>核心论文</strong>: <em>C-RADIOv4: Agglomerative Vision Backbones Technical Report</em> (2026)</li> <li><strong>学习目标</strong>: <ol> <li>掌握<strong>多教师蒸馏 (Multi-Teacher Distillation)</strong> 的架构设计。</li> <li>理解如何解决多分辨率训练中的 <strong>“Mode Switching”</strong> 问题。</li> <li>深入分析 <strong>Shift Equivariance (平移等变性)</strong> 在消除固定模式噪声中的作用。</li> <li>探讨 <strong>MESA</strong> 与 <strong>Balanced Summary Loss</strong> 的几何意义。</li> </ol> </li> </ul> <h2 id="2-基础模型的新范式---聚合-agglomeration">2: 基础模型的新范式 - 聚合 (Agglomeration)</h2> <ul> <li><strong>传统 vs. 聚合</strong>: <ul> <li><em>传统方法</em>: 从头训练 (CLIP, MAE)，依赖海量数据和单一目标函数。</li> <li><em>聚合方法</em>: 蒸馏特征表示。我们不是在学习数据，而是在<strong>学习“模型以此方式理解数据”的能力</strong>。</li> </ul> </li> <li><strong>核心定义</strong>: <ul> <li>通过蒸馏异构模型（Heterogeneous models）的特征来创建新模型。 <alphaxiv-paper-citation title="Definition" page="1" first="distilling the feature" last="heterogeneous models."></alphaxiv-paper-citation></li> <li>这不仅是模型压缩，更是<strong>能力融合</strong>。</li> </ul> </li> <li><strong>关键优势</strong>: <ul> <li><strong>统一接口</strong>: 将多个专用模型（检测、分割、文本对齐）压缩进一个 backbone。</li> <li><strong>计算效率</strong>: 在相同计算复杂度下，提供更强的下游任务性能。 <alphaxiv-paper-citation title="Efficiency" page="1" first="offering strong improvements" last="computational complexity."></alphaxiv-paper-citation></li> </ul> </li> </ul> <h2 id="3-演进史---从-am-radio-到-c-radiov4">3: 演进史 - 从 AM-RADIO 到 C-RADIOv4</h2> <ul> <li><strong>AM-RADIO (v1)</strong>: <ul> <li><em>创新</em>: 首次提出聚合概念。</li> <li><em>缺陷</em>: <strong>Mode Switching (模式切换)</strong>。学生模型学会了根据输入分辨率“作弊”，改变表示分布以最小化 Loss，导致推理时不一致。 <alphaxiv-paper-citation title="Mode Switching" page="1" first="student model learned" last="training loss,"></alphaxiv-paper-citation></li> </ul> </li> <li><strong>RADIOv2.5 &amp; PHI-S</strong>: <ul> <li><em>改进</em>: 引入 <strong>PHI-S</strong> (Teacher Distribution Balancing)，强制归一化教师分布。</li> <li><em>策略</em>: 对所有教师在所有分辨率下进行训练，解决了模式切换。 <alphaxiv-paper-citation title="v2.5 Fix" page="1" first="training against all" last="mode switching issue,"></alphaxiv-paper-citation></li> </ul> </li> <li><strong>C-RADIOv4</strong>: <ul> <li><em>核心升级</em>: 教师阵容换血 (SigLIP2, DINOv3, SAM3)。</li> <li><em>工程突破</em>: 任意分辨率支持 (Any-resolution support) 和 ViTDet 模式回归。 <alphaxiv-paper-citation title="v4 Upgrades" page="1" first="improves any-resolution support," last="ViTDet option"></alphaxiv-paper-citation></li> </ul> </li> </ul> <h2 id="4-教师阵容深度解析-为什么是它们">4: 教师阵容深度解析 (为什么是它们？)</h2> <ul> <li><strong>原则</strong>: “Improved teachers tend to yield improved students” (强师出高徒)。 <alphaxiv-paper-citation title="Principle" page="1" first="improved teachers tend" last="yield improved students,"></alphaxiv-paper-citation></li> <li><strong>1. SigLIP2 (Text-Image Alignment)</strong>: <ul> <li><em>替代了谁</em>: DFN CLIP。</li> <li><em>原因</em>: SigLIP2 是当前文本-图像编码器的前沿，且应用更广泛（如 Qwen2-VL）。 <alphaxiv-paper-citation title="SigLIP2" page="1" first="SigLIP2 [ 21] has" last="foundation encoder,"></alphaxiv-paper-citation></li> </ul> </li> <li><strong>2. DINOv3 (Dense Representation)</strong>: <ul> <li><em>能力</em>: 极强的密集感知模型，推动了 SSL 的边界。</li> <li><em>贡献</em>: 赋予学生模型强大的语义分割能力。 <alphaxiv-paper-citation title="DINOv3" page="1" first="DINOv3’s improved" last="segmentation capability,"></alphaxiv-paper-citation></li> </ul> </li> <li><strong>3. SAM3 (Segmentation)</strong>: <ul> <li><em>特殊情况</em>: SAM3 作为教师并未直接提升 benchmark 指标。</li> <li><em>战略价值</em>: 允许 C-RADIO 直接替换 SAM3 的 Vision Encoder，实现更高效的“SAM with RADIO”。 <alphaxiv-paper-citation title="SAM3 Value" page="1" first="SAM3 as a" last="selected benchmarks,"></alphaxiv-paper-citation></li> </ul> </li> </ul> <h2 id="5-架构决策---放弃-dfn-clip">5: 架构决策 - 放弃 DFN CLIP</h2> <ul> <li><strong>决策</strong>: 在 v4 中移除了 DFN CLIP 支持。</li> <li><strong>理由</strong>: <ol> <li><strong>计算减负</strong>: 减少教师数量降低训练开销。 <alphaxiv-paper-citation title="Compute" page="1" first="reduce the computational" last="DFN CLIP"></alphaxiv-paper-citation></li> <li><strong>功能重叠</strong>: SigLIP2 与 DFN CLIP 的表征和应用领域高度相似，但前者更强、更通用。 <alphaxiv-paper-citation title="Redundancy" page="3" first="both models have" last="application domains."></alphaxiv-paper-citation></li> </ol> </li> </ul> <hr/> <h1 id="模组二核心工程挑战---分辨率与噪声控制-第二小时">模组二：核心工程挑战 - 分辨率与噪声控制 (第二小时)</h1> <p><strong>目标</strong>：深入理解多分辨率训练的数学实现，以及如何通过 Shift Equivariance Loss 消除“伪影”和“固定模式噪声”。</p> <h2 id="6-随机分辨率训练-stochastic-resolutions">6: 随机分辨率训练 (Stochastic Resolutions)</h2> <ul> <li><strong>v2.5 的做法</strong>: 仅在 2 个固定分辨率下训练。</li> <li><strong>v4 的改进</strong>: 引入更密集的采样集合，实现平滑的分辨率缩放。 <ul> <li><strong>低分辩率分区</strong>: ${128, 192, 224, 256, 384, 432}$</li> <li><strong>高分辨率分区</strong>: ${512, 768, 1024, 1152}$ <alphaxiv-paper-citation title="Res Sets" page="3" first="sample from" last="high-resolution partition,"></alphaxiv-paper-citation></li> </ul> </li> <li><strong>教师适配技术</strong>: <ul> <li><strong>SigLIP2</strong>: 使用 <strong>FeatSharp</strong> 算法进行 $3\times$ 上采样（从 384px 到 1152px）。 <alphaxiv-paper-citation title="FeatSharp" page="3" first="FeatSharp [18 ] to" last="high-resolution training"></alphaxiv-paper-citation></li> <li><strong>SAM3</strong>: 使用 Mosaic Augmentation (马赛克增强)，因为它仅支持 $1152 \times 1152$ 输入。 <alphaxiv-paper-citation title="Mosaic" page="3" first="use the mosaic" last="augmentation as proposed"></alphaxiv-paper-citation></li> </ul> </li> </ul> <h2 id="7-问题的核心---固定模式噪声-fixed-pattern-noise">7: 问题的核心 - 固定模式噪声 (Fixed Pattern Noise)</h2> <ul> <li><strong>现象</strong>: 即使在均匀图像区域，教师模型也会输出高能量特征（伪影/噪声）。</li> <li><strong>数学建模</strong>: 参考 DVT [23]，特征输出 $V_{iT}(x)$ 可以分解为： \(V_{iT}(x) \approx f(x) + g(E_{pos}) + h(x, E_{pos})\) <ul> <li>$f(x)$: 输入相关的语义（我们想要的）。</li> <li>$g(E_{pos})$: 数据无关的偏差（即固定模式噪声）。 <alphaxiv-paper-citation title="Noise Model" page="3" first="𝑔 being a" last="data-invariant bias,"></alphaxiv-paper-citation></li> </ul> </li> <li><strong>观察到的伪影 (Figure 2)</strong>: <ul> <li>SigLIP2: 边界处的“黑洞”。</li> <li>SAM: ViTDet 窗口边界的伪影。</li> <li>DINOv3: 大幅度的噪声斑块。 <alphaxiv-paper-citation title="Artifact Types" page="3" first="SigLIP2 models," last="noise patches."></alphaxiv-paper-citation></li> </ul> </li> </ul> <h2 id="8-解决方案-i---空间平移等变损失-spatial-shift-equivariant-loss">8: 解决方案 I - 空间平移等变损失 (Spatial Shift Equivariant Loss)</h2> <ul> <li><strong>思想</strong>: 如果学生模型不知道教师特征的确切位置，它就无法学习位置相关的噪声 $g(E_{pos})$。</li> <li><strong>操作</strong>: <ol> <li>对学生及每个教师的输入 Crop 进行<strong>独立的随机平移</strong>。</li> <li>平移量为 Patch size 的整数倍（避免插值效应）。 <alphaxiv-paper-citation title="Shift Strategy" page="3" first="increments of the" last="interpolation effects"></alphaxiv-paper-citation></li> </ol> </li> <li><strong>映射函数 $\mathcal{F}_{S \to T}$</strong>: 将学生特征空间对齐到教师特征空间。</li> <li><strong>损失公式</strong>: \(L_{spatial}(x, \hat{y}) = \frac{1}{|\Omega|} \sum_{u \in \Omega} (\mathcal{F}_{S \to T}[x]_u - \hat{y}_u)^2\) <ul> <li>$\hat{y}$: 经过 PHI-S 归一化的教师输出。 <alphaxiv-paper-citation title="Loss Eq 1" page="3" first="𝐿spatial (x, ˆy)" last="normalized teacher output."></alphaxiv-paper-citation></li> </ul> </li> </ul> <h2 id="9-解决方案-ii---shift-equivariant-mesa">9: 解决方案 II - Shift Equivariant MESA</h2> <ul> <li><strong>MESA 背景</strong>: MESA (Model Exponential Moving Average) 用于寻找损失平坦区域，提高泛化性。</li> <li><strong>v4 的创新</strong>: 在 MESA 中引入空间平移。</li> <li><strong>机制</strong>: <ul> <li>学生模型 $S$ 和其 EMA 版本 $\tilde{S}$ 看到不同的 Crop。</li> <li>通过变换 $\mathcal{F}_{S \to \tilde{S}}$ 对齐特征。</li> </ul> </li> <li><strong>公式</strong>: \(L_{mesa}(x, \tilde{x}) = \frac{1}{|\Omega|} \sum_{u \in \Omega} (\mathcal{F}_{S \to \tilde{S}}[LN(x)]_u - LN(\tilde{x})_u)^2\) <ul> <li>注意：这里使用了不带仿射变换的 LayerNorm ($LN$)。 <alphaxiv-paper-citation title="Loss Eq 2" page="4" first="𝐿𝑚𝑒𝑠𝑎 (x, ˜x)" last="affine projection,"></alphaxiv-paper-citation></li> </ul> </li> </ul> <h2 id="10-辅助增强技术---damp">10: 辅助增强技术 - DAMP</h2> <ul> <li><strong>DAMP</strong>: Distillation with Annealed Multiplicative Perturbation.</li> <li><strong>操作</strong>: 训练时对权重施加乘性噪声。</li> <li><strong>作用</strong>: 进一步破坏模型对特定参数配置的过拟合，增强鲁棒性。 <alphaxiv-paper-citation title="DAMP" page="4" first="applies multiplicative noise" last="during training."></alphaxiv-paper-citation></li> </ul> <hr/> <h1 id="模组三高级优化与实验分析-第三小时">模组三：高级优化与实验分析 (第三小时)</h1> <p><strong>目标</strong>：理解高维特征空间中的几何问题（锥体效应），分析实验结果，并讨论聚合模型的未来。</p> <h2 id="11-几何视角---平衡摘要损失-balanced-summary-loss">11: 几何视角 - 平衡摘要损失 (Balanced Summary Loss)</h2> <ul> <li><strong>被忽视的问题</strong>: 在 PHI-S 中，我们只归一化了空间特征，忽略了 Summary Token (如 CLS token)。</li> <li><strong>错误假设</strong>: “Cosine Similarity 自带归一化，所以不需要处理。”</li> <li><strong>真实几何结构</strong>: <ul> <li>特征并非均匀分布在单位超球面上。</li> <li>特征倾向于聚集成一个<strong>圆锥体 (Cone)</strong>。 <alphaxiv-paper-citation title="Cone Geometry" page="4" first="features tend to" last="into a cone,"></alphaxiv-paper-citation></li> </ul> </li> <li><strong>不平衡来源</strong>: <ul> <li>不同教师模型的特征锥体<strong>半径 (Radius)</strong> 不同。</li> <li>半径大的锥体（方差大）产生的 Loss 会主导优化过程，导致模型过度关注某些教师而忽略其他教师。 <alphaxiv-paper-citation title="Radius Effect" page="4" first="radius of this" last="each teacher."></alphaxiv-paper-citation></li> </ul> </li> <li><strong>改进</strong>: 必须对摘要特征的方向方差进行平衡。</li> </ul> <h2 id="12-vitdet-模式回归---效率的关键">12: ViTDet 模式回归 - 效率的关键</h2> <ul> <li><strong>背景</strong>: 标准 ViT 的注意力机制是 $O(N^2)$，在高分辨率下不可接受。</li> <li><strong>ViTDet 模式</strong>: <ul> <li>允许 Transformer Block 在 <strong>Windowed Mode (窗口模式)</strong> 下运行。</li> <li>仅保留极少量的全局注意力块（Global Blocks）。</li> </ul> </li> <li><strong>效果</strong>: 见图 9 (需口述引用)，在高分辨率推理速度上有巨大提升。 <alphaxiv-paper-citation title="ViTDet Speed" page="1" first="dramatic effect on" last="inference speed"></alphaxiv-paper-citation></li> </ul> <h2 id="13-实验结果分析---分割任务">13: 实验结果分析 - 分割任务</h2> <ul> <li><strong>ADE20k 线性探测 (Table 2)</strong>: <ul> <li>对比 DINOv3-7B vs. C-RADIOv4-H。</li> <li><strong>关键结论</strong>: C-RADIOv4 展现了极强的分辨率缩放特性。即使在未训练的更高分辨率 (1536px) 下，性能依然稳健。 <alphaxiv-paper-citation title="Scaling" page="3" first="exhibit strong resolution" last="scaling properties."></alphaxiv-paper-citation></li> </ul> </li> <li><strong>SA-Co/Gold 实例分割 (Table 5)</strong>: <ul> <li><strong>Baseline</strong>: SAM3 (47.3 cgF1).</li> <li><strong>C-RADIOv4-H</strong>: 45.9 cgF1 (Global attention)。</li> <li><strong>分析</strong>: 虽然略低于 SAM3，但考虑到参数量级差异（SAM3 巨大，RADIO 仅 600M+），且 RADIO 是通用骨干，这个结果非常有竞争力。 <alphaxiv-paper-citation title="Table 5" page="7" first="Results on SA-Co/Gold" last="global attention throughout."></alphaxiv-paper-citation></li> </ul> </li> </ul> <h2 id="14-可视化对比与定性分析">14: 可视化对比与定性分析</h2> <ul> <li><strong>图 2 解析 (Figure 2)</strong>: <ul> <li>展示了 DINOv3 输出中的“斑点”噪声。</li> <li>展示了 C-RADIOv4 如何通过上述的 Shift Equivariance Loss 成功“平滑”了这些噪声，生成更干净的特征图。 <alphaxiv-paper-citation title="Fig 2 Analysis" page="4" first="Visualization of DINOv3" last="DINOv3 teacher."></alphaxiv-paper-citation></li> </ul> </li> <li><strong>图 6 解析 (Figure 6)</strong>: <ul> <li>展示将 SAM3 的 Vision Encoder 替换为 RADIO 后的效果。</li> <li><strong>结论</strong>: RADIO 能够完美复现 SAM3 的分割能力。 <alphaxiv-paper-citation title="Fig 6 Analysis" page="7" first="RADIO is able" last="SAM3 results."></alphaxiv-paper-citation></li> </ul> </li> </ul> <h2 id="15-总结与讨论">15: 总结与讨论</h2> <ul> <li><strong>核心技术回顾</strong>: <ol> <li><strong>Teacher Updates</strong>: SigLIP2 + DINOv3 + SAM3.</li> <li><strong>Stochastic Resolutions</strong>: 任意分辨率适应性。</li> <li><strong>Shift Equivariance</strong>: 消除固定模式噪声的关键数学工具。</li> <li><strong>Balanced Loss</strong>: 解决多教师特征空间的几何不平衡。</li> </ol> </li> <li><strong>思考</strong>: <ul> <li><em>Q1</em>: 如果我们引入一个视频模型作为第4个教师，时间维度的 Shift Equivariance 应该如何设计？</li> <li><em>Q2</em>: 聚合模型是否是通向 AGI 视觉系统的必经之路，还是仅仅是算力受限下的妥协？</li> </ul> </li> </ul> <hr/> <h2 id="论文中公式3-7的理解">论文中公式(3)-(7)的理解</h2> <h3 id="balanced-summary-loss-公式推导-eq-3---7">Balanced Summary Loss 公式推导 (Eq 3 - 7)</h3> <p><strong>核心目的</strong>：不同教师模型的特征分布“圆锥体”大小不同，导致 Loss 贡献不平衡。我们需要一种新的度量方式，让所有老师的 Loss 在同一个量级上。</p> <h4 id="公式-3-余弦相似度-cosine-similarity">公式 (3): 余弦相似度 (Cosine Similarity)</h4> <p>\(\cos(x, y) = \frac{x^\top y}{\|x\|\|y\|}\)</p> <ul> <li><strong>定义</strong>: 计算学生预测向量 $x$ 和教师目标向量 $y$ 之间的夹角余弦值。</li> <li><strong>背景</strong>: 这是深度学习中最常用的相似度度量，值域为 $[-1, 1]$。</li> <li><strong>局限</strong>: 它只关心方向，忽略了模长。但在 C-RADIOv4 的上下文中，我们发现仅仅归一化模长是不够的，还需要关注方向的分布方差。 <alphaxiv-paper-citation title="Eq 3" page="5" first="cos(x, y) =" last="‖x‖‖y‖"></alphaxiv-paper-citation></li> </ul> <h4 id="公式-4-角度距离-angular-distance">公式 (4): 角度距离 (Angular Distance)</h4> <p>\(\Theta(x, y) = \arccos(\cos(x, y))\)</p> <ul> <li><strong>定义</strong>: 将余弦相似度转换为实际的<strong>弧度角 (Radians)</strong>。</li> <li><strong>物理意义</strong>: 这是两个向量在单位超球面上最短路径的弧长。</li> <li><strong>为什么转换</strong>: 余弦函数是非线性的，而弧度角是线性的，更能真实反映几何上的偏离程度，方便后续计算方差。 <alphaxiv-paper-citation title="Eq 4" page="5" first="Θ(x, y) =" last="(cos(x, y))"></alphaxiv-paper-citation></li> </ul> <h4 id="公式-5-教师特征的平均方向-mean-direction">公式 (5): 教师特征的平均方向 (Mean Direction)</h4> <p>\(\mu_y = \frac{E[y]}{\|E[y]\|}\)</p> <ul> <li><strong>定义</strong>: 计算某个教师模型所有输出特征 $y$ 的期望向量，并将其归一化。</li> <li><strong>直观理解</strong>: 也就是那个“圆锥体”的<strong>中轴线</strong>。它代表了这个教师模型在特征空间中的“平均姿态”。 <alphaxiv-paper-citation title="Eq 5" page="5" first="𝜇y =" last="‖E[y]‖"></alphaxiv-paper-citation></li> </ul> <h4 id="公式-6-角散度-angular-dispersion---关键定义">公式 (6): 角散度 (Angular Dispersion) - <strong>关键定义</strong></h4> <p>\(Disp(\Theta_y) = E \left[ \Theta(y, \mu_y)^2 \right]\)</p> <ul> <li><strong>定义</strong>: 这是一个统计量。它计算的是：该教师产生的任意一个特征 $y$，与该教师的平均方向 $\mu_y$ 之间夹角的平方期望（即方差）。</li> <li><strong>物理意义</strong>: <ul> <li>它量化了<strong>特征分布圆锥体的“开口大小”</strong>（或者是超球面上的覆盖面积）。</li> <li>$Disp$ 值大 $\rightarrow$ 老师输出很发散（如 DINOv3）。</li> <li>$Disp$ 值小 $\rightarrow$ 老师输出很集中（如 SigLIP2）。 <alphaxiv-paper-citation title="Eq 6" page="5" first="Disp(Θy) =" last="(y, 𝜇y)2"></alphaxiv-paper-citation></li> </ul> </li> </ul> <h4 id="公式-7-平衡摘要损失-balanced-summary-loss---最终解决方案">公式 (7): 平衡摘要损失 (Balanced Summary Loss) - <strong>最终解决方案</strong></h4> <p>\(L_{angle}(x, y) = \frac{\Theta(x, y)^2}{Disp(\Theta_y)}\)</p> <ul> <li><strong>定义</strong>: 最终使用的 Loss 函数。</li> <li><strong>机制解析</strong>: <ul> <li>分子 $\Theta(x, y)^2$: 学生与老师之间的预测误差（角度平方）。</li> <li>分母 $Disp(\Theta_y)$: 用该老师自身的散度进行<strong>归一化</strong>。</li> </ul> </li> <li><strong>效果</strong>: <ul> <li>对于 <strong>SigLIP2</strong> (散度小，分母小): 即使分子（误差）很小，除以一个小分母后，Loss 也会变大，迫使学生重视它。</li> <li>对于 <strong>DINOv3</strong> (散度大，分母大): 即使分子（误差）很大，除以一个大分母后，Loss 也会变小，防止它主导训练。</li> </ul> </li> <li><strong>一句话总结</strong>: 这本质上是一个 <strong>Z-score 标准化</strong>的变体（$\frac{x-\mu}{\sigma^2}$），让不同分布特性的老师在 Loss 计算时能够“平起平坐”。 <alphaxiv-paper-citation title="Eq 7" page="5" first="𝐿angle (x, y) =" last="Disp(Θy)"></alphaxiv-paper-citation></li> </ul> <p>这组公式，最重要的逻辑链条是：</p> <ol> <li><strong>现象</strong>: 老师们不仅“方言”不同（特征空间不同），而且“音量”不同（散度不同）。</li> <li><strong>问题</strong>: 传统的 Cosine Loss 无法处理“音量”差异，导致大嗓门老师（DINOv3）淹没小嗓门老师（SigLIP2）。</li> <li><strong>对策</strong>: 我们先算出每个老师的“平均音量”($Disp$, Eq 6)，然后在计算 Loss 时除以这个音量 (Eq 7)，从而实现<strong>自适应的音量平衡</strong>。</li> </ol> <h2 id="table-3-说明了什么">Table 3 说明了什么</h2> <h3 id="table-3-深度解析---隐藏在几何空间中的不公平">Table 3 深度解析 - 隐藏在几何空间中的“不公平”</h3> <h4 id="1-table-3-在描述什么">1. Table 3 在描述什么？</h4> <p>Table 3 展示了不同教师模型（SigLIP2 和 DINOv3）的<strong>摘要 Token（Summary Token，如 CLS token）的角散度（Angular Dispersion）</strong>。</p> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: left">Disp($\Theta_y$)</th> <th style="text-align: left">直观理解</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>SigLIP2-g-384</strong></td> <td style="text-align: left"><strong>0.694</strong></td> <td style="text-align: left"><strong>窄圆锥</strong>：特征向量聚集在一个很小的角度范围内，方向非常集中。</td> </tr> <tr> <td style="text-align: left"><strong>DINOv3-7B</strong></td> <td style="text-align: left"><strong>2.186</strong></td> <td style="text-align: left"><strong>宽圆锥</strong>：特征向量在空间中分布得很散，方向变化很大。</td> </tr> </tbody> </table> <alphaxiv-paper-citation title="Table 3 Data" page="5" first="SigLIP2-g-384 0.694" last="DINOv3-7B 2.186"/> <h4 id="2-什么是角散度-angular-dispersion">2. 什么是“角散度 (Angular Dispersion)”？</h4> <p>用 <strong>“手电筒光束”</strong> 做类比：</p> <ul> <li> <p><strong>定义</strong>: 它衡量的是特征向量 $y$ 偏离其平均方向 $\mu_y$ 的程度。 \(Disp(\Theta_y) = E[\Theta(y, \mu_y)^2]\) 简单说，就是特征分布构成的<strong>圆锥体（Cone）的开口大小</strong>。 <alphaxiv-paper-citation title="Dispersion Def" page="5" first="Disp(Θy) =" last="(6)"></alphaxiv-paper-citation></p> </li> <li> <p><strong>直观物理意义</strong>:</p> <ul> <li><strong>SigLIP2 (0.694)</strong>: 像激光笔。所有图像的特征都指向差不多的方向，变化很小。</li> <li><strong>DINOv3 (2.186)</strong>: 像泛光灯。不同图像的特征方向差异巨大。</li> </ul> </li> </ul> <h4 id="3-table-3-揭示了什么危机核心考点">3. Table 3 揭示了什么危机？（核心考点）</h4> <p>这就是为什么之前的聚合方法（如 RADIOv2.5）在融合这两个特定老师时会失败的原因。</p> <p><strong>如果不做处理（直接用余弦距离 Loss）：</strong> \(L = 1 - \cos(x, y)\)</p> <ul> <li><strong>DINOv3 (大散度)</strong>: 因为它的特征天生就分得很开，学生模型只要稍微预测偏一点，产生的 <strong>角度误差（Angle Error）</strong> 就会非常大。这导致 DINOv3 产生的 <strong>Loss 值很大</strong>，梯度也很大。</li> <li><strong>SigLIP2 (小散度)</strong>: 因为它的特征都挤在一起，即使学生预测错了，角度误差通常也很小。这导致 SigLIP2 产生的 <strong>Loss 值很小</strong>。</li> </ul> <p><strong>结果</strong>: <strong>DINOv3 变成了“嗓门最大”的老师，SigLIP2 变成了“窃窃私语”的老师。</strong> 学生模型会为了降低总 Loss，拼命讨好 DINOv3，而完全忽略 SigLIP2 的教导。这就是文中提到的：“DINOv3 would dominate the loss term… at the expense of matching SigLIP2.” <alphaxiv-paper-citation title="Domination" page="5" first="DINOv3 would dominate" last="matching SigLIP2."></alphaxiv-paper-citation></p> <h4 id="4-table-3-的解决方案导向">4. Table 3 的解决方案导向</h4> <p>Table 3 的数据直接证明了引入 <strong>公式 (7) 平衡摘要损失 (Balanced Summary Loss)</strong> 的必要性。</p> \[L_{angle}(x, y) = \frac{\Theta(x, y)^2}{Disp(\Theta_y)}\] <ul> <li>我们用 Table 3 中的这个数值 ($Disp$) 作为分母。</li> <li><strong>SigLIP2</strong>: 分母小 (0.694) $\rightarrow$ 放大 Loss权重。</li> <li><strong>DINOv3</strong>: 分母大 (2.186) $\rightarrow$ 缩小 Loss权重。</li> </ul> <p>Table 3 告诉我们，不同模型的特征空间几何形状差异巨大。<strong>聚合模型不仅仅是把 Loss 加起来那么简单，如果不根据 Table 3 的散度值进行归一化，学生模型将永远学不会那个“低散度”老师的知识。</strong></p> <h2 id="为什么-dinov3-相比-siglip2-的-cone-会大呢-或者说-散度大">为什么 Dinov3 相比 SigLIP2 的 cone 会大呢, 或者说 散度大?</h2> <h3 id="1-核心原因文本的束缚-vs-视觉的自由">1. 核心原因：文本的“束缚” vs. 视觉的“自由”</h3> <ul> <li><strong>SigLIP2 (Text-Aligned)</strong>: <ul> <li><strong>训练机制</strong>: 它的视觉特征必须与<strong>文本特征</strong>对齐。</li> <li><strong>关键点</strong>: 文本空间（Text Embedding Space）是相对稀疏和离散的。语言是高度压缩的符号系统（比如“狗”这个词，涵盖了千万种不同样子的狗）。</li> <li><strong>几何后果</strong>: 为了匹配特定的文本向量，SigLIP2 被迫将各种视觉上差异巨大的图片（哈士奇、柯基、侧面、正面）压缩到同一个非常狭窄的特征区域。</li> <li><strong>结论</strong>: 文本像一个<strong>强力磁铁</strong>，把视觉特征强行聚拢，导致特征分布非常集中（<strong>低散度</strong>）。</li> </ul> </li> <li><strong>DINOv3 (Self-Supervised Learning)</strong>: <ul> <li><strong>训练机制</strong>: 它不依赖文本，而是通过掩码重建或实例判别来学习。它追求的是最大化提取视觉信息。</li> <li><strong>关键点</strong>: <strong>一致性与均匀性 (Alignment and Uniformity)</strong> 是 SSL 的两个黄金标准。特别是<strong>均匀性 (Uniformity)</strong>，要求特征向量尽可能均匀地分布在单位超球面上，以便区分不同的视觉实例。</li> <li><strong>几何后果</strong>: DINOv3 倾向于把特征“撑开”，占满整个特征空间，以便保留更多的纹理、光照、几何细节。</li> <li><strong>结论</strong>: 为了保留最大的信息熵，DINOv3 的特征分布必须尽可能发散（<strong>高散度</strong>）。</li> </ul> </li> </ul> <h3 id="2-语义粒度抽象类-vs-具体实例">2. 语义粒度：抽象类 vs. 具体实例</h3> <ul> <li><strong>SigLIP2 (类别级/语义级)</strong>: <ul> <li>它关注的是<strong>“What is this?”</strong>（这是一只狗）。</li> <li>它会主动<strong>丢弃</strong>不必要的视觉方差（比如背景颜色、狗的朝向），因为这些对文本匹配没用。</li> <li>方差被丢弃 $\rightarrow$ 分布变窄。</li> </ul> </li> <li><strong>DINOv3 (实例级/像素级)</strong>: <ul> <li>它关注的是<strong>“How does this look?”</strong>（这只狗长什么样，边缘在哪里）。</li> <li>它必须<strong>保留</strong>视觉方差。两张不同角度的“狗”的照片，在 DINO 的空间里应该是有区分度的，否则无法做分割等密集任务。</li> <li>方差被保留 $\rightarrow$ 分布变宽。</li> </ul> </li> </ul> <h3 id="3-损失函数的数学导向">3. 损失函数的数学导向</h3> <ul> <li><strong>SigLIP (Sigmoid Loss for Language-Image Pre-training)</strong>: <ul> <li>SigLIP 的损失函数专注于成对的二分类（匹配/不匹配）。只要正样本的得分为正，负样本得分为负即可。它并不强制要求特征填满整个空间，往往会导致特征坍缩到某些特定的方向上（Cone Effect）。</li> </ul> </li> <li><strong>DINO (Centering + Sharpening + Sinkhorn-Knopp)</strong>: <ul> <li>DINO 系列通常使用聚类（Clustering）或特征中心化（Centering）机制来防止模型坍缩（Collapse）。</li> <li>为了防止所有输出都变成同一个常数，算法会强制特征在不同的 Cluster 之间跳跃。这在数学上直接推高了特征的方差 ($Disp(\Theta_y)$)。</li> </ul> </li> </ul> <h3 id="总结-可直接用于-ppt">总结 (可直接用于 PPT)</h3> <table> <thead> <tr> <th style="text-align: left">维度</th> <th style="text-align: left">SigLIP2 (CLIP-style)</th> <th style="text-align: left">DINOv3 (SSL-style)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>监管信号</strong></td> <td style="text-align: left">文本 (Text)</td> <td style="text-align: left">图像自身 (Pixels)</td> </tr> <tr> <td style="text-align: left"><strong>空间约束</strong></td> <td style="text-align: left"><strong>受限</strong>: 必须压缩以适应低维的文本流形</td> <td style="text-align: left"><strong>自由</strong>: 追求最大化利用超球面空间</td> </tr> <tr> <td style="text-align: left"><strong>对待差异</strong></td> <td style="text-align: left"><strong>消除</strong>: 忽略非语义的视觉差异</td> <td style="text-align: left"><strong>保留</strong>: 编码细粒度的视觉结构</td> </tr> <tr> <td style="text-align: left"><strong>几何形状</strong></td> <td style="text-align: left"><strong>窄圆锥 (Low Dispersion)</strong></td> <td style="text-align: left"><strong>宽圆锥 (High Dispersion)</strong></td> </tr> <tr> <td style="text-align: left"><strong>讲课比喻</strong></td> <td style="text-align: left">像是把所有人都塞进几个特定的“房间”（类别）里。</td> <td style="text-align: left">像是把人在广场上尽量“散开”站（为了看清每个人的脸）。</td> </tr> </tbody> </table> <p><strong>结论</strong>: Table 3 中的数据 (0.694 vs 2.186) 实际上量化了<strong>“文本对齐”</strong>与<strong>“视觉自监督”</strong>这两种范式在特征空间拓扑结构上的根本差异。</p> <h1 id="radio-编年史">RADIO 编年史</h1> <p>我们从 <strong>“宏观直觉”</strong>和** “解决的痛点”<strong>两个维度来理解这一系列工作。 简单来说，</strong>RADIO 系列是在做一个“集大成者”的视觉骨干网络（Vision Backbone）。** 它的核心理念可以用一句话概括：<strong>与其在生产环境中运行三个巨大的专用模型，不如训练一个小巧的学生模型，同时学会这三个老师的本事。</strong></p> <p>以下是为您梳理的通俗版“RADIO 编年史”与核心逻辑：</p> <h3 id="1-痛点ai-界的专业分工太严重">1. 痛点：AI 界的“专业分工”太严重</h3> <p>在计算机视觉（CV）领域，我们有几类“顶流”的基础模型，但它们各有所长，互不兼容：</p> <ul> <li><strong>语言-图像专家 (CLIP/SigLIP)</strong>：擅长理解图片和文字的关系（比如“这张图里有一只猫”），但在细节定位上很弱。</li> <li><strong>特征/感知专家 (DINO)</strong>：擅长理解图片的几何结构、纹理和密集特征，但在理解抽象语义上不如 CLIP。</li> <li><strong>分割专家 (SAM)</strong>：擅长把物体从背景里抠出来（分割），但它可能不知道抠出来的东西叫什么。</li> </ul> <p><strong>问题来了</strong>：如果你想做一个机器人，既要它看懂指令（CLIP），又要它避障（DINO），还要它抓取物体（SAM），你得同时运行这三个巨大的模型。这对显存和计算资源是巨大的浪费。</p> <h3 id="2-核心方案聚合agglomeration">2. 核心方案：聚合（Agglomeration）</h3> <p>RADIO（<strong>R</strong>obust <strong>A</strong>gglomerative <strong>Di</strong>stillation… 的缩写概念）提出了一种<strong>“多教师蒸馏”</strong>（Multi-Teacher Distillation）的范式。</p> <ul> <li><strong>场景</strong>：把 CLIP、DINO、SAM 请到同一个教室当“老师”。</li> <li><strong>主角</strong>：一个单一的 Vision Transformer (ViT) 模型（即 RADIO）作为“学生”。</li> <li><strong>过程</strong>：给一张图，老师们分别提取特征。学生模型被迫调整自己的参数，使得它提取出的特征，<strong>既像 CLIP 一样懂语义，又像 DINO 一样懂结构，还能像 SAM 一样懂分割。</strong></li> </ul> <p><strong>神奇的结论</strong>：实验证明，单一的神经网络完全有能力在一个特征空间内同时编码这些不同维度的信息。(牛逼,我也认为这极有可能是机器人的一条不错的出路.)</p> <h3 id="3-这个系列的发展脉络">3. 这个系列的发展脉络</h3> <ul> <li><strong>第一代 (AM-RADIO)</strong>： <ul> <li><strong>验证概念</strong>：证明了“把大家聚合在一起”是可行的。</li> <li><strong>发现问题</strong>：发现学生模型很“鸡贼”，它会根据图片的分辨率来猜测该模仿哪个老师（比如低分辨率时模仿 CLIP，高分辨率模仿 SAM），导致推理时很不稳定（Mode Switching）。(哈哈, 聪明的学生)</li> </ul> </li> <li><strong>第二代 (RADIOv2.5)</strong>： <ul> <li><strong>修复 Bug</strong>：强制学生在所有分辨率下都要同时像所有老师学习。</li> <li><strong>引入 PHI-S</strong>：发现老师们“嗓门”不一样大（特征分布不同），于是引入数学方法（PHI-S）让老师们的特征分布归一化，让学生能听清每个老师的话。</li> </ul> </li> <li><strong>最新一代 (C-RADIOv4 - 您手里这篇)</strong>： <ul> <li><strong>最强名师阵容</strong>：老师换成了最新的 SigLIP2、DINOv3 和 SAM3。<alphaxiv-paper-citation title="New Teachers" page="1" first="the core set" last="SigLIP2, DINOv3, SAM3"></alphaxiv-paper-citation></li> <li><strong>去噪与提纯</strong>：发现老师模型本身有很多“噪音”（伪影），学生如果死记硬背会把噪音也学去。v4 引入了非常精细的数学工具（Shift Equivariance Loss），让学生只学老师的“神”（语义/结构），不学老师的“形”（固定噪声）。</li> <li><strong>全能性</strong>：支持任意分辨率输入，且速度更快。</li> </ul> </li> </ul> <h3 id="4-总结为什么它重要">4. 总结：为什么它重要？</h3> <p>理解 RADIO 的意义在于明白 <strong>“模型压缩”不仅仅是把大变小，还可以是“多变一”</strong>。 C-RADIOv4 提供了一个<strong>统一的、高性能的视觉底座</strong>。以后做下游任务（无论是检测、分割还是分类），不需要纠结选哪个预训练模型，直接用 RADIO，因为它通过蒸馏，已经拥有了当前最强几个模型的“内功”。 可以把这一系列论文看作是 <strong>“如何优雅地从多个异构模型中榨取知识，并压缩进一个标准模型”</strong> 的工程与算法指南。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[[TOC]]]></summary></entry><entry><title type="html">VLM4VLA</title><link href="https://beyondpzk.github.io/blog/2026/VLM4VLA/" rel="alternate" type="text/html" title="VLM4VLA"/><published>2026-01-06T00:00:00+00:00</published><updated>2026-01-06T00:00:00+00:00</updated><id>https://beyondpzk.github.io/blog/2026/VLM4VLA</id><content type="html" xml:base="https://beyondpzk.github.io/blog/2026/VLM4VLA/"><![CDATA[<p>[TOC]</p> <h1 id="vlm4vla">VLM4VLA</h1> <h2 id="论文地址"><a href="https://arxiv.org/abs/2601.03309">论文地址</a></h2> <h1 id="vlm4vla--探究具身智能中的视觉-语言基座效应">VLM4VLA —— 探究具身智能中的视觉-语言基座效应</h1> <p><strong>参考论文：</strong> <em>VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models</em> <strong>目标：</strong></p> <ol> <li><strong>解构</strong> VLA (Vision-Language-Action) 模型的标准范式与不同变体。</li> <li><strong>剖析</strong> VLM4VLA 的实验控制变量法设计及其背后的科学严谨性。</li> <li><strong>批判性思考</strong> “通用智能”与“具身控制”之间的特征表示差异（Feature Representation Gap）。</li> <li><strong>掌握</strong> 评估 VLA 模型性能的核心指标与基准测试方法。</li> </ol> <hr/> <h2 id="第一部分从大模型到具身智能的演进">第一部分：从大模型到具身智能的演进</h2> <h3 id="1-理论背景vlm-的解剖">1. 理论背景：VLM 的解剖</h3> <ul> <li><strong>VLM 的通用架构</strong> <ul> <li> \[\text{Input} = \{ \text{Image}, \text{Text Instructions} \}\] </li> <li> \[\text{Architecture} = \text{Vision Encoder} (e.g., \text{SigLIP, ViT}) + \text{Projector} (e.g., \text{MLP, Q-Former}) + \text{LLM Backbone}\] </li> </ul> </li> <li><strong>现状回顾：</strong> 目前开源社区有大量的 VLM（如 Qwen-VL, Paligemma, LLaVA）。它们在 VQA（视觉问答）、Captioning（图像描述）上表现出色。</li> <li><strong>核心假设：</strong> 既然 VLM 懂物理世界的语义（比如知道“杯子”是用来“喝水”的，知道“把手”在哪里），那么能否直接用它来控制机器人？</li> <li><strong>VLA 的定义：</strong> 将“动作 (Action)”视为一种特殊的“模态”或“语言”。 <ul> <li>早期尝试：RT-2 (Google) —— 将动作离散化为 Token (e.g., “128”, “255”)，直接用 LLM 自回归生成。</li> </ul> </li> </ul> <h3 id="2-本文的研究动机乱象中的反思">2. 本文的研究动机：乱象中的反思</h3> <ul> <li><strong>当前 VLA 领域的乱象：</strong> <ul> <li>每篇新论文都提出一个新的架构（Diffusion Policy, ACT, Flow Matching）。</li> <li>每篇论文都换一个 VLM 基座（Llama, Qwen, Vicuna）。</li> <li><strong>痛点：</strong> 当一个新模型 SOTA 时，我们不知道是因为架构好了？还是因为基座 VLM 变强了？还是数据清洗得更干净了？</li> </ul> </li> <li><strong>VLM4VLA 的核心定位：</strong> <ul> <li>它不是为了刷榜（SOTA），而是为了建立一个<strong>受控实验台 (Controlled Testbed)</strong>。</li> <li><strong>Research Question (RQ):</strong> VLM 的选择和能力，如何转化为下游 VLA 的策略性能？<alphaxiv-paper-citation title="Core Question" page="1" first="how VLM choice" last="policies performance?"></alphaxiv-paper-citation></li> </ul> </li> </ul> <h3 id="3-vla-的系统-1与系统-2之争">3. VLA 的“系统 1”与“系统 2”之争</h3> <ul> <li><strong>讨论：</strong> 机器人控制的层级。 <ul> <li><em>System 2 (高层规划):</em> “去厨房煮咖啡” -&gt; 需要 VLM 的推理能力。</li> <li><em>System 1 (底层控制):</em> “关节转动 0.5 弧度，手爪闭合” -&gt; 需要高频、精确的几何感知。</li> </ul> </li> <li><strong>本文的关注点：</strong> 本文关注的是 End-to-End 的策略学习，即 VLM 是否能胜任 <em>System 1</em> 的角色？这挑战了 VLM 原本的预训练目标（语义对齐）。</li> </ul> <hr/> <h2 id="第二部分方法论与实验架构详解">第二部分：方法论与实验架构详解</h2> <h3 id="1-vlm4vla-管道设计极简主义的胜利-20分钟">1. VLM4VLA 管道设计：极简主义的胜利 (20分钟)</h3> <ul> <li><strong>架构概览 (结合 Figure 2 讲解)：</strong> <ul> <li>为了公平比较，必须剔除所有花哨的技巧（Tricks）。</li> <li><strong>输入序列设计：</strong> \(\text{Sequence} = [ \langle \text{img} \rangle ... \langle \text{img} \rangle, \langle \text{text} \rangle ... \langle \text{text} \rangle, \langle \text{ActionQuery} \rangle ]\)</li> <li><strong>关键组件：ActionQuery Token</strong> <ul> <li>这是一个可学习的 Token。它的作用是从 VLM 的深层特征中“汇聚”出与动作相关的信息。</li> </ul> </li> <li><strong>解码头 (Policy Head)：</strong> <ul> <li>作者仅仅使用了一个 <strong>MLP</strong>。</li> <li><strong>深度提问：</strong> 为什么不用现在流行的 Diffusion Head？</li> <li><strong>答案：</strong> 为了<strong>减少随机性</strong>。Diffusion 引入了采样随机性，这会增加评估方差，干扰对 VLM 基座能力的判断。作者需要一个确定性的比较环境。<alphaxiv-paper-citation title="Why MLP" page="5" first="We use a" last="flow-matching) approach,"></alphaxiv-paper-citation></li> </ul> </li> </ul> </li> </ul> <h3 id="2-损失函数与训练目标-15分钟">2. 损失函数与训练目标 (15分钟)</h3> <ul> <li><strong>公式 (Equation 1)：</strong> \(\mathcal{L} = \frac{1}{|B|} \sum_{B} \left( \| a_{pos} - \hat{a}_{pos} \|^2_2 + \text{BCE}(a_{end}, \hat{a}_{end}) \right)\) <ul> <li><strong>第一项：</strong> 修正的 MSE Loss (Huber Loss)，用于回归连续的关节位置/末端执行器位姿 ($a_{pos}$)。</li> <li><strong>第二项：</strong> BCE Loss，用于二分类（比如夹爪的开/合状态 $a_{end}$）。</li> </ul> </li> <li><strong>全参数微调 (Full Fine-tuning)：</strong> <ul> <li>作者微调了 VLM 的<strong>所有参数</strong>（包括 Vision Encoder, LLM, Word Embeddings）。</li> <li>这非常关键，因为如果是 LoRA 或 Freeze，可能会掩盖基座模型的真实潜力。</li> </ul> </li> </ul> <h3 id="3-实验设置的严谨性">3. 实验设置的严谨性</h3> <ul> <li><strong>数据处理：</strong> 所有图像统一 Resize 到 $224 \times 224$。</li> <li><strong>输入限制：</strong> 只用单视角图像，不使用本体感知 (Proprioception)。这是为了强迫模型必须依赖视觉理解，防止模型“作弊”（通过本体感知死记硬背动作）。</li> <li><strong>基准测试 (Benchmarks) 的选择逻辑：</strong> <ul> <li><strong>Calvin:</strong> 测试<strong>长程序列</strong> (Long-horizon)，看模型能不能连续做对5件事。</li> <li><strong>SimplerEnv (Google):</strong> 测试<strong>泛化性</strong>，在模拟器中测试真实世界的分布偏移 (Sim-to-Real-to-Sim)。</li> <li><strong>Libero:</strong> 测试<strong>任务多样性</strong>。</li> </ul> </li> </ul> <h2 id="第三部分核心实验结果与反直觉发现">第三部分：核心实验结果与反直觉发现</h2> <h3 id="1-vlm-基座大比拼">1. VLM 基座大比拼</h3> <ul> <li><strong>参赛选手：</strong> <ul> <li>Qwen2.5-VL (3B/7B), Qwen3-VL</li> <li>Paligemma (Google, 专门为迁移学习设计)</li> <li>Kosmos-2 (Microsoft, 擅长 Grounding)</li> <li>OpenVLA, Pi0 (作为 SOTA 基线)</li> </ul> </li> <li><strong>核心图表解读 (Figure 3)：相关性分析</strong> <ul> <li><strong>现象：</strong> 作者画了一张散点图，横轴是 VLM 在通用任务（如 MMBench, Math, Coding）上的得分，纵轴是 VLA 的成功率。</li> <li><strong>结论：</strong> <strong>弱相关甚至无相关。</strong> <ul> <li>例如：Kosmos 在某些任务上打败了参数量更大、通用评分更高的 Qwen 和 Paligemma。</li> <li><strong>关键引用：</strong> <alphaxiv-paper-citation title="Prediction Failure" page="1" first="VLM’s general capabilities" last="downstream task performance."></alphaxiv-paper-citation></li> </ul> </li> <li><strong>意义：</strong> 这打破了业界的迷信——“只要把基座模型做大做强，机器人就自动变强了”。事实并非如此。</li> </ul> </li> </ul> <h3 id="2-辅助任务的滑铁卢">2. 辅助任务的“滑铁卢”</h3> <ul> <li><strong>假设验证：</strong> 如果我在 VLM 上先训练一些“相关任务”，效果会好吗？(即串行, 先在辅助任务上面训,再训练VLA。并不是像 $\pi_{0.5}$ 中的Co-Training一样.)</li> <li><strong>测试任务集：</strong> <ol> <li><strong>Robopoint:</strong> 给图，输出物体坐标（点选）。</li> <li><strong>Depth Estimation:</strong> 估计深度图。</li> <li><strong>Embodied QA:</strong> 机器人视角的问答。</li> </ol> </li> <li><strong>实验结果 (Figure 4)：</strong> <strong>大部分都是负收益或无收益。</strong> <ul> <li>即使在 Robopoint 上微调让点选准确率提升了 20%，但变成 VLA 后，抓取成功率反而可能下降。</li> <li><strong>深层原因探讨：</strong> 这种“感知能力”与“控制策略”是解耦的。知道物体坐标 (X,Y) 是一回事，生成一条平滑的、避障的 7-DoF 轨迹是完全另一回事。</li> </ul> </li> </ul> <h3 id="3-消融实验谁才是瓶颈">3. 消融实验：谁才是瓶颈?</h3> <ul> <li><strong>实验设计：</strong> <ul> <li>(A) 冻结 Vision Encoder。</li> <li>(B) 冻结 LLM。</li> <li>(C) 冻结 Word Embeddings。</li> </ul> </li> <li><strong>震耳欲聋的结论 (Table 3)：</strong> <ul> <li><strong>冻结视觉编码器 = 毁灭性打击。</strong> 性能暴跌 (e.g., Calvin 得分从 3.8 跌到 0.5)。</li> <li><strong>冻结 LLM = 影响有限。</strong></li> <li><strong>解读：</strong> VLM 的瓶颈不在于“推理”（LLM部分），而在于“看”（Vision Encoder）。现有的 CLIP/SigLIP 视觉特征主要是为了“语义对齐”（Semantic Alignment），而不是为了“几何控制”（Geometric Control）。<alphaxiv-paper-citation title="Modality Ablation" page="1" first="identifies the visual" last="performance bottleneck."></alphaxiv-paper-citation></li> </ul> </li> </ul> <hr/> <h2 id="第四部分深入探究视觉鸿沟与未来展望">第四部分：深入探究“视觉鸿沟”与未来展望</h2> <h3 id="1-到底是仿真的问题还是语义的问题">1. 到底是“仿真”的问题，还是“语义”的问题?</h3> <ul> <li><strong>质疑：</strong> 也许视觉编码器表现不好，是因为 VLM 是在真实世界图片上训练的，而测试是在仿真环境（Sim）里？是不是 Sim-to-Real 的 Gap？</li> <li><strong>精妙的验证实验 (Section 4.4)：</strong> <ul> <li>作者使用了 <strong>BridgeV2</strong> 数据集（全真实世界图像）。</li> <li>作者设计了一个 VLM 微调任务：用 VLM 预测离散化的动作 Token (Fast-Token)。</li> <li><strong>对比组：</strong> <ol> <li>微调 VLM 时冻结视觉编码器。</li> <li>微调 VLM 时解冻视觉编码器。</li> </ol> </li> <li><strong>结果 (Table 4)：</strong> 即使是在全是真实图像的数据上，如果冻结视觉编码器，性能依然很差。只有解冻并训练视觉部分，性能才提升。</li> <li><strong>结论：</strong> 这证明了 Gap <strong>不是</strong>来自于 Sim-to-Real 的风格差异，而是来自于 <strong>Pretraining Objective (图文匹配)</strong> 与 <strong>Control Objective (动作输出)</strong> 之间的本质语义鸿沟。<alphaxiv-paper-citation title="Real World Exp" page="11" first="Even when training" last="improve downstream performance."></alphaxiv-paper-citation></li> </ul> </li> </ul> <h3 id="2-总结与讨论">2. 总结与讨论</h3> <ul> <li><strong>VLM4VLA 的启示：</strong> <ul> <li>不要盲目追求大参数量 VLM。</li> <li>视觉表征（Visual Representation）是目前具身智能最大的短板。</li> <li>未来的方向不应该是“更多的数据”，而应该是“更具身的数据”来预训练视觉编码器（例如使用视频预测、光流、物理交互数据）。</li> </ul> </li> <li><strong>开放问题讨论：</strong> <ul> <li><em>Q1:</em> 如果让你设计一个新的预训练任务来替代 CLIP，专门服务于机器人，你会怎么设计？（提示：考虑 Inverse Dynamics, State Estimation）。</li> <li><em>Q2:</em> 论文中提到 MLP Head 是为了公平比较，但实际应用中，你认为 Diffusion Head 能弥补 VLM 基座的不足吗？</li> </ul> </li> </ul> <p><strong>思考：</strong> 既然 VLM 预训练特征（语义导向）与控制任务（几何/动作导向）之间存在“域差异 (Domain Gap)”，为什么 VLM 初始化依然比从头训练 (Training from Scratch) 表现要好？下面结合“流形学习 (Manifold Learning)”的概念，并参考论文 Figure 5 (关于特征空间可视化的部分) 进行解释。</p> <h4 id="核心论点从无序混沌到结构化流形的跃迁"><strong>核心论点：从“无序混沌”到“结构化流形”的跃迁</strong></h4> <p>尽管 VLM 的预训练目标（图文对齐）没有直接教导机器人如何“运动”，但它为神经网络提供了一个<strong>高度结构化的特征流形 (Structured Feature Manifold)</strong>。这种结构化的初始状态，远比随机初始化的“混沌状态”更容易通过微调收敛到最优策略。</p> <h4 id="1-流形学习视角的解释"><strong>1. 流形学习视角的解释</strong></h4> <ul> <li><strong>高维数据的低维流形：</strong> 图像是极高维的数据（$224 \times 224 \times 3$ 像素），但在高维空间中，有效的图像数据分布在一个低维的流形上。</li> <li><strong>从头训练 (Random Init) 的困境：</strong> <ul> <li>如果从零开始训练，视觉编码器 (Vision Encoder) 必须同时解决两个难题： <ol> <li><strong>表征学习 (Representation Learning)：</strong> 学会如何从像素中提取边缘、纹理、物体边界，构建视觉流形。</li> <li><strong>策略学习 (Policy Learning)：</strong> 学会根据特征输出动作。</li> </ol> </li> <li>由于机器人数据通常很稀缺（相比于互联网图文数据），从头训练的模型很难在有限数据下构建出鲁棒的视觉流形，容易陷入过拟合或无法捕捉复杂的物体关系。</li> </ul> </li> <li><strong>VLM 初始化的优势 (The “Warm Start”)：</strong> <ul> <li>预训练的 VLM（如 SigLIP 或 CLIP 编码器）已经通过数十亿张图像的学习，构建了一个成熟的视觉流形。在这个流形中，相似语义的物体已经聚集在一起，背景噪声已经被过滤。</li> <li><strong>克服“域差异”：</strong> 虽然这个流形是“语义”的（比如它知道这是“杯子”，但不知道“杯柄坐标”），但它已经具备了<strong>可塑性 (Plasticity)</strong>。将一个已经能识别“物体”的特征空间，微调成能识别“几何坐标”的空间，在优化路径上比从纯噪声开始要短得多、容易得多。</li> </ul> </li> </ul> <h4 id="2-结合-figure-5-特征空间可视化-的分析"><strong>2. 结合 Figure 5 (特征空间可视化) 的分析</strong></h4> <p>参考论文中的 Figure 5（通常展示 t-SNE 或 PCA 的特征投影图），我们可以观察到以下现象，佐证上述理论：</p> <ul> <li><strong>图表描述：</strong> 该图展示了不同模型处理输入图像后得到的 Token Embedding 在二维空间中的分布。</li> <li><strong>VLM 初始化 (Fine-tuned VLM)：</strong> <ul> <li>其特征点的分布呈现出<strong>清晰的聚类 (Clustering)</strong> 结构。</li> <li>这意味着模型能够将不同的任务指令（如“打开抽屉”与“抓取苹果”）对应的视觉场景，在特征空间中清晰地分离开来。</li> <li><strong>关键点：</strong> 这种分离能力很大程度上继承自预训练权重。VLM 能够“理解”场景内容的变化，因此策略头 (MLP Head) 只需要学习简单的线性或非线性映射即可输出动作。</li> </ul> </li> <li><strong>从头训练 (Scratch / Random Init)：</strong> <ul> <li>虽然论文并未直接画出 Scratch 失败的 t-SNE，但对比可以看出，未经过大规模预训练的特征空间往往是<strong>纠缠 (Entangled)</strong> 的。</li> <li>在纠缠的空间中，不同的任务状态混合在一起，决策边界极其复杂，导致策略学习失败。</li> </ul> </li> </ul> <h4 id="3-总结">3. 总结</h4> <ol> <li>VLM 初始化之所以有效，是因为它解决了 <strong>“感知 (Perception)”</strong> 这一最困难的第一步。 虽然 VLM 的“感知”是不完美的（缺乏几何精度，即 Domain Gap），但它提供了一个 <strong>包含物体概念和场景结构的先验知识库</strong>。微调过程实质上是在这个良好的“地基”上进行修补和对齐，而不是在平地上从头盖楼。因此，尽管通用能力不能完美预测控制性能，但预训练本身是构建高性能 VLA 不可或缺的基石。 <alphaxiv-paper-citation title="Conclusion" page="8" first="VLM initialization offers" last="training from scratch"></alphaxiv-paper-citation></li> </ol> <h2 id="对上面两个开放讨论题的一些思考">对上面两个开放讨论题的一些思考</h2> <h3 id="q1-如果让你设计一个新的预训练任务来替代-clip专门服务于机器人你会怎么设计"><strong>Q1: 如果让你设计一个新的预训练任务来替代 CLIP，专门服务于机器人，你会怎么设计？</strong></h3> <p><strong>背景：</strong> CLIP 的对比学习目标（Contrastive Learning）主要是为了对齐<strong>高层语义</strong>（例如：图片里有“狗”和“草地”），它忽略了<strong>底层几何</strong>（物体具体的空间位置）和<strong>时间动力学</strong>（物体如何运动）。机器人需要的是后两者。</p> <p>我们可以构想一个 <strong>“Physics-Aware &amp; Action-Centric” (物理感知与动作中心)</strong> 的预训练框架，包含以下三个互补的子任务：</p> <h4 id="1-逆动力学预测-inverse-dynamics-prediction"><strong>1. 逆动力学预测 (Inverse Dynamics Prediction)</strong></h4> <ul> <li><strong>设计逻辑：</strong> 不要只看静态图片。给模型看视频片段中的两帧：$I_t$ (当前帧) 和 $I_{t+k}$ (未来帧)。</li> <li><strong>任务目标：</strong> 预测 $I_t$ 到 $I_{t+k}$ 之间发生了什么动作？ \(\text{Loss} = \| f(I_t, I_{t+k}) - a_{t:t+k} \|^2\)</li> <li><strong>为什么有效：</strong> CLIP 只能告诉你“这里有一个被推到的杯子”。逆动力学预训练能强迫视觉编码器理解 <strong>“是什么动作导致了这种视觉变化”</strong>。这种特征对机器人控制（即根据目标状态反推动作）是直接同构的。</li> </ul> <h4 id="2-视频掩码预测-masked-video-modeling-with-physical-constraints"><strong>2. 视频掩码预测 (Masked Video Modeling with Physical Constraints)</strong></h4> <ul> <li><strong>设计逻辑：</strong> 类似于 MAE (Masked Autoencoder)，但在视频流上做。</li> <li><strong>任务目标：</strong> 遮挡住视频中物体接触的瞬间，让模型基于物理规律“脑补”出中间帧。</li> <li><strong>关键改进：</strong> 不仅仅预测像素颜色（Pixel Loss），还要预测<strong>光流 (Optical Flow)</strong> 或 <strong>深度图 (Depth Map)</strong> 的变化。</li> <li><strong>为什么有效：</strong> 这强迫模型学习物体恒常性 (Object Permanence) 和基础物理属性（重力、碰撞）。它让视觉特征包含“可供性 (Affordance)”信息——即知道物体是可以被推动或抓取的。</li> </ul> <h4 id="3-密集点跟踪-dense-point-tracking"><strong>3. 密集点跟踪 (Dense Point Tracking)</strong></h4> <ul> <li><strong>设计逻辑：</strong> 参考 Google 的 TAP (Tracking Any Point) 技术。</li> <li><strong>任务目标：</strong> 随机选择图像上的一个像素点（比如杯柄的一个点），要求模型在随后的视频序列中持续追踪这一个点，即使它被遮挡或旋转。</li> <li><strong>为什么有效：</strong> 机器人抓取需要极高的几何精度。通过追踪点，视觉编码器被迫学习细粒度的<strong>对应关系 (Correspondence)</strong>，而不是像 CLIP 那样只关注全局的语义标签。</li> </ul> <p><strong>总结：</strong> 未来的预训练不应是 Image-Text Pair，而应是 <strong>Video-Action Pair</strong> 或纯视频流，目标是从“识别物体”转向“理解物理交互”。</p> <h3 id="q2-论文中提到-mlp-head-是为了公平比较但实际应用中diffusion-head-能弥补-vlm-基座的不足吗"><strong>Q2: 论文中提到 MLP Head 是为了公平比较，但实际应用中，Diffusion Head 能弥补 VLM 基座的不足吗？</strong></h3> <p><strong>背景：</strong> 这是一个关于 <strong>“策略表达能力 (Policy Expressivity)” vs. “感知瓶颈 (Perceptual Bottleneck)”</strong> 的辩证讨论。Diffusion Policy 是目前的 SOTA，它能建模多模态分布（Multimodal Distribution），比简单的 MLP 强得多。</p> <p>我的回答是：<strong>Diffusion Head 可以cover部分基座的缺陷，但无法修复根本性的感知盲区。</strong></p> <h4 id="1-diffusion-head-的补救作用-the-band-aid-effect"><strong>1. Diffusion Head 的“补救”作用 (The “Band-Aid” Effect)</strong></h4> <ul> <li><strong>解决多模态分布问题：</strong> 当 VLM 基座给出的特征不够明确时，可能有多种合理的动作（比如抓杯子可以抓杯口，也可以抓杯柄）。 <ul> <li><strong>MLP 的缺陷：</strong> MLP 倾向于输出所有可能动作的<strong>平均值</strong>（Mean），这往往是一个无效动作（抓空气）。</li> <li><strong>Diffusion 的优势：</strong> 它可以建模复杂的分布，随机采样出其中一种合理的动作。因此，即使 VLM 特征有点模糊，Diffusion 也能通过强大的拟合能力生成平滑、拟人的轨迹。</li> </ul> </li> <li><strong>平滑噪声：</strong> Diffusion 的去噪过程本身具有平滑轨迹的作用，可以抵消 VLM 特征中微小的抖动或不稳定性。</li> </ul> <h4 id="2-感知瓶颈的不可逾越性-garbage-in-garbage-out"><strong>2. 感知瓶颈的不可逾越性 (Garbage In, Garbage Out)</strong></h4> <ul> <li><strong>核心论点：</strong> 策略头（Head）的能力上限受限于感知器（Backbone）的信息量。 \(P(Action | Image) = P(Action | Feature) \times P(Feature | Image)\)</li> <li><strong>“看不见”的问题：</strong> 如果 VLM 的 Vision Encoder 根本没有编码“透明玻璃杯”的边缘特征（因为 CLIP 训练数据里透明物体很少），那么特征 $Z$ 中就不包含杯子的位置信息。 <ul> <li>在这种情况下，无论后面的 Diffusion Head 多么强大，它本质上是在<strong>瞎猜 (Hallucinating)</strong>。它可能会生成一条非常平滑、非常像人类动作的轨迹，但位置完全是错的（比如抓向了杯子左边 10 厘米处）。</li> </ul> </li> <li><strong>VLM4VLA 的发现佐证：</strong> 论文中 Table 3 显示，冻结视觉编码器会导致性能崩盘。这证明了如果特征层（Perception）没有被调整到适应控制任务，后端的策略层再怎么训练也无力回天。</li> </ul> <h4 id="3-为什么-vlm4vla-坚持用-mlp"><strong>3. 为什么 VLM4VLA 坚持用 MLP？</strong></h4> <ul> <li><strong>显微镜效应：</strong> 正因为 Diffusion 太强了，它可能会把 60 分的基座和 80 分的基座都拉到 90 分（在简单任务上）。</li> <li><strong>结论：</strong> 在科研中，为了看清基座的差异，我们需要 MLP 这种“直肠子”网络作为显微镜。但在工程落地中，我们应该<strong>同时</strong>使用最好的基座（经过解冻微调的）和最好的头（Diffusion），以追求最佳性能。</li> </ul> <h1 id="关键术语表-glossary">关键术语表 (Glossary)</h1> <ul> <li><strong>Proprioception:</strong> 本体感知（机器人的关节角度、速度等内部状态）。</li> <li><strong>Action Chunking:</strong> 动作分块，一次预测未来 $k$ 步动作，用于平滑轨迹。</li> <li><strong>Affordance:</strong> 可供性，物体提供的交互可能性（例如杯柄“提供”了抓取的可能性）。</li> <li><strong>Domain Gap:</strong> 域差异，通常指训练数据分布与测试数据分布的不一致。</li> </ul> <h1 id="other-thoughts">Other thoughts</h1> <p>我认为当前具身智能（Embodied AI）研究中最核心的痛点：<strong>什么样的视觉表征（Visual Representation）才是机器人真正需要的？</strong> VLM4VLA 的结果在很大程度上暗示了现有的 VLM（以 CLIP/SigLIP 为视觉基座）并不是 VLA 最理想的预训练模型。 <strong>Dino-World</strong> (基于 DINOv2 特征的世界模型) 代表了另一条更有希望的技术路线——<strong>以物体和几何为中心的自监督学习</strong>。</p> <h3 id="1-为什么-vlm-的视觉基座clipsiglip不仅是不完美的甚至是有害的">1. 为什么 VLM 的视觉基座（CLIP/SigLIP）不仅是不完美的，甚至是有害的？</h3> <p>在 $VLM4VLA$ 论文中，我们看到冻结视觉编码器会导致性能崩盘。这背后的根本原因在于 <strong>预训练目标（Pre-training Objective）的错位</strong>：</p> <ul> <li><strong>CLIP/SigLIP 的目标：语义对齐 (Semantic Alignment)</strong> <ul> <li>它们的目标是拉近“图片 embedding”和“文本 embedding”的距离。</li> <li><strong>副作用（空间压缩）：</strong> 为了对齐文本（通常是抽象的），编码器倾向于扔掉“无关”的细节。</li> <li><em>例子：</em> 对于文本“一只狗在草地上”，CLIP 只需要知道“有狗”和“有草”就行了。至于狗的左脚坐标是多少？草地的纹理摩擦力如何？这些信息对于图文匹配是<strong>噪音</strong>，因此被<strong>压缩掉（Discarded）</strong>了。</li> </ul> </li> <li><strong>VLA 的需求：几何与物理 (Geometry &amp; Physics)</strong> <ul> <li>机器人控制需要的是：精确的 3D 坐标、物体边缘的接触点、物体之间的相对深度。</li> <li><strong>矛盾点：</strong> 机器人最需要的信息，恰恰是 CLIP 在预训练中最想扔掉的信息。</li> </ul> </li> </ul> <p>这就是为什么VLM4VLA 发现必须<strong>解冻</strong>视觉编码器。解冻的本质，就是为了<strong>找回</strong>那些被预训练丢掉的空间几何信息。</p> <h3 id="2-为什么-dino-及-dino-world-是更好的候选者">2. 为什么 DINO (及 Dino-World) 是更好的候选者？</h3> <p><strong>Dino-World</strong> (Back to the Features: DINO as a Foundation for Video World Models) 这篇论文的核心论点是：<strong>DINOv2 的特征本身就包含了构建物理世界模型所需的一切，无需微调。</strong></p> <p>如果以 DINO/Dino-World 作为 VLA 的基座，优势在于：</p> <h4 id="a-更加密集的局部特征-dense--local-features">A. 更加密集的局部特征 (Dense &amp; Local Features)</h4> <ul> <li><strong>VLM (CLIP):</strong> 关注全局语义（Global Token），是一个高度抽象的向量。</li> <li><strong>DINO:</strong> 关注局部补丁（Patch-level Features）。DINOv2 的注意力图（Attention Map）天然就能分割出物体（Object Segmentation），即使没有监督信号。</li> <li><strong>对 VLA 的意义：</strong> 机器人操作通常是局部的（比如“抓取杯柄”）。DINO 能清晰地看见“杯柄”作为一个独立的几何部件，而 CLIP 可能只看见“杯子”这个整体概念。</li> </ul> <h4 id="b-几何与对应关系-geometry--correspondence">B. 几何与对应关系 (Geometry &amp; Correspondence)</h4> <ul> <li><strong>DINO 的特性：</strong> DINO 特征在不同视角下具有极强的一致性（Correspondence）。如果摄像头移动了，DINO 特征能精确地追踪图像上的同一个点。</li> <li><strong>对 VLA 的意义：</strong> 这正是<strong>手眼协调 (Hand-Eye Coordination)</strong> 的基础。机器人需要知道“我现在的机械手位置”和“目标位置”在空间上的几何关系。Dino-World 证明了仅仅利用这些特征就能预测下一帧的物理状态，这意味着它懂<strong>物理 (Physics)</strong>。</li> </ul> <h4 id="c-无需语言的物理常识">C. 无需语言的物理常识</h4> <ul> <li>Dino-World 证明了在没有语言输入的情况下，仅靠视觉特征就能模拟物体掉落、液体流动。这种<strong>“前语言 (Pre-linguistic)”的物理直觉</strong>，正是 <em>System 1</em>（底层控制）所急需的。</li> </ul> <h3 id="3-硬币的另一面为什么还需要-vlmthe-alignment-problem">3. 硬币的另一面：为什么还需要 VLM？(The Alignment Problem)</h3> <p>虽然 DINO 在“看”和“动”方面更强，但如果完全用 DINO 替代 VLM，会面临一个巨大的挑战：<strong>语言指令的丢失 (The Grounding Gap)</strong>。</p> <ul> <li><strong>VLM 的优势：</strong> 用户说“拿红色的苹果”，VLM 知道哪个是“红色”，哪个是“苹果”。</li> <li><strong>DINO 的劣势：</strong> DINO 知道这里有一个圆形的物体，那里有一个方形的物体，但它<strong>不知道</strong>哪个叫“苹果”。DINO 的特征空间与语言空间没有对齐。</li> </ul> <h3 id="4-终极架构预测双流混合模型-hybrid-dual-stream-architecture">4. 终极架构预测：双流混合模型 (Hybrid Dual-Stream Architecture)</h3> <p>结合VLM4VLA的发现和Dino-World$ 的思考，未来的 VLA SOTA 架构很可能不是单纯的 VLM，也不是单纯的 DINO，而是两者的融合：</p> <ul> <li><strong>架构设想：</strong> <ol> <li><strong>视觉流 (Action Stream):</strong> 使用 <strong>DINOv2 / Dino-World</strong> 作为主视觉编码器。它负责提供高频、高精度的几何特征，直接输入给策略头 (Policy Head) 用于生成动作轨迹。</li> <li><strong>语义流 (Semantic Stream):</strong> 使用轻量级的 <strong>VLM</strong> 或 <strong>CLIP</strong>。它负责处理用户的文本指令，生成一个“语义引导向量 (Semantic Guidance Vector)”。</li> <li><strong>融合 (Fusion):</strong> 利用 Cross-Attention，让 DINO 的特征去“查询” VLM 的语义。 <ul> <li><em>逻辑：</em> VLM 告诉 DINO “我们要找红色的苹果”，DINO 回答 “收到，在坐标 (x, y) 处有一个符合描述的物体，这是它的边缘几何信息”。</li> </ul> </li> </ol> </li> </ul> <h3 id="5-总结">5. 总结</h3> <p>VLM4VLA从反面证明了 <strong>“以语义为中心的 VLM 不适合直接做控制”</strong>。 而 <strong>Dino-World</strong> 这类工作指出了正确的方向：<strong>回归特征本身 (Back to the Features)</strong>。未来的 VLA 预训练模型，一定是在海量视频数据上通过自监督学习（学习物体恒常性、逆动力学）得到的，而不是仅仅通过图文对齐得到的。</p> <p><strong>思考：</strong> 现有的 “Foundation Models” 大多是基于 Internet Text/Image 的。我们是否需要一个专门的 <strong>“Robotics Foundation Model”</strong>？它的预训练数据不应该是 <code class="language-plaintext highlighter-rouge">&lt;Image, Text&gt;</code>，而应该是 <code class="language-plaintext highlighter-rouge">&lt;Video, Action&gt;</code> 或者纯粹的 <code class="language-plaintext highlighter-rouge">&lt;Physics Interactions&gt;</code>。</p> <h2 id="vision-与-language-的对齐是否有必要">vision 与 language 的对齐是否有必要</h2> <p><strong>文章并没有直接说“对齐（Alignment）没有必要”，但大量的实验证据强烈暗示：现有的“视觉-语言对齐”（即 CLIP/SigLIP 式的对齐）对于下游的控制任务来说，不仅是不充分的，甚至可能是次要的。</strong> 论文通过以下三个核心实验，间接回答这个问题：</p> <h3 id="1-证据一模态消融实验--语言并不那么重要">1. 证据一：模态消融实验 —— “语言并不那么重要”</h3> <p>在 <strong>Table 3</strong>（Modality-level ablations）中，作者做了一个极具启发性的对比：</p> <ul> <li><strong>实验 A（冻结视觉）：</strong> 保持 Vision Encoder 不动（即保持完美的 VLM 预训练对齐状态），只微调 LLM 和 Projector。 <ul> <li><strong>结果：</strong> 性能崩盘（例如 Calvin 分数从 3.8 跌到 0.5）。</li> <li><strong>解读：</strong> 这说明即使保留了最完美的“视觉-语言对齐”特征，机器人也无法工作。</li> </ul> </li> <li><strong>实验 B（冻结语言）：</strong> 保持 Word Embeddings 不动（即不再调整语言空间的映射），只微调 Vision Encoder。 <ul> <li><strong>结果：</strong> 性能几乎没有下降（3.856 vs 3.849）。</li> <li><strong>深度解读：</strong> 这说明<strong>语言端的对齐甚至不需要微调</strong>。机器人只要能看懂图像里的几何信息（Vision），语言指令（Language）只需要作为一个静态的“触发器”或“条件索引”就足够了。<strong>强求更深度的视觉-语言交互并没有带来额外收益。</strong></li> </ul> </li> </ul> <alphaxiv-paper-citation title="Word Embedding Ablation" page="10" first="freeze word embedding" last="(-0.021)"/> <h3 id="2-证据二辅助任务微调--更强的对齐-neq-更强的控制">2. 证据二：辅助任务微调 —— “更强的对齐 $\neq$ 更强的控制”</h3> <p>在 <strong>Section 4.2</strong> 和 <strong>Figure 4</strong> 中，作者尝试通过 VQA（视觉问答）、Captioning（图像描述）等任务来增强 VLM。</p> <ul> <li><strong>逻辑：</strong> 这些任务本质上都是在<strong>增强</strong> Vision 和 Language 的语义对齐能力。</li> <li><strong>结果：</strong> 几乎所有的“对齐增强”操作（如 RoboPoint, Vica, Embodied VQA），在转化为 VLA 后，性能都<strong>没有提升</strong>，甚至略有下降。 (hmm~~~)</li> <li><strong>结论：</strong> 进一步强化语义对齐是<strong>徒劳</strong>的。机器人需要的不是“更懂图文对应关系”，而是“更懂动作”。</li> </ul> <alphaxiv-paper-citation title="Auxiliary Tasks Failure" page="1" first="improving a VLM’s" last="control performance."/> <h3 id="3-证据三section-44-的动作注入实验--为了控制必须破坏对齐">3. 证据三：Section 4.4 的“动作注入”实验 —— “为了控制，必须破坏对齐”</h3> <p>这是一个非常微妙的推论。</p> <ul> <li>在 Section 4.4 中，作者发现必须解冻 Vision Encoder 并注入 Action Token 的监督信号，模型才能工作。</li> <li><strong>思考一下这意味着什么：</strong> 当我们为了 Action Loss 去更新 Vision Encoder 的参数时，我们实际上是在<strong>破坏</strong>它原本与 Text Encoder 建立好的 CLIP 对齐空间。</li> <li><strong>权衡（Trade-off）：</strong> 实验表明，为了获得控制能力（Control），模型“甚至愿意”牺牲一部分原本的图文对齐特性（通过改变视觉特征分布）。这证明了在 VLA 任务中，<strong>“动作对齐 (Action Alignment)”的优先级远高于“语言对齐 (Language Alignment)”。</strong></li> </ul> <h3 id="总结">总结</h3> <p><strong>“视觉-语言对齐” (V-L Alignment) 仅仅是入场券，而不是胜负手。</strong></p> <ol> <li><strong>入场券作用：</strong> 它让机器人知道“杯子”是哪个物体（Instruction Following）。没有对齐，机器人就连任务目标都找不到。</li> <li><strong>非核心作用：</strong> 一旦锁定了目标，<strong>如何移动手臂</strong>（Motor Control）完全不依赖于语言对齐，而是依赖于几何感知。</li> <li><strong>论文的启示：</strong> 现有的 VLM 预训练不仅<strong>过度关注</strong>了对齐（导致几何信息丢失），而且在微调时，我们实际上是在<strong>对抗</strong>这种对齐，试图把视觉特征从“语义空间”拉回到“物理空间”。</li> </ol> <p>所以，<strong>论文体现出，为了做好 VLA，保持或增强通用的视觉-语言对齐是没有必要的，甚至我们需要打破这种对齐，去建立新的“视觉-动作”对齐。</strong></p> <h2 id="论文的-figure-3-已经说了vla的能力和vlm的能力并不是正相关-那辅助tasks为什么还设计成-vqa的形式">论文的 Figure 3 已经说了VLA的能力和VLM的能力并不是正相关, 那辅助tasks为什么还设计成 VQA的形式</h2> <p>我的疑问： 前提 (Result from Fig 3): 作者自己已经证明了 VLM 通用能力（VQA分数） $\nRightarrow$ VLA 控制能力。 行为 (Method in Sec 4.2): 作者却依然把辅助任务设计成 VQA 形式（试图通过提升 VQA 分数来提升 VLA 能力）。 矛盾： 这不是明知故犯吗？既然已经知道 VQA 分数没用，为什么还要用 VQA 的方式来做辅助任务？</p> <p>这是一个<strong>极其犀利且一针见血</strong>的逻辑拷问！你发现了一个论文内部在逻辑设计上看似“自相矛盾”或“不够自洽”的地方。</p> <p><strong>你的质疑：</strong></p> <ol> <li><strong>前提 (Result from Fig 3):</strong> 作者自己已经证明了 VLM 通用能力（VQA分数） $\nRightarrow$ VLA 控制能力。</li> <li><strong>行为 (Method in Sec 4.2):</strong> 作者却依然把辅助任务设计成 VQA 形式（试图通过提升 VQA 分数来提升 VLA 能力）。</li> <li><strong>矛盾：</strong> 既然已经知道 VQA 分数没用，为什么还要用 VQA 的方式来做辅助任务？</li> </ol> <hr/> <h3 id="这里的逻辑层级差异">这里的逻辑“层级”差异</h3> <p>我们需要区分 <strong>“通用 VQA”</strong> 和 <strong>“领域特定 VQA (Domain-Specific VQA)”</strong> 这两个概念。</p> <p>作者的逻辑可能是这样的：</p> <h4 id="1-第一层验证通用能力-vs-具身能力-fig-3-的结论"><strong>1. 第一层验证：通用能力 vs. 具身能力 (Fig 3 的结论)</strong></h4> <ul> <li><strong>对象：</strong> MMBench, Math, Coding, General Captioning。</li> <li><strong>发现：</strong> 你懂数学、懂代码、懂识别蒙娜丽莎（通用 VQA），这对机器人拧螺丝没帮助。</li> <li><strong>潜台词：</strong> “内容”不对口。虽然大家都是 VQA 形式，但你考的是“历史地理”，我要的是“物理体育”。</li> </ul> <h4 id="2-第二层验证如果内容对口了呢-sec-42-的动机"><strong>2. 第二层验证：如果内容对口了呢？ (Sec 4.2 的动机)</strong></h4> <ul> <li><strong>假设：</strong> 既然“内容”不对口是问题，那我就把内容换成对口的！</li> <li><strong>操作：</strong> 我让 VQA 的内容变成“深度估计”、“坐标点选”、“机器人规划”。这些都是机器人急需的知识。</li> <li><strong>核心问题：</strong> 如果<strong>内容</strong>是对口的（都是机器人知识），但<strong>形式</strong>依然是 VQA（文本问答），那还有用吗？</li> <li><strong>实验目的：</strong> 这正是 Sec 4.2 想要探究的——<strong>是“知识领域”的问题，还是“模态形式”的问题？</strong></li> </ul> <h3 id="结论的递进">结论的递进</h3> <p>通过这两步实验，论文实际上完成了一个<strong>双重否定</strong>的逻辑闭环，这反而增强了论文的深度：</p> <ol> <li><strong>Step 1 (Fig 3):</strong> 通用知识（General Knowledge）没用。 $\rightarrow$ <strong>结论：知识领域要对口。</strong></li> <li><strong>Step 2 (Sec 4.2):</strong> 即使知识领域对口了（用了 Robopoint/Depth VQA），如果还是用 VQA 这种文本形式（Text Form）来训练，依然没用！ $\rightarrow$ <strong>结论：模态形式也要对口（不能只用文本 Token）。</strong></li> <li><strong>Step 3 (Sec 4.4):</strong> 只有解冻视觉编码器，直接注入控制信号（破坏原有的文本对齐），才有用。 $\rightarrow$ <strong>终极结论：必须深入到特征层 (Feature Level) 进行改造。</strong></li> </ol> <h3 id="你的质疑为何依然有价值">你的质疑为何依然有价值？</h3> <p>尽管有上述解释，你的质疑依然非常有力，因为：</p> <p><strong>如果在做 Sec 4.2 之前，作者已经有了深刻的洞察（Insight），他们本应该预见到 VQA 形式的局限性。</strong></p> <p>这也反映了当前 AI 社区的一种<strong>路径依赖 (Path Dependence)</strong>：大家太习惯于“把所有东西都 Token 化，扔进 LLM 里微调”。</p> <ul> <li>VLM4VLA 的这部分实验，某种程度上像是一次<strong>“撞南墙”的实证记录</strong>。</li> <li>它告诉社区：<strong>“看，我们试过了，想偷懒用 VQA 统一接口来做具身增强，这条路走不通。别再试了，去改特征吧。”</strong></li> </ul> <blockquote> <p>“作者之所以明知 VQA 没用还要试，是为了<strong>控制变量</strong>，彻底排除‘是因为训练数据内容无关’这一借口，从而最终锁定‘VQA/文本模态本身不适合表达精确控制’这一根本性结论。”</p> </blockquote> <h2 id="some-thoughts">some thoughts</h2> <p>我觉得 串行模式也极有可能是导致为什么辅助任务无效的原因. 因为会有灾难性遗忘, 以及特征空间drift的问题. 或许采用Co-training,或者multi-task的方式,对于VLA的任务应该会有所帮助.</p>]]></content><author><name></name></author><category term="VLA"/><summary type="html"><![CDATA[[TOC]]]></summary></entry><entry><title type="html">JEPA_WM</title><link href="https://beyondpzk.github.io/blog/2025/JEPA_WM/" rel="alternate" type="text/html" title="JEPA_WM"/><published>2025-12-30T00:00:00+00:00</published><updated>2025-12-30T00:00:00+00:00</updated><id>https://beyondpzk.github.io/blog/2025/JEPA_WM</id><content type="html" xml:base="https://beyondpzk.github.io/blog/2025/JEPA_WM/"><![CDATA[<p>[TOC]</p> <h1 id="jepa_wm">JEPA_WM</h1> <p><a href="https://arxiv.org/pdf/2512.24497">paper链接</a></p> <h1 id="机器人学习与世界模型进阶专题">机器人学习与世界模型进阶专题</h1> <p><strong>核心论文：</strong> Terver et al., <em>What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?</em> (arXiv:2512.24497v2)</p> <hr/> <p><strong>目标：</strong> 我们将超越世界模型（World Models）理论层面的“是什么”，深入探讨工程层面的“怎么做”。我们将对应用于机器人规划的<strong>联合嵌入预测架构（JEPA）</strong>进行拆解。与那些提出单一新颖架构的论文不同，这项工作进行了一项极其严谨的<strong>消融实验（Ablation Study）</strong>，旨在找出构建此类模型的最佳“配方”。</p> <p><strong>核心学习成果：</strong></p> <ol> <li>理解用于物理规划的 JEPA-WM 范式。</li> <li>分析关键设计选择：编码器选择、上下文长度和本体感觉（Proprioception）。</li> <li>评估潜空间中的规划算法（CEM 对比 梯度下降）。</li> <li>理解模型扩展性（Scaling）在“仿真到现实（Sim-to-Real）”鸿沟中的差异。</li> </ol> <hr/> <h2 id="第一部分范式转变从像素到潜空间规划"><strong>第一部分：范式转变——从像素到潜空间规划</strong></h2> <h3 id="11-背景为什么要用世界模型"><strong>1.1 背景：为什么要用世界模型？</strong></h3> <p>在强化学习（RL）中，我们经常受困于样本效率问题。无模型（Model-Free）RL 需要数百万次交互。基于模型的 RL（MBRL）试图通过学习环境动力学来解决这个问题。</p> <p>然而，传统的 MBRL 在高维视觉空间（像素级）中往往表现不佳，因为预测每一个像素既昂贵又容易受到“噪声”干扰（例如，预测墙壁的确切纹理，而不是门的位置）。</p> <p>本文关注一种解决方案：<strong>在学习到的表征空间中进行规划</strong>，具体使用的是 <strong>JEPA-WMs</strong>。这类方法的核心在于抽象掉无关的细节，从而产生更高效的规划。 <alphaxiv-paper-citation title="Introduction" page="1" first="Planning is commonly" last="efficient planning."></alphaxiv-paper-citation></p> <h3 id="12-架构jepa-wm"><strong>1.2 架构：JEPA-WM</strong></h3> <p>让我们将系统形式化。我们预测的不是 $s_{t+1}$（像素），而是 $z_{t+1}$（潜变量）。</p> <p><strong>核心组件：</strong></p> <ol> <li><strong>编码器（$E_{\phi, \theta}$）：</strong> 将观测值（$o_t$）映射为潜状态（$z_t$）。</li> <li><strong>预测器（$P_\theta$）：</strong> 动力学模型。它接收当前状态和动作，预测<em>下一个</em>潜状态。</li> <li><strong>规划器（Planner）：</strong> 这不是神经网络，而是一种算法（如 MPC），利用预测器来寻找最优动作序列。</li> </ol> <p>论文清晰地定义了这个框架：(编码器,预测器)就是我们所说的<strong>世界模型</strong>。 <alphaxiv-paper-citation title="Definitions" page="2" first="The encoder/predictor pair" last="world model."></alphaxiv-paper-citation></p> <h3 id="13-训练循环teacher-forcing"><strong>1.3 训练循环（Teacher-Forcing）</strong></h3> <p>如果在不使用重建损失（像素误差）的情况下训练？我们使用联合嵌入（Joint-Embedding）方法。</p> <ul> <li><strong>输入：</strong> 过去的观测和动作的上下文。</li> <li><strong>目标：</strong> <em>实际</em>未来状态的嵌入（由目标编码器计算得出）。</li> <li><strong>损失：</strong> 潜空间中的距离（L2 或 L1）。</li> </ul> <p>训练过程涉及一个预测器，它接收过去观测和动作的上下文，并在时间步上并行预测下一个状态嵌入。 <alphaxiv-paper-citation title="Training" page="2" first="which is fed" last="state embedding."></alphaxiv-paper-citation></p> <hr/> <h2 id="第二部分成功的要素系统特征分析"><strong>第二部分：成功的“要素”——系统特征分析</strong></h2> <h3 id="21-眼睛编码器的选择dino-vs-其他"><strong>2.1 眼睛：编码器的选择（DINO VS 其他）</strong></h3> <p>如果你想让机器人抓起一个杯子，它需要理解背后墙壁的纹理吗？不需要。它需要的是物体恒常性和分割能力。</p> <p><em>发现：</em> 论文将 DINOv2（自监督 ViT）与 V-JEPA 编码器进行了比较。 <em>结果：</em> <strong>DINO 获胜。</strong> 为什么？DINO 拥有更优越的细粒度物体分割能力。这对于精确的定位任务至关重要。 <alphaxiv-paper-citation title="Encoder Analysis" page="9" first="DINO has better" last="segmentation capabilities,"></alphaxiv-paper-citation></p> <h3 id="22-身体本体感觉proprioception"><strong>2.2 身体：本体感觉（Proprioception）</strong></h3> <p><em>讨论：</em> 我们应该仅仅依赖视觉（像素），还是让机器人知道它的关节角度？</p> <ul> <li><strong>观察：</strong> 在许多“纯像素”论文中，为了使模型显得“通用”，往往忽略了本体感觉。</li> <li><strong>论文结果：</strong> 结合本体感觉训练的模型表现始终更好。没有它，机械臂往往会在目标周围震荡，因为仅凭视觉缺乏精细运动停止所需的精度。 <alphaxiv-paper-citation title="Proprioception" page="8" first="models trained with" last="consistently better"></alphaxiv-paper-citation></li> </ul> <h3 id="23-大脑预测器上下文与架构"><strong>2.3 大脑：预测器上下文与架构</strong></h3> <p>模型需要多少历史信息？</p> <ul> <li>$W=1$（1帧）：模型无法推断速度。</li> <li>$W=2$（2帧）：模型可以推断速度（$p_t - p_{t-1}$）。</li> <li> <p>$W=3$（3帧）：模型可以推断加速度。</p> </li> <li> <p><strong>关键发现：</strong> $W=1$ 和 $W=2$ 之间存在巨大的性能差距。速度信息是必不可少的。然而，过长的上下文（如 $W=7$）在仿真中反而会降低性能（过拟合/噪声）。有趣的是，真实世界数据（DROID）受益于稍长的上下文（$W=5$），这可能是由于真实物理动力学的复杂性。 <alphaxiv-paper-citation title="Context Length" page="8" first="gap between models" last="infer velocity."></alphaxiv-paper-citation></p> </li> <li><strong>条件化（Conditioning）：</strong> 我们如何将动作 $a_t$ 输入到 Transformer 预测器中？ <ul> <li><em>拼接（Concatenation）？</em> 简单。</li> <li><em>AdaLN（自适应层归一化）？</em> 复杂但精细。</li> <li><em>结果：</em> <strong>带 RoPE 的 AdaLN</strong> 平均表现最强，因为它将动作信息注入到<em>每一层</em>，防止了信号消失。 <alphaxiv-paper-citation title="Architecture" page="9" first="AdaLN with RoPE" last="average performance,"></alphaxiv-paper-citation></li> </ul> </li> </ul> <h3 id="24-训练目标多步展开multistep-rollout"><strong>2.4 训练目标：多步展开（Multistep Rollout）</strong></h3> <p>如果我们只训练 $t \to t+1$，当我们规划 $t+10$ 时，模型可能会发生漂移。</p> <ul> <li><strong>技术：</strong> 在训练期间增加未来多步的损失项。</li> <li><strong>结果：</strong> 2步展开（2-step rollout）的损失是最佳的。超过这个步数（例如6步）会降低仿真中的性能，使模型对即时预测任务的专业性下降。 <alphaxiv-paper-citation title="Rollout Loss" page="8" first="performance increases when" last="rollout loss models,"></alphaxiv-paper-citation></li> </ul> <hr/> <h2 id="第三部分规划算法与真实世界迁移"><strong>第三部分：规划算法与真实世界迁移</strong></h2> <h3 id="31-优化问题"><strong>3.1 优化问题</strong></h3> <p>(之前ATOM的算法是不是就有用了?)</p> <p>一旦我们有了训练好的世界模型，我们需要解以下方程： \(a_{t:t+H}^* = \arg\min_{a} \sum_{k=t}^{t+H} \text{Cost}(\hat{z}_k, z_{goal})\)</p> <p>论文比较了三大类规划器：</p> <ol> <li><strong>CEM（交叉熵方法）：</strong> 基于采样的。采样高斯动作，挑选最好的，重新拟合高斯分布。</li> <li><strong>GD（梯度下降）：</strong> 通过学习到的模型反向传播误差，直接更新动作。</li> <li><strong>Nevergrad (NG)：</strong> 一个无梯度优化库。</li> </ol> <h3 id="32-为什么梯度下降gd会失败"><strong>3.2 为什么梯度下降（GD）会失败？</strong></h3> <p>理论直觉表明 GD 应该是最好的，因为我们有一个可微的世界模型。</p> <ul> <li><strong>现实检验：</strong> GD 在导航任务（迷宫/墙壁）上表现糟糕。</li> <li><strong>原因：</strong> 潜空间中的成本曲面是非凸的，且充满局部极小值（例如，卡在墙边）。GD 无法“跳过”高成本的障碍。</li> <li><strong>赢家：</strong> CEM（L2 距离）仍然是稳健的冠军。它的探索能力更强。 <alphaxiv-paper-citation title="Planning Optimizers" page="8" first="CEM L2" last="outperforms L1 cost."></alphaxiv-paper-citation></li> </ul> <h3 id="33-扩展定律的差异仿真-vs-现实"><strong>3.3 扩展定律的差异（仿真 vs 现实）</strong></h3> <p>这是现代 AI 的重要一课。</p> <ul> <li><strong>仿真（Metaworld）：</strong> 增加模型大小（ViT-S 到 ViT-L）<strong>没有</strong>帮助。物理很简单；小模型就已经让任务饱和了。</li> <li><strong>真实世界（DROID）：</strong> 增加模型大小<strong>确实</strong>有帮助。真实世界的图像和动力学包含“偶然不确定性（aleatoric uncertainty）”和复杂性，需要更大的容量。</li> <li><strong>结论：</strong> 不要为了简单的基准测试浪费算力去扩展模型。要为了现实世界而扩展。 <alphaxiv-paper-citation title="Scaling" page="9" first="simulated environments saturate" last="lower capacities."></alphaxiv-paper-citation></li> </ul> <hr/> <h2 id="第四部分综合与黄金配方"><strong>第四部分：综合与“黄金配方”</strong></h2> <h3 id="41-提议的最佳方案"><strong>4.1 提议的最佳方案</strong></h3> <p>基于研究，作者提出了一种特定的配置，击败了基线模型（DINO-WM, V-JEPA-2-AC）。</p> <p><strong>配方：</strong></p> <ul> <li><strong>编码器：</strong> DINOv2（若追求照片级真实感可用 v3）。</li> <li><strong>预测器：</strong> 带 AdaLN 条件化的 ViT。</li> <li><strong>训练：</strong> 启用本体感觉 + 2步展开损失（2-step rollout loss）。</li> <li><strong>规划：</strong> 使用 L2 距离的 CEM。</li> </ul> <p><strong>性能：</strong> 这种特定组合显著优于先前的 SOTA。例如，在“Reach（到达）”任务中，他们实现了高得多的成功率。 <alphaxiv-paper-citation title="Results" page="10" first="outperform DINO-WM and" last="most environments."></alphaxiv-paper-citation></p> <h3 id="42-讨论"><strong>4.2 讨论</strong></h3> <ol> <li><strong>“奖励”难题：</strong> 本文使用目标图像（$z_g$）。如果任务没有目标的具体照片（例如，“尽可能快地跑”），如何调整这个 JEPA-WM？</li> <li><strong>潜空间漂移：</strong> 即使有2步展开训练，模型在长时域（50+步）上也可能漂移。为什么 MPC（模型预测控制）能缓解这个问题？（提示：每一步都重新规划）。</li> <li><strong>语言的角色：</strong> 论文提到了 VLA（视觉-语言-动作）模型。我们如何将 JEPA 预测器的条件改为文本指令而不是目标图像？</li> </ol> <hr/> <p>这是为您准备的课堂讨论问题参考答案。作为教授，我不仅提供了标准答案，还加入了一些基于论文原理的延伸思考，以便您引导学生深入讨论。</p> <hr/> <h3 id="问题-1奖励难题"><strong>问题 1：“奖励”难题</strong></h3> <p><strong>问题：</strong> 本文使用目标图像（$z_g$）作为导航终点，计算 $Cost = ||\hat{z}_t - z_g||$。如果任务没有目标的具体照片（例如“尽可能快地跑”或“保持直立”），你将如何调整这个 JEPA-WM？</p> <p><strong>参考答案：</strong> 我们需要将“目标距离”替换为一个<strong>学习到的奖励函数（Learned Reward Function）</strong>。</p> <ol> <li><strong>方法：</strong> 我们可以在 JEPA 的预测器或编码器之上训练一个轻量级的多层感知机（MLP），记为 $R(z_t)$。</li> <li><strong>训练：</strong> 使用带标注的数据集（或者通过人工反馈 RLHF）来训练这个 MLP，使其输入一个潜状态 $z_t$，输出一个标量值（Reward）。例如，如果任务是奔跑，输入当前状态的 $z$，输出当前的速度估算值。</li> <li><strong>规划：</strong> 在规划阶段（CEM），我们不再最小化与目标图像的距离，而是最大化预测轨迹的累积奖励：$\max \sum R(\hat{z}_t)$。</li> <li><strong>延伸思考：</strong> 这实际上让 JEPA-WM 从“目标条件化规划”转向了更通用的“基于模型的强化学习（MBRL）”。虽然这增加了训练奖励模型的开销，但大大扩展了模型的适用范围。</li> </ol> <hr/> <h3 id="问题-2潜空间漂移与-mpc"><strong>问题 2：潜空间漂移与 MPC</strong></h3> <p><strong>问题：</strong> 即使有2步展开训练（2-step rollout），模型在长时域（如预测50步以上）上也必然会产生累积误差（漂移）。为什么 <strong>MPC（模型预测控制）</strong> 机制能缓解这个问题？</p> <p><strong>参考答案：</strong> 关键在于 MPC 的 <strong>“闭环反馈”</strong>机制，它并没有完全信任长期的预测。</p> <ol> <li><strong>执行逻辑：</strong> 虽然我们在大脑中规划了未来 H 步（例如50步），但 MPC <strong>只执行第一个动作</strong> $a_t$。</li> <li><strong>重置误差：</strong> 执行完 $a_t$ 后，机器人会通过传感器看到<strong>真实的</strong>新状态 $o_{t+1}$。此时，我们将编码器重新应用于真实的 $o_{t+1}$ 得到真实的 $z_{t+1}$。</li> <li><strong>重新规划：</strong> 下一轮规划从真实的 $z_{t+1}$ 开始，而不是从模型预测的（可能带有误差的）$\hat{z}_{t+1}$ 开始。</li> <li><strong>结论：</strong> MPC 每一步都用真实的观测值“校准”了当前位置。这就像使用 GPS 导航：虽然它规划了全程路线，但如果你偏离了路线，它会根据你当前的<strong>真实位置</strong>重新计算，而不是假设你还在原来的路线上盲目指挥。</li> </ol> <hr/> <h3 id="问题-3语言的角色"><strong>问题 3：语言的角色</strong></h3> <p><strong>问题：</strong> 论文提到了 VLA（视觉-语言-动作）模型。我们如何将 JEPA 预测器的条件改为文本指令（如“拿起蓝色杯子”）而不是目标图像？</p> <p><strong>参考答案：</strong> 这涉及到<strong>多模态对齐（Multimodal Alignment）</strong>或<strong>条件注入（Condition Injection）</strong>。主要有两种改法：</p> <p><strong>方案 A：潜在空间对齐（CLIP 风格）</strong></p> <ul> <li><strong>原理：</strong> 使用像 CLIP 这样预训练好的模型，它能将图像和文本映射到同一个共享空间。</li> <li><strong>操作：</strong> <ol> <li>将指令“拿起蓝色杯子”通过文本编码器编码为向量 $e_{text}$。</li> <li>我们训练 JEPA 的视觉编码器，使其输出的 $z$ 与 CLIP 的空间对齐。</li> <li> <table> <tbody> <tr> <td>规划目标变为最小化当前状态与文本嵌入的距离：$Cost =</td> <td> </td> <td>\hat{z}<em>t - e</em>{text}</td> <td> </td> <td>$。</td> </tr> </tbody> </table> </li> </ol> </li> <li><strong>优点：</strong> 不需要修改预测器架构。</li> </ul> <p><strong>方案 B：预测器条件化（Cross-Attention）</strong></p> <ul> <li><strong>原理：</strong> 将文本指令作为一种“上下文”输入给预测器，告诉预测器“在这种意图下，世界会如何演变”。</li> <li><strong>操作：</strong> <ol> <li>在预测器（Predictor）的 Transformer 架构中插入<strong>交叉注意力层（Cross-Attention Layers）</strong>。</li> <li>Query 是当前的状态 $z_t$，Key/Value 是文本指令的嵌入。</li> <li>这样，预测器不仅仅是在预测物理规律，而是在预测“为了实现该指令”而产生的状态变化。</li> </ol> </li> <li><strong>论文关联：</strong> 论文中提到的 AdaLN（自适应层归一化）其实就是一种条件注入方式。我们可以把注入“动作”的地方，改为注入“动作 + 文本嵌入”，让模型根据语言指令来调节动力学预测。</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[[TOC]]]></summary></entry><entry><title type="html">ImprovedMeanFlows</title><link href="https://beyondpzk.github.io/blog/2025/ImprovedMeanFlows/" rel="alternate" type="text/html" title="ImprovedMeanFlows"/><published>2025-12-01T00:00:00+00:00</published><updated>2025-12-01T00:00:00+00:00</updated><id>https://beyondpzk.github.io/blog/2025/ImprovedMeanFlows</id><content type="html" xml:base="https://beyondpzk.github.io/blog/2025/ImprovedMeanFlows/"><![CDATA[<p>[TOC]</p> <h1 id="improvedmeanflows">ImprovedMeanFlows</h1> <ul> <li><a href="https://arxiv.org/abs/2512.02012">paper地址</a></li> </ul> <p>这篇论文在单步生成模型（One-step Generative Models）领域是一个重要的里程碑。</p> <hr/> <h1 id="improved-mean-flows迈向高效的单步生成模型">Improved Mean Flows——迈向高效的单步生成模型</h1> <hr/> <h2 id="第一部分motivation--background">第一部分：Motivation &amp; Background</h2> <h3 id="11-为什么我们需要快进生成-fastforward-generative-models">1.1 为什么我们需要“快进”生成 (Fastforward Generative Models)？</h3> <p>我们要探讨的话题是如何让生成模型“跑得更快”。</p> <ul> <li><strong>现状</strong>：目前的扩散模型（Diffusion Models）和 Flow Matching 虽然效果惊人，但通常需要几十甚至上百步的迭代求解 ODE（常微分方程）才能生成一张图。</li> <li><strong>目标</strong>：我们的终极目标是 <strong>1-NFE (Number of Function Evaluations)</strong>，也就是只需要<strong>一步</strong>网络前向传播就能生成高质量图像。</li> <li><strong>挑战</strong>：将复杂的 ODE 轨迹压缩成一步，就像是不仅要学会走路，还要学会“瞬间移动”。这类模型我们称为“快进生成模型”（Fastforward Generative Models）。 <alphaxiv-paper-citation title="Concept" page="1" first="Using the concept" last="underlying differential equations."></alphaxiv-paper-citation></li> </ul> <h3 id="12-回顾original-meanflow-mf-是什么">1.2 回顾：Original MeanFlow (MF) 是什么？</h3> <p>在深入主角 iMF 之前，我们需要快速回顾一下它的前身——Original MeanFlow [12]。</p> <ul> <li><strong>核心思想</strong>：传统的 Flow Matching 学习的是“瞬时速度” $v_t$。而 MeanFlow 提出，我们可以直接学习两点之间的<strong>平均速度（Average Velocity）</strong> $u$。</li> <li><strong>MeanFlow Identity</strong>：为了训练这个 $u$，作者推导出了一个微分关系（MeanFlow Identity），将未知的平均速度与瞬时速度联系起来，从而建立训练目标。 <alphaxiv-paper-citation title="MeanFlow Basics" page="1" first="In MF, instead" last="time steps."></alphaxiv-paper-citation></li> </ul> <hr/> <h2 id="第二部分original-meanflow-的两大痛点-problem-statement">第二部分：Original MeanFlow 的两大痛点 (Problem Statement)</h2> <p>虽然 MeanFlow 开启了单步生成的新思路，但作者发现它存在两个核心缺陷，这也是 iMF 要解决的问题：</p> <h3 id="痛点-1训练目标的非标准回归-dependent-training-target">痛点 1：训练目标的“非标准”回归 (Dependent Training Target)</h3> <ul> <li><strong>问题描述</strong>：在原始 MF 中，网络不仅要预测 $u$，而且训练的目标本身也依赖于网络的预测 $u_\theta$。这导致了一个“我预测我自己”的怪圈。</li> <li><strong>后果</strong>：这不仅仅是不稳定。更严重的是，为了计算 Loss，原始 MF 实际上把 conditional velocity ($e-x$) 作为了输入的一部分。这在回归问题中是不合法的，因为它泄露了部分答案，且引入了高方差。 <alphaxiv-paper-citation title="Issues" page="2" first="the training target" last="standard regression problem;"></alphaxiv-paper-citation></li> </ul> <h3 id="痛点-2僵化的引导策略-inflexible-guidance">痛点 2：僵化的引导策略 (Inflexible Guidance)</h3> <ul> <li><strong>问题描述</strong>：Classifier-Free Guidance (CFG) 是提升生成质量的神器。但原始 MF 在训练时必须<strong>固定</strong>一个 guidance scale $\omega$（比如固定为 7.0）。</li> <li><strong>后果</strong>：你训练完模型后，推理时就不能调整这个参数了。但我们在实践中知道，不同的步数、不同的模型大小，最佳的 $\omega$ 都是不同的。固定死参数牺牲了巨大的灵活性。 <alphaxiv-paper-citation title="Issues" page="2" first="MF handles the" last="sacrifices flexibility."></alphaxiv-paper-citation></li> </ul> <hr/> <h2 id="第三部分核心方法论-methodology---imf">第三部分：核心方法论 (Methodology - iMF)</h2> <p>iMF 通过三个维度的改进解决了上述问题。</p> <h3 id="31-改进一重构训练目标-refining-the-objective">3.1 改进一：重构训练目标 (Refining the Objective)</h3> <p>这是本论文最理论化的部分。</p> <ul> <li> <p><strong>Original MF 的做法</strong>： 它构建了一个复合函数 $V_\theta(z_t, e-x)$。注意看这个输入，它包含了 $e-x$。在 Flow Matching 中，$e-x$ 实际上就是我们要回归的目标（conditional velocity）。把目标作为输入的一部分，导致这个回归任务定义是不严谨的。</p> </li> <li> <p><strong>iMF 的做法（Legitimate Regression）</strong>： 作者将目标重写为一个标准的 $v$-loss（瞬时速度损失）。 关键公式如下： \(V_\theta(z_t) \triangleq u_\theta(z_t) + (t-r) \text{JVP}_{sg}(u_\theta; v_\theta)\)</p> <p>这里的核心变化是：<strong>$V_\theta$ 现在只接受 $z_t$ 作为输入</strong>，不再依赖 $e-x$。</p> <ul> <li><strong>$u_\theta$</strong>：网络预测的平均速度。</li> <li><strong>$v_\theta$</strong>：网络预测的瞬时速度（作为 JVP 的切向量）。</li> <li><strong>JVP (Jacobian-Vector Product)</strong>：雅可比向量积，用于处理微分项。</li> </ul> <p><strong>关键点</strong>：作者发现，不需要额外的网络来预测 $v_\theta$，只需要利用边界条件 $v(z_t, t) \equiv u(z_t, t, t)$，即直接复用 $u$ 网络在 $r=t$ 时的输出即可。这使得改进几乎是“免费”的。 <alphaxiv-paper-citation title="Refined Parameterization" page="4" first="Formally, we re-define" last="standard regression problem."></alphaxiv-paper-citation></p> </li> </ul> <h3 id="32-改进二灵活的引导-flexible-guidance-as-conditioning">3.2 改进二：灵活的引导 (Flexible Guidance as Conditioning)</h3> <p>既然不能固定 CFG scale $\omega$，那我们就把它变成一个<strong>条件（Condition）</strong>。</p> <ul> <li><strong>做法</strong>：就像模型需要输入时间步 $t$ 一样，我们把 $\omega$ 也作为一个输入传给网络。 \(u_\theta(z_t | c, \omega)\)</li> <li><strong>训练时</strong>：随机采样不同的 $\omega$ 值进行训练。</li> <li><strong>推理时</strong>：用户可以随意指定 $\omega$，甚至可以使用 <strong>CFG Interval</strong>（只在特定时间段开启引导）。</li> <li><strong>效果</strong>：图 4 (Figure 4) 展示了不同设置下最佳 $\omega$ 是变化的，iMF 完美适应了这一点。 <alphaxiv-paper-citation title="Flexible Guidance" page="5" first="we reformulate the" last="inference time."></alphaxiv-paper-citation></li> </ul> <h3 id="33-改进三in-context-conditioning-架构优化">3.3 改进三：In-Context Conditioning (架构优化)</h3> <p>为了处理这么多条件（时间 $t$、参考时间 $r$、类别 $c$、引导强度 $\omega$、引导区间 $t_{min}, t_{max}$），传统的 <code class="language-plaintext highlighter-rouge">adaLN-zero</code> 模块显得不堪重负且参数量巨大。</p> <ul> <li><strong>创新点</strong>：作者放弃了 <code class="language-plaintext highlighter-rouge">adaLN-zero</code>，改用 <strong>In-Context Conditioning</strong>。</li> <li><strong>具体实现</strong>：将所有的条件（$t, c, \omega$ 等）映射为 Token，直接拼接到图像 Latent Token 的序列前面。 <ul> <li>ImageNet 类别：8个 tokens</li> <li>时间/引导：各4个 tokens</li> </ul> </li> <li><strong>优势</strong>：模型参数量减少了 <strong>1/3</strong>（去掉了庞大的 adaLN 层），同时效果更好。 <alphaxiv-paper-citation title="In-Context Conditioning" page="6" first="Overall, our experiments" last="reduces model size"></alphaxiv-paper-citation></li> </ul> <hr/> <h2 id="第四部分实验结果-experiments">第四部分：实验结果 (Experiments)</h2> <p>让我们看看 iMF 到底有多强。</p> <ol> <li><strong>SOTA 性能</strong>： 在 ImageNet 256x256 上，训练只需一步生成的模型： <ul> <li><strong>Original MF</strong>: FID 3.43 (XL model)</li> <li><strong>iMF (Ours)</strong>: FID <strong>1.72</strong> (XL model)</li> <li><strong>提升</strong>：相对误差降低了 <strong>50%</strong>！这是非常巨大的进步。 <alphaxiv-paper-citation title="Main Results" page="7" first="Our iMF-XL/2" last="MF-XL/2's 3.43."></alphaxiv-paper-citation></li> </ul> </li> <li> <p><strong>不依赖蒸馏 (From Scratch)</strong>： 很多单步模型（如 Consistency Distillation）需要先训练一个教师模型再进行蒸馏。而 iMF 是<strong>完全从头训练 (Training from Scratch)</strong> 的。iMF 的效果甚至超过了很多基于蒸馏的方法。 <alphaxiv-paper-citation title="Comparison" page="8" first="Our iMF-XL/2" last="2.16"></alphaxiv-paper-citation></p> </li> <li><strong>消融实验 (Ablation Study)</strong>： <ul> <li>只改进 Loss：FID 从 6.17 降到 5.68。</li> <li>加上灵活 Guidance：FID 降到 4.57。</li> <li>加上 In-Context Conditioning：FID 降到 4.09。</li> <li>这就证明了每个模块都是有效的。 <alphaxiv-paper-citation title="Ablation" page="6" first="Replacing adaLN-zero" last="to 4.09."></alphaxiv-paper-citation></li> </ul> </li> </ol> <hr/> <h2 id="第五部分总结与思考-conclusion--discussion">第五部分：总结与思考 (Conclusion &amp; Discussion)</h2> <h3 id="总结">总结</h3> <ol> <li><strong>回归本质</strong>：iMF 将复杂的 MeanFlow 目标重新通过重参数化（Re-parameterization）变回了标准的、合法的回归问题，去除了输入中的作弊成分。</li> <li><strong>拥抱变化</strong>：通过将 CFG scale 作为条件输入，实现了推理时的灵活性。</li> <li><strong>架构减负</strong>：In-Context Conditioning 证明了简单的 Token 拼接比复杂的 adaLN 模块更高效。</li> </ol> <h3 id="思考">思考</h3> <ul> <li><strong>Q1</strong>: 为什么在 JVP 中，输入 conditional velocity ($e-x$) 会导致高方差？（提示：思考 $e-x$ 和边际速度 $v(z_t)$ 的区别）。</li> <li><strong>Q2</strong>: In-Context Conditioning 虽然减少了参数，但增加了序列长度（Sequence Length）。在处理极高分辨率图像时，这会是瓶颈吗？</li> </ul> <h3 id="结束语">结束语</h3> <p>这篇论文告诉我们，有时候“从头训练”一个极速生成模型是完全可行的，不需要依赖复杂的蒸馏流程。只要我们将<strong>优化目标定义得足够清晰</strong>，网络就能学会“一步到位”。</p> <hr/>]]></content><author><name></name></author><category term="AIGC"/><category term="AIGC"/><summary type="html"><![CDATA[[TOC]]]></summary></entry><entry><title type="html">SurveyOnWorldModelsForEmbodiedAI</title><link href="https://beyondpzk.github.io/blog/2025/SurveyOnWorldModelsForEmbodiedAI/" rel="alternate" type="text/html" title="SurveyOnWorldModelsForEmbodiedAI"/><published>2025-10-19T00:00:00+00:00</published><updated>2025-10-19T00:00:00+00:00</updated><id>https://beyondpzk.github.io/blog/2025/SurveyOnWorldModelsForEmbodiedAI</id><content type="html" xml:base="https://beyondpzk.github.io/blog/2025/SurveyOnWorldModelsForEmbodiedAI/"><![CDATA[<p>[TOC]</p> <h1 id="surveyonworldmodelsforembodiedai">SurveyOnWorldModelsForEmbodiedAI</h1> <p><a href="https://arxiv.org/abs/2510.16732">论文链接</a></p> <h1 id="具身智能中的世界模型-world-models-for-embodied-ai">具身智能中的世界模型 (World Models for Embodied AI)</h1> <hr/> <h2 id="引言与概念基础">引言与概念基础</h2> <p><strong>目标</strong>：理解什么是世界模型，它与传统视觉模型的区别，以及其在具身智能中的历史演变。</p> <h3 id="1-人类认知的启示">1. 人类认知的启示</h3> <p>我们先思考一个认知科学的问题：人类是如何在复杂的环境中行动的？当我们走在一个拥挤的街道上，我们不仅是在“看”，我们还在“预测”。如果我们快步走，我们知道前面的行人可能会避让；如果我们撞到障碍物，我们知道会发生什么。</p> <p>认知科学表明，人类通过整合感官输入构建世界的内部模型。这些模型不仅预测和模拟未来事件，还塑造感知并指导行动 <alphaxiv-paper-citation title="Cognitive Science" page="1" first="Cognitive science suggests" last="guide action"></alphaxiv-paper-citation>。这种 <strong>“心中有数”</strong> 的能力，就是我们今天要讲的“世界模型”的雏形。</p> <h3 id="2-定义具身智能中的世界模型">2. 定义：具身智能中的世界模型</h3> <p>那么，在AI领域，特别是具身智能（Embodied AI）中，世界模型到底是什么？</p> <p>首先，具身智能要求代理感知复杂的多模态环境，在其中行动，并预测其行动将如何改变未来的世界状态 <alphaxiv-paper-citation title="Embodied AI Goal" page="1" first="EMBODIED AI aims" last="future world states"></alphaxiv-paper-citation>。</p> <p>在这个背景下，世界模型的核心定义是：一种<strong>内部模拟器</strong>（Internal Simulator）。它能够捕捉环境的动态变化，支持前向（Forward）和反事实（Counterfactual）的推演，从而服务于感知、预测和决策 <alphaxiv-paper-citation title="Core Definition" page="1" first="World models serve" last="decision making"></alphaxiv-paper-citation>。</p> <p><strong>关键区别点</strong>：请大家注意，这与我们常见的计算机视觉模型（如目标检测、语义分割）不同。世界模型侧重于生成可操作的预测，将其与静态场景描述符或纯生成视觉模型区分开来 <alphaxiv-paper-citation title="Distinction" page="1" first="This survey focuses" last="controllable dynamics."></alphaxiv-paper-citation>。</p> <h3 id="3-历史演变从rl到生成式ai">3. 历史演变：从RL到生成式AI</h3> <p>世界模型的发展并非一蹴而就，它经历了几个重要阶段：</p> <ol> <li><strong>基于模型的强化学习 (Model-based RL)</strong>：早期研究根植于此，利用潜在的状态转移模型来提高样本效率和规划性能 <alphaxiv-paper-citation title="Early Origins" page="1" first="early AI research" last="planning performance"></alphaxiv-paper-citation>。</li> <li><strong>里程碑式工作</strong>：Ha 和 Schmidhuber 在2018年的开创性工作正式确立了“世界模型”这一术语。随后，Dreamer 系列模型进一步强调了学习到的动力学如何驱动基于想象的策略优化 <alphaxiv-paper-citation title="Seminal Works" page="1" first="seminal work of" last="policy optimization."></alphaxiv-paper-citation>。</li> <li><strong>通用模拟器时代</strong>：最近，随着大规模生成建模（如Sora, V-JEPA）的进步，世界模型已扩展到通用环境模拟器，不仅限于策略学习，还能进行高保真的未来预测 <alphaxiv-paper-citation title="Recent Expansion" page="1" first="More recently, advances" last="future prediction"></alphaxiv-paper-citation>。</li> </ol> <hr/> <h2 id="核心分类学一-功能性与时间建模">核心分类学（一）—— 功能性与时间建模</h2> <p><strong>目标</strong>：深入解析世界模型的分类框架，重点讲解功能定位和时间维度上的预测机制。</p> <h3 id="1-综述提出的统一框架">1. 综述提出的统一框架</h3> <p>为了解决领域内术语混乱的问题，采用一种新的三轴分类法：(1) 功能性，(2) 时间建模，(3) 空间表示 <alphaxiv-paper-citation title="Taxonomy" page="1" first="propose a three-axis" last="Rendering Representation."></alphaxiv-paper-citation>。这不仅是分类工具，更是设计世界模型时的三个核心维度。</p> <h3 id="2-维度一功能性-functionality">2. 维度一：功能性 (Functionality)</h3> <p>根据设计目的，世界模型主要分为两类：</p> <ul> <li><strong>决策耦合型 (Decision-Coupled)</strong>： <ul> <li>这类模型通常与具体的控制任务紧密结合。它们不仅预测未来，还直接参与策略（Policy）的训练。</li> <li>典型代表是Dreamer系列。其核心在于利用模型进行“想象中”的试错，从而减少在真实环境中的风险和采样成本。</li> </ul> </li> <li><strong>通用目的型 (General-Purpose)</strong>： <ul> <li>这类模型更像是一个纯粹的物理引擎或视频生成器。它们的目标是尽可能真实地模拟环境，而不一定绑定特定的下游任务。</li> <li>例如Sora或V-JEPA，它们展示了强大的环境理解能力，可以作为通用的基础模型服务于各种下游应用。</li> </ul> </li> </ul> <h3 id="3-维度二时间建模-temporal-modeling">3. 维度二：时间建模 (Temporal Modeling)</h3> <p>环境是动态的，捕捉这种动态性至关重要。忠实地捕捉环境动态需要解决状态的时间演化问题 <alphaxiv-paper-citation title="Dynamics Requirement" page="1" first="Faithfully capturing environment" last="of scenes"></alphaxiv-paper-citation>。目前主流的方法有两种：</p> <ul> <li><strong>序列模拟与推理 (Sequential Simulation and Inference)</strong>： <ul> <li>这是最直观的方法。模型一步步地推演：$t \to t+1 \to t+2$。</li> <li>这种方法符合因果律，非常适合实时控制和规划。但它面临的主要挑战是长视野推演中的误差累积 <alphaxiv-paper-citation title="Error Accumulation" page="1" first="Long-horizon rollouts" last="policy imagination"></alphaxiv-paper-citation>。如果第一步预测偏了一点，第100步可能就完全错误了。</li> </ul> </li> <li><strong>全局差异预测 (Global Difference Prediction)</strong>： <ul> <li>有些模型不进行逐帧预测，而是预测一个较长时间段内的整体变化。这种方法在处理非因果任务或视频插帧时较为常见，但在实时控制中应用相对较少。</li> </ul> </li> </ul> <hr/> <h2 id="核心分类学二-空间表示">核心分类学（二）—— 空间表示</h2> <p><strong>目标</strong>：这是最“硬核”的部分。探讨如何将复杂的3D物理世界压缩进神经网络中。</p> <h3 id="1-为什么空间表示如此重要">1. 为什么空间表示如此重要？</h3> <p>很多早期的世界模型只是在处理2D图像。但是，粗糙或以2D为中心的布局提供的几何细节不足以处理遮挡、物体恒常性和几何感知规划等挑战 <alphaxiv-paper-citation title="2D Limitations" page="1" first="coarse or 2D-centric" last="geometry-aware planning."></alphaxiv-paper-citation>。</p> <p>如果机器人要抓取杯子，它必须知道杯子的3D形状和位置，而不仅仅是像素颜色。</p> <h3 id="2-四种主流的空间表示法">2. 四种主流的空间表示法</h3> <p>根据这篇综述，我们将空间表示分为四类 <alphaxiv-paper-citation title="Spatial Taxonomy" page="1" first="Spatial Representation, Global" last="Rendering Representation."></alphaxiv-paper-citation>：</p> <ol> <li><strong>全局潜在向量 (Global Latent Vector)</strong>： <ul> <li><strong>原理</strong>：将整个图像压缩为一个极低维的向量（如VAE的瓶颈层）。</li> <li><strong>优点</strong>：计算极快，适合快速规划。</li> <li><strong>缺点</strong>：丢失了大量空间细节，无法处理复杂的物体交互。</li> </ul> </li> <li><strong>Token 特征序列 (Token Feature Sequence)</strong>： <ul> <li><strong>原理</strong>：类似于Transformer处理语言，将图像切成Patch，变成一串Token。</li> <li><strong>优点</strong>：利用了Transformer强大的注意力机制，能捕捉长距离依赖。</li> <li><strong>缺点</strong>：计算量大，且Token序列本身缺乏显式的3D几何结构。</li> </ul> </li> <li><strong>空间潜在网格 (Spatial Latent Grid)</strong>： <ul> <li><strong>原理</strong>：保留特征图的空间结构（如 $H \times W \times C$ 的特征图或3D体素）。</li> <li><strong>优点</strong>：保留了局部性，对于卷积操作非常友好。相比于2D布局，体积或3D占用表示提供了更好的几何结构来支持预测和控制 <alphaxiv-paper-citation title="3D Benefits" page="1" first="volumetric or 3D" last="and control."></alphaxiv-paper-citation>。</li> </ul> </li> <li><strong>分解式渲染表示 (Decomposed Rendering Representation)</strong>： <ul> <li><strong>原理</strong>：这是最前沿的方向。结合了NeRF或3D Gaussian Splatting等图形学技术，将场景分解为对象、背景、光照等。</li> <li><strong>意义</strong>：这使得世界模型不仅能预测“图像”，还能预测“3D结构”，实现了真正的物理一致性。</li> </ul> </li> </ol> <h3 id="3-总结">3. 总结</h3> <p>空间表示的选择往往决定了模型的上限。如果你只是想预测视频下一帧，Token序列可能够了；但如果你要让机器人做精细操作，空间潜在网格或分解式渲染可能是必须的。</p> <hr/> <h2 id="应用领域与评估体系">应用领域与评估体系</h2> <p><strong>目标</strong>：了解世界模型在不同领域的实际表现，以及我们如何衡量它的好坏。</p> <h3 id="1-三大应用领域">1. 三大应用领域</h3> <p>综述系统化了跨机器人、自动驾驶和通用视频设置的数据资源和指标 <alphaxiv-paper-citation title="Domains" page="1" first="Systematize data resources" last="video settings"></alphaxiv-paper-citation>。</p> <ul> <li><strong>机器人 (Robotics)</strong>： <ul> <li>关注点：操作（Manipulation）和移动（Locomotion）。</li> <li>难点：接触动力学（Contact Dynamics）很难模拟。</li> </ul> </li> <li><strong>自动驾驶 (Autonomous Driving)</strong>： <ul> <li>关注点：安全性和长尾场景生成。</li> <li>应用：生成事故场景来训练感知算法，或者直接作为驾驶策略的大脑。有关自动驾驶的专门综述也有很多.</li> </ul> </li> <li><strong>通用视频 (General Video)</strong>： <ul> <li>关注点：高分辨率、高帧率、视觉逼真度。</li> <li>现状：Sora等模型展示了惊人的物理一致性涌现能力。</li> </ul> </li> </ul> <h3 id="2-评估指标不仅仅是psnr">2. 评估指标：不仅仅是PSNR</h3> <p>我们要如何评价一个世界模型的好坏？仅仅看生成的视频清不清晰是不够的。</p> <ul> <li><strong>像素预测质量 (Pixel Prediction Quality)</strong>： <ul> <li>指标：PSNR, SSIM, FID。</li> <li>局限：一个模糊但物理正确的预测，可能比一个清晰但违反物理定律的预测得分更低。</li> </ul> </li> <li><strong>状态级理解 (State-level Understanding)</strong>： <ul> <li>指标：预测的物体位置、速度误差。</li> <li>适用：仅适用于有Ground Truth状态的仿真环境。</li> </ul> </li> <li><strong>任务性能 (Task Performance)</strong>： <ul> <li><strong>这是终极标准</strong>。如果一个世界模型能帮助强化学习Agent拿到更高的分数，那么即便它生成的画面像“马赛克”，它也是一个好的世界模型。</li> </ul> </li> </ul> <hr/> <h2 id="挑战未来与总结">挑战、未来与总结</h2> <p><strong>目标</strong>：探讨当前技术的瓶颈，激发兴趣。</p> <h3 id="1-关键开放挑战">1. 关键开放挑战</h3> <p>根据综述，目前主要面临三大挑战 <alphaxiv-paper-citation title="Open Challenges" page="1" first="distill key open" last="error accumulation."></alphaxiv-paper-citation>：</p> <ol> <li><strong>数据与评估的缺失</strong>： <ul> <li>我们需要统一的数据集，以及能够评估<strong>物理一致性</strong>而非仅仅是像素保真度的指标 <alphaxiv-paper-citation title="Metric Challenge" page="1" first="evaluation metrics that" last="pixel fidelity"></alphaxiv-paper-citation>。目前的指标太偏向视觉效果了。</li> </ul> </li> <li><strong>性能与效率的权衡</strong>： <ul> <li>这是一个经典的工程问题：模型性能与实时控制所需的计算效率之间的权衡 <alphaxiv-paper-citation title="Efficiency Tradeoff" page="1" first="trade-off between model" last="real-time control"></alphaxiv-paper-citation>。Sora生成一分钟视频可能需要几十分钟渲染，这显然不能用于控制每秒需要做10次决策的机器人。</li> </ul> </li> <li><strong>长视野一致性</strong>： <ul> <li>这是核心建模难点：实现长视野的时间一致性，同时减轻误差累积 <alphaxiv-paper-citation title="Consistency Challenge" page="1" first="modeling difficulty of" last="error accumulation."></alphaxiv-paper-citation>。如何让模型在“想象”未来10秒时，不会把车子“想”没了，或者把路“想”歪了？</li> </ul> </li> </ol> <h3 id="2-未来展望">2. 未来展望</h3> <ul> <li><strong>物理感知的增强</strong>：未来的模型会更多地结合3D几何先验（如3D Gaussians）。</li> <li><strong>多模态融合</strong>：不仅仅是视觉，还要结合触觉、听觉甚至语言。</li> <li><strong>Sim-to-Real</strong>：如何将在模拟器中训练的世界模型无缝迁移到真实机器人上。</li> </ul> <h3 id="3-总结-1">3. 总结</h3> <p>今天我们系统地学习了具身智能中的世界模型。我们从认知科学的源头出发，了解了它作为“内部模拟器”的本质。我们通过功能性、时间建模和空间表示这三个轴，解构了当前最先进的模型架构。</p> <p>最后，我想引用综述中的观点：世界模型不仅是预测未来的工具，更是通向通用人工智能（AGI）的重要基石。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[[TOC]]]></summary></entry><entry><title type="html">MeanFlows</title><link href="https://beyondpzk.github.io/blog/2025/MeanFlows/" rel="alternate" type="text/html" title="MeanFlows"/><published>2025-05-19T19:52:00+00:00</published><updated>2025-05-19T19:52:00+00:00</updated><id>https://beyondpzk.github.io/blog/2025/MeanFlows</id><content type="html" xml:base="https://beyondpzk.github.io/blog/2025/MeanFlows/"><![CDATA[<p>[TOC]</p> <h1 id="meanflows">meanflows</h1> <ul> <li><a href="https://arxiv.org/abs/2505.13447">paper地址</a></li> </ul> <p>我的理解: 我觉得meansflows是针对 rectified flow的弱点来的,尤其是训练时的弱点.</p> <p>基于论文内容，MeanFlow 的训练目标、Loss 函数以及 Ground Truth (GT) 的构建方式非常独特。它不像传统的监督学习那样直接有一个固定的“标签”，而是通过一个<strong>微分恒等式</strong>构造了一个<strong>自洽（Self-consistent）的回归目标</strong>。</p> <p>以下是详细的数学原理和步骤解析：</p> <h3 id="1-核心理论基础meanflow-identity">1. 核心理论基础：MeanFlow Identity</h3> <p>理解 Loss 的前提是理解论文推导出的核心公式——<strong>MeanFlow Identity（平均流恒等式）</strong>。</p> <ul> <li><strong>定义：</strong> <ul> <li>$v(z_t, t)$：瞬时速度（Instantaneous Velocity）. (我的理解是最终要学到的速度场 $v$ 在 $t$ 时刻, 位置 $z_t$ 时的值.)</li> <li>$u(z_t, r, t)$：平均速度（Average Velocity），即从时间 $r$ 到 $t$ 的位移除以时间间隔。</li> </ul> </li> <li> <p><strong>恒等式：</strong> 论文推导出 $u$ 和 $v$ 必须满足以下微分关系（论文公式 6）： \(\frac{d}{dt} u(z_t, r, t) = \frac{v(z_t, t) - u(z_t, r, t)}{t - r}\)</p> <p><strong>移项后，我们可以得到 $u$ 的表达式：</strong> \(u(z_t, r, t) = v(z_t, t) - (t - r) \underbrace{\frac{d}{dt} u(z_t, r, t)}_{\text{全导数}}\)</p> <p><strong>这个移项后的公式，就是训练目标的来源。</strong></p> </li> </ul> <hr/> <h3 id="2-训练目标-regression-target-与-gt-构建">2. 训练目标 (Regression Target) 与 GT 构建</h3> <p>MeanFlow 的训练本质上是训练神经网络 $u_\theta$ 去拟合上述恒等式的右边。</p> <h4 id="ground-truth-gt-的构成u_texttgt">Ground Truth (GT) 的构成：$u_{\text{tgt}}$</h4> <p>训练时的“目标值” $u_{\text{tgt}}$ 并不是预先计算好的固定值，而是由<strong>已知物理量</strong>和<strong>网络当前的导数预测</strong>动态组合而成的。</p> <p>根据论文公式 (10) 和 (11)，目标值 $u_{\text{tgt}}$ 定义为：</p> \[u_{\text{tgt}} = \underbrace{v_t}_{\text{数据决定的瞬时速度}} - (t - r) \times \underbrace{\left( v_t \cdot \nabla_z u_\theta + \partial_t u_\theta \right)}_{\text{网络预测的全导数 (JVP)}}\] <p>这里包含两个关键部分：</p> <ol> <li><strong>$v_t$ (瞬时速度，真正的 Ground Truth 来源)：</strong> <ul> <li>这是唯一来自数据的外部信号。</li> <li>在 Flow Matching 框架下，对于一条直线路径（Straight Path），给定数据点 $x$（图片）和噪声 $\epsilon$，在时间 $t$ 的位置是 $z_t = (1-t)x + t\epsilon$。</li> <li>此时的<strong>条件瞬时速度</strong>是已知的：<strong>$v_t = \epsilon - x$</strong> (或者 $x - \epsilon$，取决于具体定义，论文中 $v_t = \epsilon - x$ 对应 $z_1=\epsilon, z_0=x$)。</li> <li><strong>注意：</strong> 这个 $v_t$ 不需要模型预测，是直接算出来的。</li> </ul> </li> <li><strong>全导数项 (网络自身的性质)：</strong> <ul> <li>$\frac{d}{dt} u$ 被展开为 $\frac{\partial u}{\partial z} \frac{dz}{dt} + \frac{\partial u}{\partial t}$。</li> <li>其中 $\frac{dz}{dt}$ 就是 $v_t$。</li> <li>这一项通过 <strong>Jacobian-Vector Product (JVP)</strong> 计算。即计算网络输出 $u_\theta$ 对输入 $(z, r, t)$ 的导数，并沿着向量 $(v_t, 0, 1)$ 方向投影。</li> </ul> </li> </ol> <h4 id="stop-gradient-停止梯度">Stop-Gradient (停止梯度)</h4> <p>为了避免训练不稳定和二阶导数计算（Double Backpropagation），论文对目标值使用了 <strong>Stop-Gradient (sg)</strong> 操作： \(\text{Target} = \text{sg}(u_{\text{tgt}})\) 这意味着在计算 Loss 对网络参数 $\theta$ 的梯度时，<strong>不</strong>对 $u_{\text{tgt}}$ 里的 $u_\theta$ 求导。目标值被视为一个常数。</p> <hr/> <h3 id="3-loss-函数">3. Loss 函数</h3> <p>论文使用的 Loss 函数形式如下（公式 9）：</p> \[\mathcal{L}(\theta) = \mathbb{E}_{t, r, x, \epsilon} \left[ \| u_\theta(z_t, r, t) - \text{sg}(u_{\text{tgt}}) \|^2 \right]\] <p><strong>详细展开后：</strong></p> \[\mathcal{L}(\theta) = \| u_\theta(z_t, r, t) - \text{sg}\left( v_t - (t-r)(\underbrace{v_t \cdot \nabla_z u_\theta + \partial_t u_\theta}_{\text{JVP}}) \right) \|^2\] <p><strong>Loss 的直观解释：</strong></p> <ul> <li>网络预测的平均速度 $u_\theta$，应该等于“瞬时速度 $v_t$”减去“因时间变化导致的修正项”。</li> <li>如果 $t=r$，则 $t-r=0$，Loss 变为 $| u_\theta - v_t |^2$，这退化为标准的 Flow Matching（平均速度等于瞬时速度）。</li> <li>如果 $t \neq r$，模型就被迫学习如何根据当前的瞬时速度和变化率，去推断跨越时间段的平均速度。</li> </ul> <p><strong>加权 Loss (Adaptive Weighting):</strong> 在实际训练中（Section 4.3），为了平衡不同时间步的学习难度，作者使用了一个自适应权重 $w$： \(\mathcal{L}_{\text{final}} = w \cdot \| \Delta \|^2\) 其中 $w = \frac{1}{| \Delta |^2 + c}$（$c$ 是小常数），这使得 Loss 表现得像 Pseudo-Huber Loss，能提高训练稳定性。</p> <hr/> <h3 id="4-训练流程总结-step-by-step">4. 训练流程总结 (Step-by-Step)</h3> <p>根据论文 Algorithm 1，一步训练的具体操作如下：</p> <ol> <li><strong>采样时间：</strong> 随机采样两个时间点 $t$ 和 $r$（通常 $t, r \in [0, 1]$，且 $t &gt; r$）。</li> <li><strong>采样数据：</strong> 采样一张真实图片 $x$ 和高斯噪声 $\epsilon$。</li> <li><strong>构造输入 $z_t$：</strong> 使用线性插值构造当前时刻的噪声图：$z_t = (1-t)x + t\epsilon$。</li> <li><strong>计算瞬时速度 $v_t$：</strong> 直接计算 $v_t = \epsilon - x$。这是物理真值。</li> <li><strong>前向传播与 JVP：</strong> <ul> <li>将 $(z_t, r, t)$ 输入网络 $u_\theta$。</li> <li>同时利用自动微分框架（如 PyTorch 的 <code class="language-plaintext highlighter-rouge">torch.func.jvp</code>）计算全导数 $dudt = \text{jvp}(u_\theta, (z_t, r, t), (v_t, 0, 1))$。</li> </ul> </li> <li><strong>构建目标 $u_{\text{tgt}}$：</strong> 计算 $u_{\text{tgt}} = v_t - (t - r) \times dudt$。</li> <li><strong>计算 Loss：</strong> 计算预测值 $u$ 和目标值 $\text{sg}(u_{\text{tgt}})$ 之间的加权平方误差。</li> <li><strong>反向传播：</strong> 更新网络参数 $\theta$。</li> </ol> <h3 id="5-特殊情况带-cfg-的-gt-构建">5. 特殊情况：带 CFG 的 GT 构建</h3> <p>如果使用 Classifier-Free Guidance (CFG)，GT 会发生变化（Section 4.2）：</p> <ul> <li><strong>目标场变化：</strong> 目标不再是单纯的 $v_t$，而是混合了 CFG 权重的速度场。</li> <li><strong>公式：</strong> $u_{\text{tgt}}$ 中的 $v_t$ 被替换为 $\tilde{v}<em>t = \omega v_t + (1-\omega) u</em>{\theta}(z_t, t, t)$。</li> <li>这意味着在训练阶段，模型不仅要拟合物理速度，还要拟合“被引导后”的速度场，从而使得推理时只需 1 步即可完成 CFG 生成。</li> </ul> <p>下面是详细解释.</p> <h1 id="cfg-的gt构建">CFG 的GT构建</h1> <p>关于 <strong>带 CFG（Classifier-Free Guidance）的 Ground Truth (GT) 构建</strong>，这是 MeanFlow 论文中非常精彩的一部分，因为它巧妙地解决了传统 CFG 推理速度慢的问题。</p> <p>以下是关于带 CFG 的 GT 构建的详细完整解读：</p> <hr/> <h3 id="1-核心思想把-cfg-内化到训练目标中">1. 核心思想：把 CFG “内化”到训练目标中</h3> <ul> <li> <p><strong>传统 CFG 的痛点：</strong> 在采样（推理）阶段，传统方法需要计算公式：$\text{Output} = \text{Uncond} + \omega (\text{Cond} - \text{Uncond})$。这意味着每生成一步，都要跑两次网络（一次有条件，一次无条件），导致计算量翻倍（2-NFE）。</p> </li> <li> <p><strong>MeanFlow 的解决方案：</strong> 作者定义了一个新的<strong>混合物理场</strong> $v_{\text{cfg}}$。既然我们知道推理时想要的是混合后的结果，不如直接训练网络去预测这个<strong>已经混合好的平均速度</strong> $u_{\text{cfg}}$。这样推理时只需要跑一次网络（1-NFE）。</p> </li> </ul> <hr/> <h3 id="2-构造新的混合瞬时速度-tildev_t">2. 构造新的“混合瞬时速度” ($\tilde{v}_t$)</h3> <p>在普通训练中，瞬时速度的 GT 是 $v_t = \epsilon - x$。 在 CFG 训练中，我们需要构造一个<strong>混合了引导尺度的瞬时速度</strong> $\tilde{v}_t$ 作为基础。</p> <p>根据论文公式 (13) 和 (19)，新的瞬时速度定义为：</p> \[\tilde{v}_t = \omega \cdot v_t + (1 - \omega) \cdot u_\theta(z_t, t, t)\] <p>这里包含三部分：</p> <ol> <li><strong>$\omega$ (Guidance Scale)：</strong> 引导强度（例如 2.0 或 7.5），这是训练时的超参数。</li> <li><strong>$v_t$ (Conditional Velocity)：</strong> 数据决定的物理真值（即 $\epsilon - x$）。这代表了“有条件”的理想方向。</li> <li><strong>$u_\theta(z_t, t, t)$ (Unconditional Velocity)：</strong> <strong>这是关键点。</strong> <ul> <li>当 $r=t$ 时，平均速度等于瞬时速度。</li> <li>这里使用<strong>模型自己预测的</strong>、在 $t$ 时刻的瞬时速度，来近似“无条件”的速度场。</li> <li>注意：这一项通常是把类别条件置空（Drop Condition）后得到的输出。</li> </ul> </li> </ol> <p><strong>直观理解：</strong> 训练目标不再是纯粹的物理真实速度，而是“物理真实速度”和“模型自己认为的无条件速度”的一个线性组合。</p> <hr/> <h3 id="3-构造带-cfg-的训练目标-u_texttgt">3. 构造带 CFG 的训练目标 ($u_{\text{tgt}}$)</h3> <p>有了上面的 $\tilde{v}<em>t$，我们再次利用 <strong>MeanFlow Identity</strong> 来构造最终的回归目标 $u</em>{\text{tgt}}$。</p> <p>公式 (18) 如下：</p> \[u_{\text{tgt}} = \tilde{v}_t - (t - r) \times \underbrace{\left( \tilde{v}_t \cdot \nabla_z u^{\text{cfg}}_\theta + \partial_t u^{\text{cfg}}_\theta \right)}_{\text{基于 } \tilde{v}_t \text{ 计算的 JVP}}\] <p><strong>具体步骤变化：</strong></p> <ol> <li><strong>计算 JVP 时：</strong> 投影向量不再是 $(v_t, 0, 1)$，而是变成 $(\tilde{v}_t, 0, 1)$。这意味着我们计算的是沿着 CFG 混合轨迹的导数。</li> <li><strong>计算目标值时：</strong> 基准速度变成了 $\tilde{v}_t$。</li> </ol> <hr/> <h3 id="4-进阶技巧improved-cfg-appendix-b1">4. 进阶技巧：Improved CFG (Appendix B.1)</h3> <p>论文在附录中提出了一个改进版（Improved CFG），引入了一个混合参数 $\kappa$，进一步提升了效果。</p> <p><strong>问题：</strong> 基础版公式只利用了“无条件”的模型输出。 <strong>改进：</strong> 作者认为应该同时混合“有条件”和“无条件”的模型输出到目标中。</p> <p><strong>改进后的混合瞬时速度公式 (Eq. 21)：</strong></p> \[\tilde{v}_t = \omega (\epsilon - x) + \kappa \cdot u_\theta(z_t, t, t | c) + (1 - \omega - \kappa) \cdot u_\theta(z_t, t, t | \emptyset)\] <ul> <li> <table> <tbody> <tr> <td>**$u_\theta(z_t, t, t</td> <td>c)$：** 模型预测的<strong>有条件</strong>瞬时速度。</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>**$u_\theta(z_t, t, t</td> <td>\emptyset)$：** 模型预测的<strong>无条件</strong>瞬时速度。</td> </tr> </tbody> </table> </li> <li><strong>$\kappa$：</strong> 一个新的超参数，用于调节混合比例。</li> </ul> <p><strong>为什么这样做？</strong> 这相当于一种<strong>自蒸馏 (Self-Distillation)</strong>。模型在训练过程中，不仅在学习拟合数据（$\epsilon - x$），还在学习拟合“自己之前的预测混合”。这使得模型预测的平均速度场 $u_{\text{cfg}}$ 更加平滑、一致，从而在单步生成时画质更好。</p> <hr/> <h3 id="5-总结带-cfg-的训练与推理">5. 总结：带 CFG 的训练与推理</h3> <h4 id="训练阶段-training">训练阶段 (Training)</h4> <ol> <li>随机采样 $t, r, x, \epsilon$。</li> <li>计算物理速度 $v_t = \epsilon - x$。</li> <li>让模型预测当前的瞬时速度 $u_{\text{inst}} = u_\theta(z_t, t, t)$（可能包含有条件和无条件两次前向）。</li> <li><strong>合成目标速度场：</strong> $\tilde{v}<em>t = \text{Mix}(v_t, u</em>{\text{inst}}, \omega)$。</li> <li><strong>计算 JVP：</strong> 基于 $\tilde{v}_t$ 计算全导数。</li> <li><strong>计算 Loss：</strong> $| u_\theta(z_t, r, t) - \text{sg}(u_{\text{tgt}}) |^2$。</li> </ol> <h4 id="推理阶段-inference--sampling">推理阶段 (Inference / Sampling)</h4> <p><strong>极度简单：</strong> 因为模型 $u_\theta$ 已经学会了预测“混合后的平均速度”，所以推理时<strong>不需要</strong>做任何 CFG 公式计算，也不需要跑两次模型。</p> \[z_0 = z_1 - (1 - 0) \cdot u_\theta(z_1, 0, 1 | c)\] <p><strong>只需 1 次 NFE，就能得到带 Guidance 效果的高质量图像。</strong></p> <h2 id="平均流恒等式的推导">平均流恒等式的推导</h2> <p>接下来详细推导 <strong>MeanFlow Identity（平均流恒等式）</strong>。这是整篇论文的理论基石，它建立起了“平均速度”与“瞬时速度”之间的微分关系。(我把它理解为宏观速度场与微观粒子间速度的关系)</p> <hr/> <h3 id="1-符号定义">1. 符号定义</h3> <p>首先，我们需要明确几个核心物理量的定义：</p> <ol> <li><strong>$z_t$</strong>：在时间 $t$ 时刻的状态（即数据点或噪声点的位置）。</li> <li><strong>$v(z_t, t)$</strong>：<strong>瞬时速度 (Instantaneous Velocity)</strong>。 <ul> <li>根据定义，它是位置随时间的导数： \(\frac{d z_t}{dt} = v(z_t, t)\)</li> </ul> </li> <li><strong>$u(z_t, r, t)$</strong>：<strong>平均速度 (Average Velocity)</strong>。 **(注意再注意,这个量是作者定义的!!!) ** <ul> <li>定义为从时间 $r$ 到时间 $t$ 的位移，除以时间间隔 $(t-r)$。</li> <li>数学表达式（论文公式 3）： \(u(z_t, r, t) \triangleq \frac{1}{t - r} \int_r^t v(z_\tau, \tau) d\tau\)</li> </ul> </li> </ol> <p>虽然这里是定义出来的,但实际上也确实是这样.</p> <hr/> <h3 id="2-推导过程">2. 推导过程</h3> <p>我们的目标是求出 $u(z_t, r, t)$ 关于时间 $t$ 的全导数 $\frac{d}{dt}$。</p> <h4 id="第一步消除分母转化为积分形式">第一步：消除分母，转化为积分形式</h4> <p>为了方便求导，我们将平均速度的定义式（公式 3）两边同时乘以 $(t - r)$，得到论文中的公式 (4)：</p> \[(t - r) \cdot u(z_t, r, t) = \int_r^t v(z_\tau, \tau) d\tau\] <h4 id="第二步对两边同时关于-t-求全导数">第二步：对两边同时关于 $t$ 求全导数</h4> <p>现在，我们对等式两边分别进行 $\frac{d}{dt}$ 运算。注意，这里我们将 $r$ 视为一个固定的起始时间，不随 $t$ 变化（即 $\frac{dr}{dt} = 0$）。</p> <p><strong>左边求导 (LHS)：应用乘法法则 (Product Rule)</strong> 左边是两个关于 $t$ 的函数的乘积：$f(t) = (t-r)$ 和 $g(t) = u(z_t, r, t)$。 \(\frac{d}{dt} \left[ (t - r) \cdot u(z_t, r, t) \right] = \underbrace{\frac{d}{dt}(t - r)}_{1} \cdot u + (t - r) \cdot \frac{d}{dt} u\) \(\text{LHS} = u(z_t, r, t) + (t - r) \frac{d}{dt} u(z_t, r, t)\)</p> <p><strong>右边求导 (RHS)：应用微积分基本定理 (Fundamental Theorem of Calculus)</strong> 右边是一个变上限积分函数。根据微积分基本定理，对积分上限 $t$ 求导，结果就是被积函数在 $t$ 处的值。 \(\frac{d}{dt} \left[ \int_r^t v(z_\tau, \tau) d\tau \right] = v(z_t, t)\) \(\text{RHS} = v(z_t, t)\)</p> <h4 id="第三步联立等式与整理">第三步：联立等式与整理</h4> <p>将左边和右边相等：</p> \[u(z_t, r, t) + (t - r) \frac{d}{dt} u(z_t, r, t) = v(z_t, t)\] <p>现在，我们将这一项移项，把 $\frac{d}{dt} u$ 留在左边，或者整理成论文公式 (6) 的形式：</p> \[(t - r) \frac{d}{dt} u(z_t, r, t) = v(z_t, t) - u(z_t, r, t)\] <p>进而得到 <strong>MeanFlow Identity</strong>：</p> \[\frac{d}{dt} u(z_t, r, t) = \frac{v(z_t, t) - u(z_t, r, t)}{t - r}\] <hr/> <h3 id="3-深入解析全导数-fracddt-u-的展开">3. 深入解析：全导数 $\frac{d}{dt} u$ 的展开</h3> <p>在实际训练神经网络时，我们需要计算左边的 $\frac{d}{dt} u$。这是一个<strong>全导数 (Total Derivative)</strong>，因为 $u$ 依赖于 $z_t$，$r$ 和 $t$，而 $z_t$ 本身又随 $t$ 变化。</p> <p>根据链式法则（Chain Rule）：</p> \[\frac{d}{dt} u(z_t, r, t) = \frac{\partial u}{\partial z_t} \cdot \frac{d z_t}{dt} + \frac{\partial u}{\partial r} \cdot \frac{d r}{dt} + \frac{\partial u}{\partial t} \cdot \frac{d t}{dt}\] <p>代入已知条件：</p> <ol> <li>$\frac{d z_t}{dt} = v(z_t, t)$ （这是瞬时速度的定义）。</li> <li>$\frac{d r}{dt} = 0$ （在求导过程中，$r$ 被视为独立变量）。</li> <li>$\frac{d t}{dt} = 1$。</li> </ol> <p>于是，全导数展开为：</p> \[\frac{d}{dt} u(z_t, r, t) = \underbrace{v(z_t, t) \cdot \nabla_z u}_{\text{对 z 的偏导与速度的点积}} + \underbrace{\frac{\partial u}{\partial t}}_{\text{对 t 的偏导}}\] <p>这正是论文中提到的 <strong>Jacobian-Vector Product (JVP)</strong> 的来源。 在代码实现中，我们计算函数 $u$ 对输入 $(z, r, t)$ 的 Jacobian 矩阵与向量 $(v, 0, 1)$ 的乘积。</p> <hr/> <h3 id="4-推导的意义">4. 推导的意义</h3> <p>这个推导之所以重要，是因为它完成了一个看似不可能的任务：</p> <ol> <li><strong>消除了积分：</strong> 原始定义（公式 3）包含一个积分 $\int$，这在训练中是无法直接计算的（太慢）。</li> <li><strong>建立了局部联系：</strong> 推导出的恒等式（公式 6）只包含<strong>当前时刻</strong>的变量（$u, v$）和<strong>导数</strong>。</li> <li><strong>可优化目标：</strong> 它把一个积分问题转化为了一个微分方程求解问题。神经网络只需要去满足这个微分方程（即让 Loss 最小化），就能隐式地学会那个复杂的积分关系。</li> </ol> <p>这就是 MeanFlow 能够从零开始训练（From Scratch）且不需要模拟积分过程的核心数学原理。</p> <h2 id="之前和同事讨论有的会问-在t和r的瞬时速度不都是-x1-x0-吗所以平均速度就是-x1-x0-直接预测它不就行了不就是flow-matching吗">之前和同事讨论,有的会问, 在t和r的瞬时速度不都是 $(x1-x0)$ 吗,所以平均速度就是 $(x1-x0)$, 直接预测它不就行了,不就是flow matching吗</h2> <p>简单来说：<strong>对于单条数据轨迹，是对的；但对于模型学习的整个向量场，是错的。</strong></p> <p>这里涉及到两个核心概念的区别：<strong>条件流 (Conditional Flow)</strong> vs <strong>边缘流 (Marginal Flow)</strong>。 (其实就是论文里的 Figure 2)</p> <h3 id="1-条件流-conditional-flow--直线">1. 条件流 (Conditional Flow) —— 直线</h3> <p>假设我们只看<strong>一张</strong>图片 $x$ 和<strong>一个</strong>对应的噪声 $\epsilon$。 在 Flow Matching 中，我们确实通常把路径设计成直线的： \(z_t = (1-t)x + t\epsilon\) 对这个式子求导，瞬时速度确实是常数： \(v_t = \epsilon - x\) 在这种情况下，无论 $t$ 和 $r$ 是多少，速度都是一样的。平均速度自然也等于瞬时速度。 <strong>如果模型只需要记住这一张图，结论完全成立。</strong></p> <h3 id="2-实际情况边缘流-marginal-flow--曲线">2. 实际情况：边缘流 (Marginal Flow) —— 曲线</h3> <p>但在训练生成模型时，模型面对的是成千上万张图片和无数的噪声。模型不知道当前的 $z_t$ 到底属于哪一张具体的图片 $x$。</p> <p><strong>问题出现在“路径交叉”：</strong> 想象一下，在 $t=0.5$ 的时刻，空间中有一个点 $P$。</p> <ul> <li><strong>路径 A</strong>（从噪声 $\epsilon_A$ 到图片 $x_A$）经过点 $P$，它的方向是向“左上”。</li> <li><strong>路径 B</strong>（从噪声 $\epsilon_B$ 到图片 $x_B$）也经过点 $P$，它的方向是向“右上”。</li> </ul> <p>模型在点 $P$ 只能输出<strong>一个</strong>速度向量。它该听谁的？ 根据 Flow Matching 的理论（公式 1），模型学习的是所有经过该点的可能速度的<strong>期望（平均值）</strong>： \(v(z_t, t) = \mathbb{E}[v_t | z_t]\) 在这个例子里，模型会输出“正上方”（左上和右上的平均）。</p> <h3 id="3-结果弯曲的轨迹">3. 结果：弯曲的轨迹</h3> <p>一旦模型输出了平均方向（正上方），生成的轨迹就不再是原来的路径 A（左上），也不是路径 B（右上），而是一条<strong>新的、弯曲的轨迹</strong>。</p> <ul> <li><strong>论文图 2 (Figure 2) 专门展示了这个现象：</strong> <ul> <li><strong>左图 (Conditional)：</strong> 每一条单独的线都是直的。</li> <li><strong>右图 (Marginal)：</strong> 当无数条直线叠加并取平均后，形成的<strong>向量场是弯曲的</strong>。</li> </ul> </li> </ul> <h3 id="4-结论为什么平均速度-neq-瞬时速度">4. 结论：为什么平均速度 $\neq$ 瞬时速度？</h3> <p>因为最终生成的轨迹（Marginal Trajectory）是<strong>弯曲</strong>的：</p> <ol> <li><strong>瞬时速度 ($v$)：</strong> 是曲线在某一点的<strong>切线</strong>方向。因为曲线在弯，所以切线方向时刻在变。</li> <li><strong>平均速度 ($u$)：</strong> 是连接起点 $z_r$ 和终点 $z_t$ 的<strong>割线</strong>（直线）方向。</li> </ol> <p><strong>在弯曲的路径上，切线（瞬时）和割线（平均）是不重合的。</strong></p> <h3 id="总结">总结</h3> <ul> <li><strong>$(x_1 - x_0)$：</strong> 是上帝视角下，连接特定噪声和特定图片的直线速度。</li> <li><strong>模型学的 $v(z_t, t)$：</strong> 是凡人视角下，在迷雾中看到的众生相的平均方向，这导致路变弯了。</li> <li><strong>MeanFlow 的 $u$：</strong> 是试图在<strong>弯曲的路径</strong>上，直接找到从起点跳到终点的那个“捷径”向量。</li> </ul> <p>或者:</p> <ol> <li>对于单条数据: $v$ 就是 $ (x1-x0) $, 路径是直的.</li> <li>对于模型学习的目标: $v$ 是无数个 $x_1-x_0$ 的统计平均.</li> </ol> <p>简单回答：<strong>在数学推导的公式里，$v$ 是一个随位置 $z$ 和时间 $t$ 变化的函数 $v(z_t, t)$，而不是常数。</strong> 之所以会觉得它“看起来像” $(x_1 - x_0)$，是因为我们在<strong>构造训练数据</strong>时使用了直线插值。</p> <p>为了解开这个困惑，我们需要区分<strong>“推导时的定义”</strong>和<strong>“训练时的采样”</strong>。</p> <hr/> <h3 id="1-推导时的视角v-是一个场-field">1. 推导时的视角：$v$ 是一个场 (Field)</h3> <p>在推导 MeanFlow Identity 时，我们并没有假设粒子走的是直线。</p> <ul> <li><strong>公式回顾：</strong> \(u(z_t, r, t) = \frac{1}{t-r} \int_r^t v(z_\tau, \tau) d\tau\)</li> <li><strong>这里的 $v(z_\tau, \tau)$ 是什么？</strong> 它是<strong>边缘速度场 (Marginal Velocity Field)</strong>。 也就是在时间 $\tau$、位置 $z_\tau$ 处，所有可能经过这里的粒子的平均速度。它代表的是我们模型最终想学到的速度场. 正如我们之前讨论的，这个场通常是<strong>弯曲的</strong>， $v$ 随时间 $\tau$ 在不断变化，<strong>它一般不等于常数</strong>。</li> <li><strong>微积分基本定理：</strong> 推导中用到了 $\frac{d}{dt} \int_r^t v(\tau) d\tau = v(t)$。 这个定理成立的前提<strong>不需要</strong> $v$ 是常数。无论 $v$ 是一条直线还是一条疯狂的曲线，这个导数关系都成立。</li> </ul> <p><strong>结论：</strong> 在公式证明阶段，$v$ 是变量，不是常数 $(x_1 - x_0)$。</p> <hr/> <h3 id="2-训练时的视角v-是采样样本-sample">2. 训练时的视角：$v$ 是采样样本 (Sample)</h3> <p>那么，为什么在训练代码里，我们又把 $v$ 当作 $(x_1 - x_0)$ 呢？</p> <p>这是因为我们无法直接获得那个完美的、弯曲的“边缘速度场”。我们只有一堆离散的数据点（图片 $x$ 和噪声 $\epsilon$）。</p> <p>这里用到了 <strong>Flow Matching 的核心原理：期望匹配 (Expectation Matching)</strong>。</p> <ul> <li> <p><strong>理论目标（弯曲的）：</strong> \(\text{Loss} = \| u_\theta - \text{Target}_{\text{Marginal}} \|^2\) 其中 $\text{Target}_{\text{Marginal}}$ 是基于那个弯曲的 $v(z, t)$ 计算出来的。但我们算不出来。</p> </li> <li> <p><strong>实际操作（直线的）：</strong> 我们把 Loss 写成期望形式： \(\text{Loss} \approx \mathbb{E}_{x, \epsilon} [ \| u_\theta - \text{Target}_{\text{Conditional}} \|^2 ]\) 在这里，对于<strong>每一个单独的样本</strong>，我们假定它走直线，所以我们用 $v_{sample} = \epsilon - x$ 代替了公式里的 $v$。</p> </li> </ul> <p><strong>神奇的事情发生了：</strong> 虽然每一个样本提供的 $v_{sample}$ 都是直线的（常数），但因为它们在空间中相互交叉、冲突，神经网络 $u_\theta$ 为了同时让 Loss 最小化，<strong>被迫</strong>去学习所有这些直线的<strong>平均效果</strong>。</p> <p>而这个“平均效果”，恰恰就是那个<strong>弯曲的边缘场</strong>。</p> <hr/> <h3 id="3-为什么公式里的导数项不为-0">3. 为什么公式里的导数项不为 0？</h3> <p>回到困惑：</p> <blockquote> <p>“如果 $v$ 是常数，那么 $u$ 也是常数，$\frac{du}{dt}$ 不就是 0 吗？”</p> </blockquote> <p>在训练 Loss 中： \(u_{\text{tgt}} = v_{\text{sample}} - (t-r) \times \frac{d}{dt} u_\theta\)</p> <ol> <li><strong>$v_{\text{sample}}$：</strong> 确实是常数 $(\epsilon - x)$。</li> <li><strong>$u_\theta$ (神经网络)：</strong> <strong>它不是常数！</strong> <ul> <li>神经网络 $u_\theta(z, r, t)$ 是一个复杂的非线性函数。</li> <li>当你输入不同的 $z$（即使是沿着直线 $z_t$ 移动），网络的输出 $u_\theta$ 会发生变化。</li> <li>因为网络试图拟合的是那个“弯曲的场”，而不是当前的“直线样本”。</li> <li>所以，<strong>$\frac{d}{dt} u_\theta$ (即 JVP) 不等于 0</strong>。</li> </ul> </li> </ol> <h3 id="总结-1">总结</h3> <ul> <li><strong>在数学证明里：</strong> $v$ 是边缘场，是变化的，公式描述的是场内部的自洽性。</li> <li><strong>在训练数据里：</strong> 我们用直线的 $v = \epsilon - x$ 作为<strong>探针</strong>。</li> <li><strong>在 Loss 计算里：</strong> <ul> <li>$v$ 用的是直线的（常数）。</li> <li>但导数 $\frac{du}{dt}$ 用的是网络的（变化的）。</li> <li><strong>Loss 的本质是：</strong> 强迫网络预测的“变化率”与“直线样本和网络预测值的偏差”保持一致。当网络在大量样本上都满足这个关系时，它就学会了真正的 MeanFlow。</li> </ul> </li> </ul> <p>模型训练的过程其实就是用个体去估计整体的过程.</p> <ol> <li>个体是直的，整体是弯的 <ul> <li><strong>个体 (Individual)：</strong> 训练时，每一次迭代我们只采样一对 $(x, \epsilon)$。对于这一对数据，我们假设它们之间是<strong>直线连接</strong>的，速度就是简单的 $v_{sample} = \epsilon - x$。</li> <li><strong>整体 (Whole)：</strong> 实际上，数据分布是极其复杂的。在空间中的某一点，可能有成千上万条来自不同 $(x, \epsilon)$ 的直线穿过，方向各不相同。</li> <li><strong>估计过程：</strong> 神经网络 $u_\theta$ 无法同时满足所有冲突的直线方向。为了让总 Loss 最小，它只能被迫去学习这些方向的<strong>期望（平均值）</strong>。</li> </ul> <ul> <li>无数条直线的平均 $\rightarrow$ 变成了一条平滑的曲线（整体场）。</li> </ul> </li> <li>MeanFlow 的独特之处：用“局部”估计“跨度” 普通的 Flow Matching 也是用个体估计整体，但 MeanFlow 更进一步：</li> </ol> <ul> <li><strong>普通 Flow Matching：</strong> 用个体的“直线方向”去估计整体的“切线方向”。（所以我得一步步走，因为切线在变）。</li> <li><strong>MeanFlow：</strong> 用个体的“直线方向” + <strong>微分恒等式</strong>，去估计整体的<strong>“一步跨越的平均速度”</strong>。</li> </ul> <p>这就像是：</p> <ul> <li><strong>个体数据说：</strong> “我现在想沿直线走。”</li> <li><strong>MeanFlow 恒等式说：</strong> “如果你想一步跳到终点，你现在的变化率必须满足这个物理规律。”</li> <li><strong>模型说：</strong> “好吧，结合你们俩的要求，我算出了一个能代表整体趋势的‘捷径’。”</li> </ul> <ol> <li>为什么这能行？（大数定律） 虽然每次训练只看一个个体，但在训练了几十万步（Batch Size $\times$ Iterations）之后： <ul> <li>个体的随机性（方差）被平均掉了。</li> <li>留下的就是整体的规律（偏差/均值）。</li> </ul> </li> </ol> <p><strong>MeanFlow 的训练过程，就是通过不断喂给模型无数个“走直线的个体”，利用 Loss 函数的约束，强迫模型在脑海中重构出那个“看不见的、弯曲的整体流场”，并学会如何“一步跨越”它。</strong></p> <p>然后在看公式的时候,一定要明确 哪个量是模型要估计的整体量, 哪个量是要采样出来的个体量. 所以我的理解是平均流恒等式建立了一个宏观速度场与微观粒子的速度的关系. 而且这个微观粒子的速度也是人为定义的,并不是模型学完之后估计出的.</p>]]></content><author><name></name></author><category term="AIGC"/><summary type="html"><![CDATA[[TOC]]]></summary></entry><entry><title type="html">Pi05</title><link href="https://beyondpzk.github.io/blog/2025/Pi05/" rel="alternate" type="text/html" title="Pi05"/><published>2025-04-22T00:00:00+00:00</published><updated>2025-04-22T00:00:00+00:00</updated><id>https://beyondpzk.github.io/blog/2025/Pi05</id><content type="html" xml:base="https://beyondpzk.github.io/blog/2025/Pi05/"><![CDATA[<p>[TOC]</p> <h1 id="pi_05-a-vision-language-action-model-with-open-world-generalization">$\pi_{0.5}$: a Vision-Language-Action Model with Open-World Generalization</h1> <p><a href="https://arxiv.org/abs/2504.16054">paper link</a></p> <h2 id="先快速对比一下-pi_0">先快速对比一下 $\pi_{0}$</h2> <p>目前的演进路线主要包含两个关键节点：<strong>基础模型 $\pi_0$ (Pi-Zero)</strong> 和 <strong>增强泛化能力的 $\pi_{0.5}$ (Pi-Zero-Point-Five)</strong>。</p> <p>这篇论文明确指出 $\pi_{0.5}$ 是基于 $\pi_0$ 构建的，二者的演进逻辑并非简单的“参数量增加”，而是从<strong>通用的物理操作能力</strong>向<strong>开放世界的语义理解与长程规划能力</strong>的跃迁。</p> <p>以下是演进路线：</p> <h3 id="1-基石pi_0-the-foundation">1. 基石：$\pi_0$ (The Foundation)</h3> <ul> <li><strong>定位：</strong> 通用基础 VLA（视觉-语言-动作）模型。</li> <li><strong>核心能力：</strong> $\pi_0$ 旨在解决<strong>通用的物理灵巧性（Dexterity）</strong>。它能够控制多种不同的机器人形态（如双臂机器人、单臂机器人、移动底盘等），执行各种低层物理动作。</li> <li><strong>架构特点：</strong> 采用基于 Flow Matching（流匹配）的策略来生成连续的动作轨迹，具有极强的物理执行能力。</li> <li><strong>局限性：</strong> 尽管 $\pi_0$ 在执行特定动作上非常强，但单纯的 $\pi_0$ 在面对完全陌生的环境（如从未见过的厨房）或需要长时间序列推理（如“打扫整个房间”）时，可能缺乏足够的高层语义规划能力。</li> </ul> <h3 id="2-进化pi_05-the-generalist">2. 进化：$\pi_{0.5}$ (The Generalist)</h3> <ul> <li><strong>定位：</strong> 面向<strong>开放世界泛化（Open-World Generalization）</strong>的增强版模型。</li> <li><strong>演进动力：</strong> 解决“机器人走出实验室”的问题。仅仅会“抓取”是不够的，机器人需要知道“在乱糟糟的房间里先抓哪个、放到哪里、为什么要这么做”。</li> <li><strong>关键演进点（相对 $\pi_0$）：</strong> <ol> <li><strong>引入高层语义推理（High-Level Reasoning）：</strong> $\pi_{0.5}$ 不仅输出电机指令，还会在每一步先预测一个“语义子任务”（Semantic Subtask，如“打开柜子”）。这就像给 $\pi_0$ 装上了一个“大脑”，让它能进行思维链推理。 <alphaxiv-paper-citation title="Architecture" page="2" first="predicts the semantic subtask" last="low-level robot action"></alphaxiv-paper-citation></li> <li><strong>异构协同训练（Heterogeneous Co-training）：</strong> $\pi_{0.5}$ 的训练数据不再局限于机器人操作数据，而是大量引入了网络数据（VQA、定位）、其他形态机器人的数据以及人类的口头指导数据。这使得模型获得了超越物理经验的“常识”。 <alphaxiv-paper-citation title="Data Strategy" page="2" first="overwhelming majority of training" last="from these other sources"></alphaxiv-paper-citation></li> <li><strong>长程任务能力：</strong> $\pi_0$ 可能擅长 10 秒的抓取，而 $\pi_{0.5}$ 被设计用来执行 10-15 分钟的复杂任务（如整理床铺、清洁厨房）。 <alphaxiv-paper-citation title="Long Horizon" page="1" first="performing complex multi-stage" last="10 to 15 minutes."></alphaxiv-paper-citation></li> </ol> </li> </ul> <h3 id="演进路线图">演进路线图</h3> <table> <thead> <tr> <th style="text-align: left">特性</th> <th style="text-align: left">$\pi_0$ (基座)</th> <th style="text-align: left">$\pi_{0.5}$ (当前版本)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>核心目标</strong></td> <td style="text-align: left"><strong>怎么做 (How)</strong>：解决物理控制与灵巧操作</td> <td style="text-align: left"><strong>做什么 (What &amp; Why)</strong>：解决语义理解与任务规划</td> </tr> <tr> <td style="text-align: left"><strong>训练数据</strong></td> <td style="text-align: left">主要是机器人轨迹数据</td> <td style="text-align: left">机器人数据 + <strong>海量网络数据 + 语义标注 + 口头指令</strong></td> </tr> <tr> <td style="text-align: left"><strong>推理模式</strong></td> <td style="text-align: left">观察 $\to$ 动作</td> <td style="text-align: left">观察 $\to$ <strong>语义意图</strong> $\to$ 动作</td> </tr> <tr> <td style="text-align: left"><strong>应用场景</strong></td> <td style="text-align: left">实验室环境、单一任务</td> <td style="text-align: left"><strong>全新家庭环境 (In the wild)</strong>、长程家务任务</td> </tr> </tbody> </table> <p>可以将 $\pi_0$ 想象成一个 <strong>“肢体发达”</strong>的运动员，它的肌肉记忆（Motor Control）非常完美；而 $\pi_{0.5}$ 则是给这个运动员配备了<strong>“生活常识”</strong>和<strong>“任务规划手册”</strong>，让他不仅能跑能跳，还能理解“把家里打扫干净”意味着要把脏衣服放进篮子而不是扔在地上。这篇论文正是通过 $\pi_{0.5}$ 展示了如何通过数据和架构的调整，让机器人模型从“专才”走向“通才”。</p> <h2 id="notes-of-pi_05">Notes of $\pi_{0.5}$</h2> <p>在过去的几年里，以深度学习为基础的系统在自然语言处理（NLP）和计算机视觉（CV）领域取得了规模化的成功。然而，物理智能（Physical Intelligence）——即智能体在物理世界中执行复杂任务的能力——仍然面临着巨大的挑战。其中最核心的难题之一，就是 <strong>开放世界泛化（Open-World Generalization）</strong>。 《$\pi_{0.5}$: a Vision-Language-Action Model with Open-World Generalization》。这篇论文由 Physical Intelligence 公司发布，它展示了一个端到端的学习系统，能够控制移动操作机器人在从未见过的家庭环境中执行长周期的家务任务。</p> <hr/> <h3 id="第一模块核心动机与挑战-introduction--motivation">第一模块：核心动机与挑战 (Introduction &amp; Motivation)</h3> <h4 id="11-机器人学习的泛化墙">1.1 机器人学习的“泛化墙”</h4> <p>在传统的机器人研究中，我们在实验室受控环境下训练机器人，它们往往表现优异。但是，一旦我们将机器人移出实验室，面对现实世界中纷繁复杂的场景、光照、物体布局，系统的鲁棒性往往会急剧下降。</p> <p>论文开篇引用了雷·布拉德伯里（Ray Bradbury）的名言：“Stuff your eyes with wonder… See the world.” 这不仅是文学上的修辞，更点出了物理智能的愿景：具身系统只有离开实验室，处理现实世界中的意外事件，才真正有用。 <alphaxiv-paper-citation title="Introduction" page="1" first="Open-world generalization represents" last="in the real world."></alphaxiv-paper-citation></p> <h4 id="12-视觉-语言-动作vla模型的兴起">1.2 视觉-语言-动作（VLA）模型的兴起</h4> <p>为了解决泛化问题，学术界引入了视觉-语言-动作模型（VLA）。VLA通过将预训练的视觉-语言模型（VLM）微调用于机器人控制，试图利用互联网规模的语义知识。然而，现有的VLA模型虽然展示了惊人的语言遵循能力，但主要还是在与训练数据非常相似的环境中进行评估。</p> <p>本论文提出的 $\pi_{0.5}$ 模型，基于 $\pi_0$ 架构，旨在回答一个核心问题：这种模型在野外（in the wild）究竟能泛化到什么程度？ <alphaxiv-paper-citation title="Abstract" page="1" first="While vision-language-action (VLA)" last="generalize in the wild."></alphaxiv-paper-citation></p> <h4 id="13-pi_05-的核心突破">1.3 $\pi_{0.5}$ 的核心突破</h4> <p>$\pi_{0.5}$ 的核心贡献在于它不仅使用了移动操作机器人的数据，还引入了异构任务的协同训练（co-training）。这使得机器人能够在从未见过的家庭中执行长达10到15分钟的复杂多阶段行为，例如清洁厨房或卧室。 <alphaxiv-paper-citation title="Performance" page="1" first="performing complex multi-stage" last="10 to 15 minutes."></alphaxiv-paper-citation></p> <hr/> <h3 id="第二模块异构数据策略-heterogeneous-data-strategy">第二模块：异构数据策略 (Heterogeneous Data Strategy)</h3> <p>这篇论文最引人注目的地方在于其数据配方。为了实现类似人类的泛化能力——即利用间接经验、书本知识和类比推理——$\pi_{0.5}$ 并没有单纯依赖大规模的特定机器人数据。</p> <h4 id="21-数据来源的多样性">2.1 数据来源的多样性</h4> <p>$\pi_{0.5}$ 的训练数据包含以下几类：</p> <ol> <li><strong>移动操作机器人数据（Mobile Manipulator Data）：</strong> 直接在真实家庭中收集，约400小时。</li> <li><strong>静态机器人数据（Static Robot Data）：</strong> 来自实验室或其他环境的非移动机械臂数据。</li> <li><strong>高层语义预测（High-Level Semantic Prediction）：</strong> 预测下一步应该做什么（例如“捡起盘子”）。</li> <li><strong>网络数据（Web Data）：</strong> 包括图像描述、问答和物体定位。</li> <li><strong>口头指令（Verbal Instructions）：</strong> 人类监督员逐步指导机器人的数据。</li> </ol> <p>令人惊讶的是，在第一阶段训练中，提供给 $\pi_{0.5}$ 的绝大多数训练样本（97.6%）并非来自移动操作机器人执行家务任务，而是来自其他机器人或网络数据。 <alphaxiv-paper-citation title="Data Sources" page="2" first="The overwhelming majority" last="from the web."></alphaxiv-paper-citation></p> <h4 id="22-为什么需要异构数据">2.2 为什么需要异构数据？</h4> <p>这种策略背后的逻辑是知识迁移。</p> <ul> <li><strong>动作迁移：</strong> 移动机器人可以从其他静态机器人的数据中学习低层控制策略（如抓取）。</li> <li><strong>语义迁移：</strong> 高层推理过程可以从网络数据和语义标注中受益。</li> <li><strong>指令理解：</strong> 通过人类监督员像教人一样教机器人，模型学习了如何将复杂任务分解。 <alphaxiv-paper-citation title="Reasoning" page="3" first="instructing it (much" last="completing a complex task"></alphaxiv-paper-citation></li> </ul> <hr/> <h3 id="第三模块分层架构与推理-hierarchical-architecture--inference">第三模块：分层架构与推理 (Hierarchical Architecture &amp; Inference)</h3> <p>$\pi_{0.5}$ 采用了一种简单而有效的分层架构设计，这种设计与其异构数据策略紧密结合。</p> <h4 id="31-两阶段推理过程">3.1 两阶段推理过程</h4> <p>在推理时的每一步，模型执行以下两个阶段：</p> <ol> <li><strong>语义子任务预测（High-Level Semantic Action）：</strong> 首先，模型根据任务结构和场景语义，预测下一个适当的行为（Subtask），例如“把物品放入抽屉”或“重新整理枕头”。</li> <li><strong>低层动作生成（Low-Level Action Chunk）：</strong> 基于预测出的子任务，模型生成具体的机器人动作块。</li> </ol> <p>这种架构使得系统能够兼顾长周期的多阶段任务推理和精细的动作执行。 <alphaxiv-paper-citation title="Architecture" page="2" first="At runtime, during" last="level robot action chunk"></alphaxiv-paper-citation></p> <h4 id="32-统一模型的优势">3.2 统一模型的优势</h4> <p>与许多使用两个独立模型（一个VLM做规划，一个Policy做执行）的方法不同，$\pi_{0.5}$ 使用同一个模型进行高层和低层推理。这类似于思维链（Chain-of-Thought）或测试时计算（Test-time Compute）的方法。低层推理受益于其他机器人的动作数据，而高层推理则受益于网络语义数据。 <alphaxiv-paper-citation title="Design" page="2" first="This simple architecture" last="two levels: the low-level"></alphaxiv-paper-citation></p> <hr/> <h3 id="第四模块实验评估-experimental-evaluation">第四模块：实验评估 (Experimental Evaluation)</h3> <p>实验设计旨在验证模型在“全新环境”中的表现。研究团队构建了模拟环境（Mock Environments）用于定量对比，并在真实家庭（Real Homes）中进行了最终测试。</p> <h4 id="41-评估环境设置">4.1 评估环境设置</h4> <ul> <li><strong>模拟房间：</strong> 用于可复现的定量评估。</li> <li><strong>真实家庭：</strong> 包含从未在训练数据中出现的全新厨房和卧室，具有新颖的物体、背景和布局。 <alphaxiv-paper-citation title="Environments" page="8" first="We evaluate pi0.5" last="and layouts."></alphaxiv-paper-citation></li> </ul> <h4 id="42-定量与定性结果">4.2 定量与定性结果</h4> <p>在真实家庭的评估中，机器人被要求执行如“把物品放入抽屉”、“把盘子放入水槽”等任务。结果显示，$\pi_{0.5}$ 能够成功完成这些任务。图表显示，模型在模拟环境中的表现与其在真实家庭中的表现具有代表性一致性。 <alphaxiv-paper-citation title="Results" page="8" first="find that pi0.5's" last="in real homes."></alphaxiv-paper-citation></p> <p>特别值得注意的是，$\pi_{0.5}$ 展示了在完全陌生的家庭中清洁厨房或卧室的能力，这是之前的VLA模型难以企及的。 <alphaxiv-paper-citation title="Capability" page="3" first="demonstrate an end-to-end" last="in entirely new homes."></alphaxiv-paper-citation></p> <h4 id="43-消融实验什么才是关键">4.3 消融实验：什么才是关键？</h4> <p>为了理解各个组件的重要性，论文进行了详细的消融研究（Ablation Study）。以下是几个关键发现：</p> <ol> <li><strong>高层推理的重要性：</strong> 完整的 $\pi_{0.5}$ 模型（包含显式的高层和低层推理）表现最佳。</li> <li><strong>隐式高层的意外发现：</strong> 有趣的是，“Implicit HL”设置（训练时包含高层数据，但推理时不输出子任务）是表现第二好的模型。这表明，仅仅在训练混合数据中包含子任务预测数据，就能显著提升模型性能。 <alphaxiv-paper-citation title="Ablation" page="11" first="predicting subtask labels" last="in the training mixture."></alphaxiv-paper-citation></li> <li><strong>口头指令的关键性：</strong> 去除口头指令数据（No VI）会导致性能显著下降。尽管这部分数据仅占高层移动操作样本的11%，但它对模型理解复杂任务至关重要。 <alphaxiv-paper-citation title="Verbal Instructions" page="11" first="which only constitutes" last="significantly weaker."></alphaxiv-paper-citation></li> <li><strong>GPT-4 的局限性：</strong> 直接使用 GPT-4 作为高层策略（Zero-shot）的表现最差，证明了使用机器人数据适配 VLM 的必要性。 <alphaxiv-paper-citation title="GPT-4 Comparison" page="11" first="GPT-4 ablation attains" last="with robot data."></alphaxiv-paper-citation></li> </ol> <hr/> <h3 id="第五模块局限性与未来展望-limitations--future-work">第五模块：局限性与未来展望 (Limitations &amp; Future Work)</h3> <h4 id="51-存在的挑战">5.1 存在的挑战</h4> <p>论文坦诚地讨论了失败案例：</p> <ul> <li><strong>物理交互难点：</strong> 模型在处理不熟悉的抽屉把手，或者物理上难以打开的柜子时会遇到困难。</li> <li><strong>部分可观测性（Partial Observability）：</strong> 例如，机械臂可能会遮挡住需要擦拭的污渍。</li> <li><strong>高层推理干扰：</strong> 有时高层推理会被干扰，导致机器人重复打开和关闭抽屉。 <alphaxiv-paper-citation title="Limitations" page="11" first="Some environments present" last="away items)."></alphaxiv-paper-citation></li> </ul> <h4 id="52-未来方向">5.2 未来方向</h4> <p>未来的工作可能会集中在解决这些感知和物理交互的限制上。此外，论文提到 $\pi_{0.5}$ 处理的提示词（Prompts）相对简单。未来的系统需要结合更丰富的上下文和记忆能力，以处理跨房间导航或记忆物体位置等更复杂的任务。 <alphaxiv-paper-citation title="Future Work" page="11" first="incorporating richer context" last="objects are stored."></alphaxiv-paper-citation></p> <h3 id="小结">小结</h3> <p>$\pi_{0.5}$ 并不单纯是数据规模的胜利，而是<strong>数据多样性</strong>和<strong>架构设计</strong>的胜利。它证明了通过混合协同训练（Co-training），我们可以利用非机器人数据（如网络数据）和非特定任务数据（如其他机器人数据）来弥补特定场景数据的不足，从而跨越“泛化墙”，迈向真正的物理智能。</p> <h2 id="关键点-co-training的细节">关键点 Co-Training的细节</h2> <p>这是现代大规模多模态训练（Multi-modal Training）中处理异构数据（Heterogeneous Data）的标准做法。</p> <h3 id="1-你的模型batch-内部的分治法">1. 你的模型：Batch 内部的“分治法”</h3> <p>在 $\pi_{0.5}$ 的训练中，一个 Batch（比如 size=100）确实是一个混合体。就像你说的：</p> <ul> <li><strong>前 50 条数据（非机器人任务，如 VQA/Caption）：</strong> <ul> <li><strong>输入：</strong> 图片 + 问题（如“图里有什么？”）</li> <li><strong>目标：</strong> 文本回答（如“一个苹果”）</li> <li><strong>Loss 计算：</strong> 计算交叉熵损失（Cross-Entropy Loss）。</li> <li><strong>关键点：</strong> 对于这 50 条数据，<strong>Flow Matching Loss 会被强制置为 0（Masked out）</strong>。因为这些数据没有物理动作标签，我们不想让模型去“瞎猜”一个动作，从而干扰动作生成的权重。</li> </ul> </li> <li><strong>后 50 条数据（机器人任务）：</strong> <ul> <li><strong>输入：</strong> 机器人视角图片 + 指令（如“捡起苹果”）</li> <li><strong>目标：</strong> <ol> <li><strong>语义子任务（Subtask）：</strong> 文本（如 <code class="language-plaintext highlighter-rouge">&lt;pick_object&gt;</code>）</li> <li><strong>物理动作（Action）：</strong> 连续轨迹</li> </ol> </li> <li><strong>Loss 计算：</strong> <ul> <li><strong>部分 A：</strong> 针对子任务文本，计算 <strong>交叉熵损失</strong>。</li> <li><strong>部分 B：</strong> 针对动作轨迹，计算 <strong>Flow Matching Loss</strong>。</li> </ul> </li> <li><strong>关键点：</strong> $\pi_{0.5}$ 的独特之处在于，机器人数据<strong>同时也贡献了文本 Loss</strong>（因为要预测子任务），这是它与普通 VLA 不同的地方。 <alphaxiv-paper-citation title="Hierarchical Architecture" page="2" first="predicts the semantic subtask" last="low-level robot action chunk"></alphaxiv-paper-citation></li> </ul> </li> </ul> <h3 id="2-数学上的总-loss">2. 数学上的“总 Loss”</h3> <p>对于整个 Batch，最终的 Loss 是所有样本 Loss 的平均值。但在计算梯度时，是根据样本类型加权的。</p> <p>我们可以写成一个通用的公式：</p> \[L_{\text{total}} = \frac{1}{N} \sum_{i=1}^{N} \left( \mathbb{I}_{i \in \text{Text}} \cdot L_{\text{CE}}^{(i)} + \mathbb{I}_{i \in \text{Robot}} \cdot (L_{\text{CE\_Subtask}}^{(i)} + \lambda L_{\text{Flow}}^{(i)}) \right)\] <p>其中：</p> <ul> <li>$\mathbb{I}$ 是指示函数（Indicator Function）：如果是该类任务则为 1，否则为 0。这就是所谓的<strong>掩码（Mask）</strong>。</li> <li>对于 VQA 数据，只有第一项生效。</li> <li>对于机器人数据，后两项生效。</li> </ul> <h3 id="3-为什么要混在一个-batch-里shared-backbone">3. 为什么要混在一个 Batch 里？（Shared Backbone）</h3> <p>你可能会问，既然 Loss 是分开算的，为什么不干脆训练两个模型？</p> <p>原因在于<strong>梯度回传（Backpropagation）</strong>。</p> <p>虽然 Loss 是在最后分开算的，但梯度会沿着网络反向传播，最终汇聚到<strong>同一个</strong>视觉编码器（Vision Encoder）和<strong>同一个</strong> Transformer 主干（Backbone）上。</p> <ul> <li>当你用 VQA 数据更新参数时，你在教视觉编码器：“看，这个圆圆红红的东西叫苹果”。</li> <li>当你用机器人数据更新参数时，你在教同一个视觉编码器：“看到这个圆圆红红的东西（苹果），机械臂要伸过去”。</li> </ul> <p>通过在同一个 Batch 里混合训练，视觉编码器学会了提取<strong>既能回答问题、又能指导动作</strong>的通用特征。这就是为什么 $\pi_{0.5}$ 能泛化的核心原因。 <alphaxiv-paper-citation title="Co-training" page="2" first="co-training framework for VLAs" last="enable broad generalization."></alphaxiv-paper-citation></p> <h3 id="4-特殊设计action-expert-动作专家">4. 特殊设计：Action Expert (动作专家)</h3> <p>还有一个细节需要补充：$\pi_{0.5}$ 在架构上使用了 <strong>Action Expert</strong>。</p> <p>虽然主干网络是共享的，但在最后输出层，处理动作数据的部分是一个独立的小型网络（Expert）。</p> <ul> <li><strong>文本数据</strong> 只走 Text Head。</li> <li><strong>机器人动作数据</strong> 会走 Action Expert Head。</li> </ul> <p>这意味着，虽然大家共用大脑（Backbone），但“说话”用的是嘴（Text Head），“动手”用的是手（Action Expert）。这种设计防止了动作生成的特殊需求（连续值、高精度）干扰到语言生成的逻辑。 <alphaxiv-paper-citation title="Action Expert" page="4" first="use a different set" last="the rest of the LLM"></alphaxiv-paper-citation></p> <h3 id="小结-1">小结</h3> <p>Batch 混合 + Loss 掩码是 VLA 训练的标准操作。而 $\pi_{0.5}$ 的特殊之处在于：</p> <ol> <li><strong>机器人数据也算文本 Loss</strong>（为了预测子任务）。</li> <li><strong>使用了 Action Expert</strong>（专门的动作输出头）。</li> </ol> <h2 id="对比自动驾驶中的先决策后执行逻辑">对比自动驾驶中的”先决策后执行”逻辑</h2> <p>$\pi_{0.5}$ 的架构本质上就是一种<strong>分层决策（Hierarchical Decision Making）</strong>，这在自动驾驶领域是非常经典的范式。</p> <p>我们可以把自动驾驶的 <strong>“规划-控制”</strong>流程与 $\pi_{0.5}$ 的流程做一个严丝合缝的映射：</p> <h3 id="1-自动驾驶-vs-pi_05核心流程映射">1. 自动驾驶 vs. $\pi_{0.5}$：核心流程映射</h3> <table> <thead> <tr> <th style="text-align: left">阶段</th> <th style="text-align: left">自动驾驶 (AD) 的类比</th> <th style="text-align: left">$\pi_{0.5}$ (本论文) 的实现</th> <th style="text-align: left">作用 (Why?)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Input</strong></td> <td style="text-align: left">摄像头/激光雷达看到路口</td> <td style="text-align: left">机器人看到乱糟糟的厨房</td> <td style="text-align: left">感知环境状态</td> </tr> <tr> <td style="text-align: left"><strong>High-Level Decision</strong></td> <td style="text-align: left"><strong>行为规划 (Behavior Planning)</strong><br/>决策：“左转”、“变道”、“减速让行”</td> <td style="text-align: left"><strong>语义子任务 (Semantic Subtask)</strong><br/>决策：“打开微波炉”、“捡起抹布”、“把盘子放进水槽”</td> <td style="text-align: left"><strong>确定意图</strong>：将复杂的长任务分解为可执行的短片段。</td> </tr> <tr> <td style="text-align: left"><strong>Low-Level Control</strong></td> <td style="text-align: left"><strong>轨迹规划/控制 (Trajectory/Control)</strong><br/>输出：方向盘转角 15°，油门 20%</td> <td style="text-align: left"><strong>动作分块 (Action Chunk)</strong><br/>输出：机械臂 7 个关节的角度变化、夹爪开合</td> <td style="text-align: left"><strong>执行意图</strong>：将高层决策转化为具体的物理运动。</td> </tr> </tbody> </table> <p>论文中明确提到：$\pi_{0.5}$ 首先预测语义子任务（semantic subtask），然后将其作为上下文（Context）来预测低层的动作块（action chunk）。这与 AD 中先做行为决策再做轨迹规划的逻辑<strong>完全一致</strong>。 <alphaxiv-paper-citation title="Hierarchical Inference" page="2" first="predicts the semantic subtask" last="low-level robot action chunk"></alphaxiv-paper-citation></p> <h3 id="2-但是pi_05-有一个关键的进化">2. 但是，$\pi_{0.5}$ 有一个关键的“进化”</h3> <p>虽然逻辑相似，但在<strong>实现方式</strong>上，$\pi_{0.5}$ 走得比传统 AD 更远，它更接近于特斯拉 FSD v12 那种 <strong>End-to-End（端到端）</strong> 的理念。</p> <ul> <li><strong>传统 AD：</strong> 通常是模块化的。感知模块是一个网络，决策模块可能是一个状态机或另一个网络，控制模块是 PID 或 MPC。它们是<strong>分离</strong>的。</li> <li><strong>$\pi_{0.5}$：</strong> 是<strong>统一</strong>的。 <ul> <li>同一个 Transformer 大脑（Backbone）。</li> <li>先“自言自语”说出决策（生成文本 Token：”Open microwave”）。</li> <li>紧接着，基于这个决策，立刻生成动作（生成动作 Token）。</li> </ul> </li> </ul> <p>这种方式在论文中被类比为 <strong>Chain-of-Thought (思维链)</strong>。模型不需要切换大脑，它只是先“想”一下（输出文本），然后马上“做”（输出动作）。 <alphaxiv-paper-citation title="Chain-of-Thought" page="2" first="analogously to chain" last="of thought inference [82]."></alphaxiv-paper-citation></p> <h3 id="3-为什么这一步决策对-pi_05-如此重要">3. 为什么这一步（决策）对 $\pi_{0.5}$ 如此重要？</h3> <p>在自动驾驶中，如果你不先决定“左转”，直接去预测轨迹，车可能会在路口左右摇摆（模式坍缩）。</p> <p>同理，对于 $\pi_{0.5}$ 这种长周期任务（10-15分钟的家务），如果没有这个中间的“决策层”，机器人很容易迷失。</p> <ul> <li><strong>例子：</strong> 整理床铺。</li> <li><strong>没有决策层：</strong> 机器人看着乱糟糟的床，不知道该拉床单还是拍枕头，动作可能会由于概率分布的混杂而变得抽搐。</li> <li><strong>有决策层：</strong> 模型先强制自己输出文本：“<strong>Grab the pillow</strong>”。一旦这几个字生成出来，后面的动作预测就被<strong>锁定</strong>在了“抓枕头”这个子空间里，动作就会非常坚定和准确。</li> </ul> <p><strong>实验证明：</strong> 论文中的消融实验显示，如果去掉这个高层语义预测（No HL），模型的成功率会大幅下降。这证明了显式的“决策”步骤对于长程任务是不可或缺的。 <alphaxiv-paper-citation title="High-Level Ablation" page="11" first="no HL ablation" last="performs significantly worse."></alphaxiv-paper-citation></p> <h2 id="模型训完后vlm的能力保持的如何还需要保持vlm的能力吗">模型训完后,VLM的能力保持的如何?还需要保持VLM的能力吗?</h2> <p>在传统的微调（Fine-tuning）范式中，我们经常担心“灾难性遗忘”（Catastrophic Forgetting），即模型在学习新任务（如机器人控制）时，丢失了原本预训练获得的通用知识（如识别“红苹果”与“青苹果”的区别）。 对于 $\pi_{0.5}$ 而言，<strong>保持 VLM 的能力不仅是“通过与否”的测试，而是其实现泛化的核心机制</strong>。</p> <h3 id="1-训完之后vlm-能力保持得如何">1. 训完之后，VLM 能力保持得如何？</h3> <p>在 $\pi_{0.5}$ 的架构中，VLM 的能力通过一种称为“协同训练”（Co-training）的策略得到了很好的保持，** 甚至在特定领域得到了增强**。研究团队并没有只用机器人数据去“刷写”模型，而是将机器人数据与大量的网络数据（Web Data）混合在一起训练。</p> <p>这种策略使得模型在学习“如何移动手臂”的同时，不仅没有忘记“什么是苹果”，反而加强了将视觉语义与物理动作关联的能力。实验中的消融研究（Ablation Study）强有力地证明了这一点：当研究人员移除网络数据（即 VQA、图像描述等维持 VLM 能力的数据）时，机器人的物理操作性能显著下降。这表明，模型正是利用了 VLM 的通用语义能力来理解陌生的环境。 <alphaxiv-paper-citation title="Co-training Importance" page="11" first="no WD ablation is" last="improving the high-level policy."></alphaxiv-paper-citation></p> <p>具体来说，$\pi_{0.5}$ 的训练数据中包含了数百万张带有边框（Bounding Boxes）和描述的图像。这意味着模型在训练过程中一直在复习和巩固其视觉识别能力。因此，虽然论文没有展示该模型在纯文本基准测试（如 MMLU）上的得分，但在具身场景下，其“理解世界”的能力（即 VLM 的核心能力）是被刻意保留且表现优异的。 <alphaxiv-paper-citation title="Web Data" page="5" first="We co-train with" last="VQA and captioning."></alphaxiv-paper-citation></p> <h3 id="2-训完-vla-之后还需要保持-vlm-的能力吗">2. 训完 VLA 之后，还需要保持 VLM 的能力吗？</h3> <p><strong>绝对需要。</strong> 甚至可以说，<strong>VLM 能力的保持是 VLA 能够实现“开放世界泛化”的前提</strong>。 (但是VLM4VLA这个论文里面又说VLM的能力和VLA的能力并不是正相关的.)</p> <p>如果我们只关心机器人在流水线上重复同一个动作（例如拧螺丝），我们确实不需要 VLM 的能力，传统的策略网络（Policy Network）就足够了。但是，$\pi_{0.5}$ 的目标是让机器人走进从未见过的家庭，去处理从未见过的物体。</p> <p>首先，<strong>语义理解是泛化的基础</strong>。当用户指令是“把那袋薯片拿给我”时，如果模型因为微调过度而忘记了“薯片”长什么样（VLM 能力丢失），或者无法区分“这袋”和“那袋”的语义差别，那么无论它的机械臂控制得多么平滑，任务都会失败。$\pi_{0.5}$ 的高层策略（High-Level Policy）本质上就是一个 VLM，它负责观察环境并输出文本形式的子任务（例如“Open the microwave”）。如果 VLM 能力丧失，这个“大脑”就瘫痪了。 <alphaxiv-paper-citation title="High-Level Reasoning" page="2" first="predicts the semantic subtask" last="low-level robot action"></alphaxiv-paper-citation></p> <p>其次，<strong>处理意外情况依赖常识</strong>。在非结构化环境中，机器人会遇到各种意外（例如抽屉被卡住、物体被遮挡）。VLM 预训练中包含的物理常识（虽然是隐式的）和逻辑推理能力，能帮助机器人进行简单的因果推断。论文中提到，去除高层语义预测数据的模型表现会变差，这进一步证实了保留这种类似 VLM 的推理能力对于处理复杂长程任务是必须的。 <alphaxiv-paper-citation title="Ablation Results" page="11" first="implicit HL ablation" last="subtask prediction, in training."></alphaxiv-paper-citation></p> <p>VLM 能力和 VLA 能力看作是“零和博弈”。在 $\pi_{0.5}$ 这种现代具身智能范式中，<strong>VLM 能力是 VLA 的“底座”</strong>。我们不需要模型能写诗（纯文本 VLM 能力），但我们必须保持它识别万物、理解复杂指令和进行因果推理的能力。一旦这些能力丢失，机器人就会退化回传统的、只能在实验室特定环境下工作的“特化机器”。</p>]]></content><author><name></name></author><category term="VLA"/><summary type="html"><![CDATA[[TOC]]]></summary></entry><entry><title type="html">pytorch_weights_datasets</title><link href="https://beyondpzk.github.io/blog/2024/pytorch_weights_datasets/" rel="alternate" type="text/html" title="pytorch_weights_datasets"/><published>2024-12-26T00:00:00+00:00</published><updated>2024-12-26T00:00:00+00:00</updated><id>https://beyondpzk.github.io/blog/2024/pytorch_weights_datasets</id><content type="html" xml:base="https://beyondpzk.github.io/blog/2024/pytorch_weights_datasets/"><![CDATA[ <p>可以通过下面的方式简单地实现加权数据集的采样</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class WeightDatasets(Dataset):
    def __init__(self, datasets, p_datasets=None):
        self.datasets = datasets
        if p_datasets is None:
            self.p_datasets = [len(d) for d in self.datasets]
        p_total = sum(p_datasets)
        self.p_datasets = [x/p_total for x in p_datasets]

    def __len__(self):
        return sum([len(d) for d in self.datasets])

    def __getitem__(self, index):
        dataset = random.choices(self.datasets, self.p_datasets)[0]
        return dataset.__getitem__(index)
</code></pre></div></div> <p>然后把这个dataset 传入dataloder即可, 参考ATOM的代码.</p>]]></content><author><name></name></author><category term="reading"/><category term="reading"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">pytorch_bug</title><link href="https://beyondpzk.github.io/blog/2024/pytorch_bug/" rel="alternate" type="text/html" title="pytorch_bug"/><published>2024-12-02T00:00:00+00:00</published><updated>2024-12-02T00:00:00+00:00</updated><id>https://beyondpzk.github.io/blog/2024/pytorch_bug</id><content type="html" xml:base="https://beyondpzk.github.io/blog/2024/pytorch_bug/"><![CDATA[ <h1 id="pytorch-常见问题解决">pytorch 常见问题解决</h1> <h2 id="torchcudaoutofmemoryerror-cuda-out-of-memory-tried-to-allocate-more-than-1eb-memory">torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate more than 1EB memory.</h2> <p>这个问题是由于多机多卡的时候,每个卡上的dataloader的长度不一样,导致有的已经训完了,有的还在等待, 会出现这个错误, 解决方法是强行把每个卡上的dataloader长度调成一致.</p>]]></content><author><name></name></author><category term="pytorch"/><category term="pytorch"/><summary type="html"><![CDATA[]]></summary></entry></feed>