---
layout: post
title: mu_law
date: 2022-05-12
categories: [Tricks]
toc:
    sidebar: left
    max_level: 4
---

[TOC]

# mu_law


### 连续值的离散化：$\mu$-law 编码

机器人的本体感觉（如关节角度）和动作（如力矩）通常是连续浮点数。Transformer本质上是一个分类器，擅长处理离散符号。Gato如何处理连续值？

它采用了一种在音频处理（如WaveNet）中常用的技术：**$\mu$-law 压扩（Companding）**。首先将数据展平，然后通过以下公式将数值非线性地映射到 $[-1, 1]$ 区间：

$$F(x) = \text{sgn}(x) \frac{\ln(|x|\mu + 1.0)}{\ln(M\mu + 1.0)}$$

其中 $\mu=100$，$M=256$。这种编码方式对接近0的小数值有更高的分辨率，而对大数值的分辨率较低，非常适合人类感知和控制信号的特性。映射后，数值被离散化为1024个均匀的箱（bins），并平移到 $[32000, 33024)$ 的整数范围内，以避免与文本Token冲突。 <alphaxiv-paper-citation title="Continuous Values" page="3" first="Continuous values, e.g." last="uniform bins." />


没问题。这是一个非常直观且有助于理解“为什么 Gato 能控制机器人”的好角度。

### **为什么要用 $\mu$-law？（直觉理解）**

想象你在操纵一个机械臂。
*   当你需要**穿针引线**或者**抓取鸡蛋**时，你的动作幅度很小（接近 0），你需要**极高的精度**（比如 0.01 和 0.02 的区别很重要）。
*   当你需要**挥动手臂**去够一个远处的物体时，你的动作幅度很大（接近 1），你**不在乎精度**（0.95 和 0.96 的区别无所谓）。

$\mu$-law 的作用就是：**把有限的 Token（资源）“偏心”地分配给靠近 0 的小数值，而对大数值这就比较“吝啬”。**

---

### **Python 代码与可视化**

这段代码会生成两张图：
1.  **左图（传输函数）**：展示连续值 $x$ 是如何非线性地映射到编码空间 $y$ 的。你会看到经典的“S型”曲线。
2.  **右图（分辨率对比）**：如果我们将 $[-1, 1]$ 均匀切分成 50 个 Token，在线性编码（Linear）和 $\mu$-law 编码下，这些 Token 对应的真实物理值分布有何不同。

```python
import numpy as np
import matplotlib.pyplot as plt

def mu_law_encode(x, mu=100):
    """
    Gato 论文中使用的 mu-law 编码公式 (简化版核心逻辑)。
    F(x) = sgn(x) * ln(1 + mu*|x|) / ln(1 + mu)
    """
    return np.sign(x) * np.log(1 + mu * np.abs(x)) / np.log(1 + mu)

def mu_law_decode(y, mu=100):
    """
    mu-law 的逆变换，用于从 Token 还原回连续值。
    """
    return np.sign(y) * ((1 + mu)**np.abs(y) - 1) / mu

# 参数设置
mu = 100  # Gato 使用的值
x = np.linspace(-1, 1, 1000)
y = mu_law_encode(x, mu)

# 模拟离散化：假设我们只有 50 个 Token (Gato 有 1024 个，这里为了看清用 50 个)
num_bins = 50
# 在编码空间 (y) 中，我们是均匀切分的（因为 Token ID 是连续整数）
y_quantized = np.linspace(-1, 1, num_bins)
# 映射回原始物理空间 (x)
x_quantized_mu = mu_law_decode(y_quantized, mu)
# 对比：线性量化
x_quantized_linear = np.linspace(-1, 1, num_bins)

# --- 绘图 ---
plt.figure(figsize=(14, 6))

# 图 1: 映射函数曲线
plt.subplot(1, 2, 1)
plt.plot(x, y, label=f'$\mu$-law ($\mu={mu}$)', color='blue', linewidth=2)
plt.plot(x, x, label='Linear (No companding)', color='gray', linestyle='--', alpha=0.5)
plt.title('$\mu$-law Mapping Function', fontsize=14)
plt.xlabel('Real Physical Value (Continuous Action)', fontsize=12)
plt.ylabel('Encoded Value (To be Tokenized)', fontsize=12)
plt.grid(True, alpha=0.3)
plt.legend()
plt.text(-0.5, 0.5, "Steep slope near 0 \n= High Sensitivity", color='blue', fontsize=10)
plt.text(0.5, 0.8, "Flat slope near 1 \n= Low Sensitivity", color='blue', fontsize=10)

# 图 2: Token 分布密度 (可视化分辨率)
plt.subplot(1, 2, 2)
plt.title('Where are the Tokens allocated?', fontsize=14)
# 绘制线性量化的点
plt.scatter(x_quantized_linear, np.ones_like(x_quantized_linear)*0.5, 
            color='gray', alpha=0.6, s=20, label='Linear Quantization')
# 绘制 mu-law 量化的点
plt.scatter(x_quantized_mu, np.ones_like(x_quantized_mu)*1.5, 
            color='red', s=20, label=f'$\mu$-law Quantization')

# 为了美观，把 y 轴隐藏，只看 x 轴的分布
plt.yticks([0.5, 1.5], ['Linear', '$\mu$-law'], fontsize=12)
plt.xlabel('Real Physical Value (Action/Sensor)', fontsize=12)
plt.ylim(0, 2)
plt.grid(axis='x', alpha=0.3)
plt.legend(loc='center right')

# 添加解释箭头
plt.annotate('Dense High Precision!', xy=(0, 1.5), xytext=(0, 1.8),
             arrowprops=dict(facecolor='black', shrink=0.05),
             ha='center')
plt.annotate('Sparse (Low Res)', xy=(0.9, 1.5), xytext=(0.7, 1.2),
             arrowprops=dict(facecolor='black', shrink=0.05),
             ha='center')

plt.tight_layout()
plt.show()
```

### **图解分析（教授讲义版）**

运行这段代码后，关注**右边的图**：

1.  **灰色点（Linear）**：
    *   这是传统的线性量化。
    *   可以看到点与点之间的距离是**完全相等**的。
    *   无论你是做微小的调整（0.01）还是大幅度的挥动（0.9），模型可用的“词汇量”是一样的。

2.  **红色点（$\mu$-law）**：
    *   这是 Gato 的做法。
    *   **在中心区域（0 附近）**：点非常密集！这意味着模型有大量的 Token ID 可以用来描述微小的动作变化。这对于机器人**堆叠积木**、**插入插销**这种需要精细力矩控制的任务至关重要。
    *   **在边缘区域（-1 或 1 附近）**：点变得非常稀疏。当机器人需要以最大速度移动时，我们不需要区分它是用 $95\%$ 的力还是 $96\%$ 的力，所以这里浪费 Token 是没有意义的。

### **小结**

**$\mu$-law 本质上是一种“注意力机制”的硬编码版本**——它强迫模型将有限的词表容量（1024 个 bins）更多地分配给**最常发生且最需要精度**的小数值区间。

这也是为什么 Gato 虽然只有 1024 个离散动作值，却能表现出仿佛是连续控制一般的流畅性。



---

**在自动驾驶的加速度（Acceleration）控制中，$\mu$-law 或类似的非线性量化策略不仅“可以用”，而且是“非常有必要用”的。**
我们可以从**“乘客体验（舒适度）”**和**“控制理论（稳定性）”**两个维度来深入探讨为什么加速度也需要这种“偏心”的分布。

### 1. 为什么加速度也符合 $\mu$-law 分布？

自动驾驶中的加速度 $a$（单位 $m/s^2$）和 Gato 中的机械臂力矩惊人地相似：**绝大多数时间都在 0 附近微调，极少数时间需要打满。**

#### A. 舒适区（Comfort Zone）：对精度极其敏感
*   **场景**：高速公路巡航、跟车、缓慢停车。
*   **数值范围**：$[-1.0, 1.0] \ m/s^2$。
*   **需求**：
    *   人类对加速度的变化率（即**加加速度，Jerk**）非常敏感。如果量化精度不够（比如最小步长是 $0.5 \ m/s^2$），车子就会出现“点头”或“顿挫”感。
    *   我们需要区分 $0.05 \ m/s^2$ 和 $0.10 \ m/s^2$ 的区别，这决定了乘客是觉得“老司机在开车”还是“新手在练车”。
*   **$\mu$-law 的作用**：在 0 附近分配极高密度的 Token，保证模型能输出极其细腻的微调指令，实现丝般顺滑的跟车。

#### B. 极端区（Safety/Survival Zone）：对精度不敏感
*   **场景**：紧急避险、前方突然出现障碍物（AEB）、红绿灯起步地板油。
*   **数值范围**：$[-5.0, -9.8] \ m/s^2$ （急刹）或 $[3.0, 5.0] \ m/s^2$ （急加速）。
*   **需求**：
    *   这时候乘客的舒适度已经不重要了，**生存**才是第一位的。
    *   当模型决定要急刹车时，输出 $-8.5 \ m/s^2$ 和 $-8.8 \ m/s^2$ 对结果影响微乎其微——车子都会进入 ABS（防抱死）状态，物理抓地力才是瓶颈。
*   **$\mu$-law 的作用**：在这些极端区域，Token 可以非常稀疏。我们不需要浪费宝贵的词表去区分“非常用力的刹车”和“超级用力的刹车”。

---

### 2. 如果不用 $\mu$-law 会发生什么？（反面教材）

假设我们训练一个像 Gato 这样的 Transformer 来开自动驾驶汽车，输出层是离散的 Token。如果我们使用**线性量化（Linear Quantization）**：

*   **Token 浪费**：大量的 Token 被浪费在 $[-9.8, -5.0]$ 这种几百年才用一次的急刹车区间里。
*   **控制震荡（Oscillation）**：
    *   在 0 附近，Token 的粒度太粗（比如最小刻度是 $0.2 \ m/s^2$）。
    *   当车只需要 $0.1 \ m/s^2$ 的减速时，模型找不到这个 Token，只能被迫选择 $0$ 或者 $0.2$。
    *   **结果**：车子会在“不减速”和“减速过猛”之间来回切换，导致车辆在这个速度点附近**震荡**。这在控制理论中被称为“死区（Dead Zone）效应”或“量化噪声（Quantization Noise）”。

### 3. 自动驾驶中的实际应用案例

事实上，现代端到端自动驾驶模型在设计 Action Space 时，都隐式或显式地遵循了这个逻辑：

1.  **非线性分桶（Non-uniform Binning）**：
    不一定是严格的 $\mu$-law 公式，但通常是**K-Means 聚类**。
    *   研究人员会统计人类驾驶员几千小时的数据，把所有加速度值画成直方图。
    *   你会发现这是一个极尖的**高斯分布（Gaussian-like）**或**拉普拉斯分布**，峰值在 0。
    *   然后用 K-Means 算法自动聚类出 256 或 1024 个中心点。结果就是：0 附近点密，两头点疏。这本质上就是数据驱动的 $\mu$-law。

2.  **混合输出（Hybrid Output）**：
    为了避免离散化的精度损失，更先进的方法（如 **RT-2** 后期版本）有时会采用：
    *   **Token 预测粗略区间**（比如：大概是中等刹车）。
    *   **连续头（Continuous Head）预测偏移量**（比如：在中等刹车的基础上，再加 $0.02$）。

### 4. 总结

无论是机械臂抓积木，还是自动驾驶控制油门，**物理世界的交互都遵循“长尾分布（Long-tailed Distribution）”**。

*   **头部（Head）**：99% 的时间我们在做微小的维持性操作（Maintainance），需要高精度。
*   **尾部（Tail）**：1% 的时间我们在做大幅度的改变（Transition），需要高动态范围。

$\mu$-law 正是解决这种物理世界长尾分布的最佳数学工具之一。
