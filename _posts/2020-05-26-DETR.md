---
layout: post
title: DETR.md
date: 2020-05-26
categories: [Perception]
tags: Perception
---
<!--more-->


- [paper地址](https://arxiv.org/abs/2005.12872)

# End-to-End Object Detection with Transformers

## sketch the paper

DETR是一种结合了CNN和Transformer的深度学习模型，用于目标检测。它首先由CNN提取图像特征并添加位置编码，然后输入Transformer的编码器和解码器。解码器中的对象查询与编码器输出交互，生成预测。损失函数包括L1loss和GIoUloss，采用Hungarian匹配进行标签分配。DETR的优势在于其非局部的Transformer结构，但对小目标检测效果不佳，且训练时间较长。

## key points

1. 方法更加本质,去除掉了像 anchor, nms 的手工设计.

回忆之前的一些基于 anchor 的方法,之所以要做 nms, 是方法本身决定的,即密集搜索式,不管图像上那个位置有没有目标，先用 anchor
覆盖住, 最后根据 anchor 的得分筛选一遍, 而
DETR更加本质的原因就在于，它直接就去找图像中的目标位置,或者说可能有目标的地方, 这个信息我认为就是在 decoder 中, object
query 的作用.
object query 其实是 nn.Embedding, 里面有 weight, 是可学习的,
这可以理解为,在这个检测任务里面,除了图像输入以外，还有一个常值 "1" 输入, 这个常值"1" 经过 object query 的作用,得到的还是
object query. 而 object query 作为参数是可更新的. 

2. object query 的作用很值得回味.

举个形象的例子,它就像是一个会聚焦的相机, 给定图片之后, 它会自动聚焦到它关心的地方.
而这个"关心"的能力是通过数据学习出来的.

输入 head 的是 object query, 而不像有像工作比如 FPN 是 dense feature map, 
单从 输入 head 来看, object query 是高度浓缩的信息, 是取代 dense feature map 的很好的方法.
(后面有些工作就用了这种思想,比如 bevformer 是 dense feature 接 head 做 3d 目标检测, 而 petr 送的就是 object query,
这对于下游更需要速度的任务来说,是一个很大的福利.)

## Methods

1. 一个cnn的backbone, 比如(resnet50), 提图像的feature, 比如, HWC.
2. position_embedding. 
3. feature 与 position Embedding 相加 (在Transformer里面就是二者相加)
4. 输入encoder,(self attention)
5. 输入decoder (这里有object queries 同时作为输入,做 Cross-attention)
6. 然后接Prediction Heads (FFN), 比如分类和回归. (Heads 的输入是 object query)

## adapt to other task

paper 里面说了,类似于 Faster-Rcnn -> Mask-Rcnn, 这里另一个 mask-head，也可以做分割的任务.

The mask head can be trained either jointly, or in a two steps process, where
we train DETR for boxes only, then freeze all the weights and train only the mask
head. 
Experimentally, these two approaches give similar results. (但是后者训练更快.)

To predict the final panoptic segmentation we simply use an argmax over
the mask scores at each pixel, and assign the corresponding categories to the
resulting masks. This procedure guarantees that the final masks have no overlaps. (pixel
上面谁的分高就认为是谁,不需要博弈.)

另外一点,从 paper 中的 attention map 上也可以看出来, 有 object 的位置响应就是高,我认为这也是object query 能作为 mask-head 的 输入的本质原因.

我理解这里 mask-head 的做法是, 比如原始是 100 个 object query, 有 object 的 N 个,  与 image features 做 Multihead attention, 得到
（N, M, h, w） 的 feature map, 这里的 M 是 multihead 的 head 的个数. 然后经过 FPN 类似的结构，最后输出 (N, h, w) 的
logits. 这个的含义是,比如输入的 N个 object query 的第一个是 cat, 那么输出的每一个(h, w) 的 logits,
就代表每一个像素是不是 cat 的概率(经过 sigmoid 之后), 最终 每个 pixel 的位置都会有 N 个概率,
哪个大就选哪个作为最终的输出.

## other

1. object query 有无顺序?  比如位置顺序,类别顺序等.

