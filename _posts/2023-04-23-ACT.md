---
layout: post
title: ACT
date: 2023-04-23
categories: [EmbodiedAI]
toc:
    sidebar: left
    max_level: 4
---

[TOC]

# ACT: Action Chunking With Transformers

[paper link (Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware)](https://arxiv.org/abs/2304.13705) 

今天要深入剖析的这篇论文，在机器人操纵（Robot Manipulation）领域，尤其是基于模仿学习（Imitation Learning）的路线上，具有里程碑式的意义。

这篇论文的题目是 **"Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware"**，这篇工作由斯坦福大学、UC伯克利和Meta的研究人员共同完成，一经发表便在学术界和开源社区引起了巨大的反响。

为什么这篇论文如此重要？主要有两个原因：
第一，它打破了“高精度操作必须依赖昂贵硬件”的固有偏见，证明了低成本硬件（<$20k）配合优秀的算法也能实现穿针引线般的精细操作。 <alphaxiv-paper-citation title="System Cost" page="1" first="The whole system" last="off-the-shelf" />
第二，它提出的 **ACT (Action Chunking with Transformers)** 算法，非常优雅地解决了模仿学习中的误差累积和多模态分布问题，成为了目前模仿学习领域的基准算法之一。

我们将分四个部分详细展开：
1.  **背景与动机**：精细操作的挑战与传统方法的局限。
2.  **硬件系统 (ALOHA)**：低成本遥操作系统的设计哲学与数据收集。
3.  **核心算法 (ACT)**：动作分块、Transformer与CVAE的深度解析（这是今天的重难点）。
4.  **实验与讨论**：算法有效性验证及未来展望。

---

# 第一部分：背景与动机 (Introduction & Motivation)

### 1.1 精细操作（Fine-Grained Manipulation）的挑战

首先，我们要定义什么是“精细操作”。在机器人领域，抓取（Pick and Place）通常被认为是相对简单的任务，通过刚体物理和简单的抓取点检测即可完成。但是，诸如“给扎带穿孔”、“安装电池”或“打开调料杯盖”这样的任务，我们称之为精细操作。

这类任务有三个核心难点：
1.  **极高的精度要求**：误差容忍度通常在毫米级甚至更低。
2.  **丰富的接触力（Contact-Rich）**：涉及复杂的力交互，比如滑入电池时的摩擦力、撬开盖子时的反作用力。
3.  **闭环视觉反馈（Closed-loop Visual Feedback）**：机器人必须根据环境的微小变化（如盖子滑动的角度）实时调整动作。 <alphaxiv-paper-citation title="Fine Manipulation" page="1" first="Fine manipulation tasks" last="visual feedback." />

传统的解决方案通常依赖工业级的高精度机械臂（如KUKA）和昂贵的力传感器。但这篇论文的核心假设是：**既然人类并没有工业级的本体感知精度，却能靠视觉反馈完成精细操作，那么机器人是否也能通过学习算法，在低成本硬件上实现同样的能力？** <alphaxiv-paper-citation title="Human Proprioception" page="1" first="Humans also do" last="proprioception" />

### 1.2 模仿学习中的“复合误差”问题

采用的方法是**端到端模仿学习（End-to-End Imitation Learning）**，即直接从像素（Pixels）映射到关节动作（Joint Actions）。

这里有一个经典的难题，叫做**复合误差（Compounding Errors）**。
在行为克隆（Behavioral Cloning, BC）中，我们假设数据是独立同分布的（i.i.d）。但在实际部署中，如果机器人在 $t$ 时刻产生了一个微小的误差，导致状态偏离了训练数据的分布，那么在 $t+1$ 时刻，策略网络可能会做出更错误的决策，导致误差随着时间迅速累积，最终导致任务失败。对于精细操作，这种现象尤为致命。 <alphaxiv-paper-citation title="Compounding Errors" page="2" first="Small errors in" last="compounding error" />

ACT 算法正是为了解决这个问题而生的。

---

# 第二部分：硬件系统 ALOHA (A Low-cost Open-source Hardware System)

在深入算法之前，先理解数据是如何来的。高质量的数据是模仿学习成功的关键。

### 2.1 遥操作系统的设计 (Teleoperation System)

ALOHA 的全称是 **A Low-cost Open-source Hardware System**。它的设计非常巧妙，采用了“主从式”（Leader-Follower）架构。

*   **从端（Follower）**：两台 ViperX 300 机械臂（每台约数千美元），负责实际执行任务。
*   **主端（Leader）**：两台 WidowX 250 机械臂。这部分是用于操作员手持的。

为什么选择这种配置？
1.  **同构映射**：Leader 和 Follower 虽然型号不同，但运动学结构相似，可以直接通过关节角度映射（Joint-space mapping）进行控制，无需复杂的逆运动学解算。 <alphaxiv-paper-citation title="Teleoperation Setup" page="2" first="They are approximately" last="for teleoperation." />
2.  **被动触觉反馈**：这是 ALOHA 的一个隐形优势。虽然它没有主动的力反馈电机，但由于使用了相同的舵机（Dynamixel），操作员在推拉 Leader 机械臂时，可以通过电机反电动势或机械结构的阻力，隐约感受到从端机械臂是否碰到了障碍物。这种“透明度”对于采集接触丰富的数据至关重要。

### 2.2 数据采集流程

在收集数据时，系统会记录以下信息作为训练数据：
*   **观察（Observations）**：4个摄像头的RGB图像（两个手腕视角，一个顶视，一个前视）以及从端机械臂的当前关节位置。
*   **动作（Actions）**：主端（Leader）机械臂的关节位置序列。注意，这里直接使用 Leader 的位置作为目标位置，利用底层的 PID 控制器驱动 Follower 去追踪。 <alphaxiv-paper-citation title="Action Definition" page="4" first="We record the" last="as actions." />

---

# 第三部分：核心算法 ACT (Action Chunking with Transformers)

ACT 算法结合了**动作分块（Action Chunking）**、**Transformer** 和 **CVAE（条件变分自编码器）**。我们将逐一拆解。

### 3.1 核心思想：动作分块 (Action Chunking)

为了对抗复合误差，作者引入了心理学中的“分块”概念。
传统的 BC 策略是 $\pi(a_t | s_t)$，即根据当前状态预测下一个时间步的动作。
而 ACT 的策略是 $\pi(a_{t:t+k} | s_t)$，即根据当前状态，预测未来 $k$ 个时间步的动作序列。

这样做有两个巨大的好处：
1.  **减少有效视界（Effective Horizon）**：如果任务需要 $T$ 步，每次预测 $k$ 步，那么决策次数就变成了 $T/k$。这直接减少了误差累积的机会。 <alphaxiv-paper-citation title="Horizon Reduction" page="4" first="This implies a" last="the task." />
2.  **处理非马尔可夫行为**：比如人类在操作时可能会停顿一下。这种停顿不仅取决于当前状态，还取决于时间上下文。预测一个序列可以更好地捕捉这种时序上的相关性。

### 3.2 解决多模态问题：CVAE (Conditional VAE)

人类的演示数据通常是 **多模态（Multi-modal）** 的 。
举个例子：当你去抓一个杯子，你可以从左边抓，也可以从右边抓。这两种轨迹都是正确的。
如果我们使用确定性的模型（如简单的回归网络）去训练，模型会试图最小化均方误差（MSE），结果就是预测出两条轨迹的“平均值”——也就是直接撞向杯子中间。

为了解决这个问题，ACT 使用了 CVAE 来建模动作分布。
*   **训练阶段**：
    CVAE 的编码器（Encoder）接收当前的观察（图像+关节）和**真实的动作序列**作为输入，将其压缩成一个潜在变量 $z$（Style Variable）。这个 $z$ 捕捉了动作的“风格”或“模式”。 <alphaxiv-paper-citation title="CVAE Architecture" page="4" first="The encoder of" last="style variable." />
*   **推理阶段**：
    我们没有真实的动作序列了。此时，我们直接设 $z=0$（即先验分布的均值），或者从先验分布中采样。解码器（Decoder）根据观察和这个 $z$ 来生成动作序列。

### 3.3 模型架构详解 (Architecture Detail)

让我们详细剖析一下图 4 和图 11 中的网络结构。这是一个典型的 Transformer Encoder-Decoder 架构。

#### 1. 输入处理
*   **图像**：4个摄像头的图像经过 ResNet-18 主干网络，提取特征图，然后展平（Flatten）并通过线性层投影，加上二维正弦位置编码（2D Sinusoidal Positional Embedding）。
*   **关节状态**：当前的关节角度通过线性层投影。
*   **潜在变量 $z$**：来自 CVAE 编码器或先验分布。

#### 2. Transformer Encoder (编码器)
*   **输入**：上述所有特征（图像特征 + 关节特征 + $z$）拼接在一起作为序列输入。
*   **作用**：利用自注意力机制（Self-Attention），融合不同视角和本体感知的信息。例如，模型需要结合“顶视图看到物体位置”和“手腕图看到抓取点”的信息。

#### 3. Transformer Decoder (解码器)
*   **输入**：这就非常有意思了。解码器的输入不是传统的 shifted output，而是固定的**位置嵌入（Fixed Positional Embeddings）**。
    *   这个位置嵌入的长度就是 $k$（分块的大小）。
    *   你可以理解为我们在询问模型：“请告诉我第1步该怎么做？第2步该怎么做？...第 $k$ 步该怎么做？”
*   **交叉注意力（Cross-Attention）**：解码器通过交叉注意力机制，查询 Encoder 输出的丰富上下文信息。
*   **输出**：通过多层感知机（MLP）预测出未来 $k$ 步的动作序列 $a_{t:t+k}$。 <alphaxiv-paper-citation title="Decoder Function" page="4" first="Right: The decoder" last="transformer decoder." />

### 3.4 时间集成 (Temporal Ensembling)

如果我们在 $t$ 时刻预测了 $t$ 到 $t+k$ 的动作，然后在 $t+1$ 时刻又预测了 $t+1$ 到 $t+k+1$ 的动作，这两次预测在重叠的时间段内可能会不一致，导致动作抖动。

ACT 引入了**时间集成**策略。
*   并不是每 $k$ 步才推理一次，而是**每一步**（或每隔几步）都进行推理。
*   对于每一个时间点 $t_i$，会有来自不同历史时刻预测的多个动作值。
*   算法对这些值进行加权平均。权重采用指数衰减（Exponential Weighting），越近的预测权重越高，或者根据具体实现采用特定的加权函数。
*   **效果**：这使得机器人的动作极其平滑，哪怕是在高频控制下（如 50Hz）也不会出现抖动。 <alphaxiv-paper-citation title="Temporal Ensembling" page="2" first="To further improve" last="action chunks." />

---

# 第四部分：实验与结果分析 (Experiments & Results)

### 4.1 实验设置

作者在真实世界中设计了6个高难度的任务，包括：
1.  **打开调料杯**：需要双手配合，一只手拿杯子，另一只手撬开盖子。
2.  **安装电池**：电池槽公差很小，且有弹簧阻力。
3.  **扎带穿孔（Zip Tie）**：这是最难的任务，孔径极小，且扎带是柔性的。

### 4.2 基线对比 (Baselines)

论文对比了以下几种方法：
*   **BC-ConvMLP**：最基础的行为克隆，使用卷积网络和MLP。
*   **BeT (Behavior Transformer)**：另一种基于Transformer的模仿学习方法。
*   **VINN**：基于最近邻检索的方法。

### 4.3 核心结论

1.  **ACT 显著优于所有基线**。在“滑入电池”任务中，ACT 达到了 **80-90%** 的成功率，而基线方法几乎全部失败。 <alphaxiv-paper-citation title="Success Rate" page="2" first="opening a translucent" last="success" />
2.  **分块（Chunking）至关重要**。消融实验显示，如果不使用分块（即 $k=1$），ACT 的性能会大幅下降。这验证了减少有效视界对于精细操作的必要性。
3.  **CVAE 提升性能**。去除 CVAE（即使用确定性训练）会导致性能下降，特别是在那些具有多模态特性的任务中。

---

# 第五部分：总结

### 5.1 总结

这篇论文告诉我们：
1.  **软硬结合**：低成本硬件可以通过优秀的算法弥补精度的不足。
2.  **算法创新**：ACT 通过 Action Chunking 解决了长序列预测的漂移问题，通过 CVAE 解决了多模态分布问题，通过 Transformer 实现了强大的多感知融合。
3.  **数据效率**：仅用 50 条演示数据（约10分钟）就能训练出一个高鲁棒性的策略，展示了模仿学习在实际应用中的巨大潜力。 <alphaxiv-paper-citation title="Data Efficiency" page="2" first="from only 10" last="demonstration trajectories." />

### 5.2 思考

1.  ACT 依然依赖于大量的演示数据。如果遇到极其罕见的边缘情况（Corner Case），这种纯模仿学习的方法会如何表现？如何引入强化学习（RL）来进行微调？
2.  目前的观察空间仅限于 RGB 图像和本体感知。如果引入深度图（Depth）或者触觉传感器（Tactile Sensors），会对 ACT 的架构产生什么影响？
