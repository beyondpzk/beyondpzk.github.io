---
layout: post
title: GeRo
date: 2026-01-16
categories: []
toc:
    sidebar: left
    max_level: 4
---

[TOC]

# GeRo: Generative Scenario Rollouts for End-to-End Autonomous Driving
[paper link](https://arxiv.org/abs/2601.11475)

### takeaways for me:
1. rollouts & GRPO
2. planning head. (猜测CVAE), 这和之前看到的用mlp,或者drive policy不一样. 有独到之处.



要剖析的这篇论文是 **"Generative Scenario Rollouts for End-to-End Autonomous Driving" (简称 GeRo)**。这篇工作由高通 AI 研究院（Qualcomm AI Research）提出，它代表了当前自动驾驶领域的一个重要趋势：将视觉-语言-动作（VLA, Vision-Language-Action）模型从单纯的“模仿学习者”进化为具备“生成式推演能力”的智能体。

我们将从背景挑战、GeRo 核心框架、生成式场景推演（Scenario Rollouts）、基于 GRPO 的强化学习优化以及实验分析五个维度进行深入拆解。

---

### 第一部分：背景与动机 (Introduction & Motivation)

#### 1.1 端到端自动驾驶的演进
在深入 GeRo 之前，我们需要理解当前的语境。传统的自动驾驶系统通常采用模块化设计（感知、预测、规划分离），而近年来的趋势是转向端到端（End-to-End）的学习范式，直接从原始传感器输入映射到车辆控制信号或轨迹。

而在端到端范式中，**VLA（Vision-Language-Action）模型**正在崛起。这类模型利用大语言模型（LLM）的推理能力，将语言上下文整合到运动规划中，试图实现更具解释性和逻辑性的驾驶决策。 <alphaxiv-paper-citation title="VLA Context" page="1" first="Vision-Language-Action (VLA) models" last="autonomous driving systems." />

#### 1.2 现有 VLA 模型的局限性
尽管 VLA 模型引入了语言推理，但作者敏锐地指出了当前方法的四个核心痛点：

1.  **稀疏的语言-动作监督 (Sparse language-action supervision)**：
    现有的数据集往往只提供场景级别的描述，缺乏与时间轴紧密结合的细粒度动作描述。例如，模型可能难以区分“变道”和“超车”在时序上的细微差别。 <alphaxiv-paper-citation title="Sparse Supervision" page="1" first="Sparse language-action supervision:" last="multi-step maneuvers [10]." />

2.  **生成能力的未充分利用 (Under-utilized generative capability)**：
    目前的 VLA 方法大多仅依赖模仿学习（Imitation Learning）来拟合专家轨迹，忽略了模型本身作为生成式模型（Generative Model）进行自回归推演和探索的潜力。 <alphaxiv-paper-citation title="Generative Potential" page="1" first="Under-utilized generative capability:" last="reasoning and exploration." />

3.  **描述性语言 vs. 程序性语言**：
    目前的语言监督多是描述“正在发生什么”（Descriptive），而不是指导“如何执行”（Procedural），这限制了规划层面的逻辑深度。

4.  **语言-动作错位 (Language-action misalignment)**：
    由于很多数据集的指令是事后生成的，往往会出现视觉输入与语言指令不匹配的情况（例如视觉上是红灯，但语言指令却是加速），导致模型产生幻觉。 <alphaxiv-paper-citation title="Misalignment" page="2" first="Language-action misalignment:" last="paired with acceleration." />

**GeRo 的核心思想**：提出一个即插即用（Plug-and-Play）的框架，不仅利用 VLA 做规划，更利用其进行**生成式场景推演（Generative Scenario Rollouts）**，即在潜空间（Latent Space）中自回归地生成未来的交通场景和自我意图，并利用强化学习来对齐语言和动作。

---

### 第二部分：GeRo 模型架构与预训练 (Architecture & Pretraining)

GeRo 的架构设计是为了让 VLA 模型能够理解并生成动态的驾驶场景。整个流程分为两个阶段：预训练（Pretraining）和生成式推演（Generative Scenario Rollout）。

#### 2.1 基础架构 (Backbone)
GeRo 构建在多模态大语言模型之上（例如 Qwen2.5-VL）。其核心组件包括：

*   **视觉编码器 (Vision Encoder)**：使用预训练的 ViT（如 EVA-ViT）将多视角图像转化为视觉 Token。
*   **文本 Tokenizer**：处理场景描述和问答文本。
*   **LLM 核心**：作为中枢大脑，将多模态 Token 投影到共享的潜空间（Shared Token Space）。
*   **输出头 (Output Heads)**：
    *   **生成式规划头 (Generative Planning Head)**：通常是一个变分自编码器（VAE），用于解码自车轨迹。
    *   **运动预测头 (Motion Prediction Head)**：用于预测周围交通参与者（Agents）的未来轨迹和边界框。 <alphaxiv-paper-citation title="Architecture Details" page="7" first="GeRo leverages the" last="motion prediction heads." />

#### 2.2 第一阶段：预训练 (Pretraining)
这一阶段的目标是学习一个紧凑且语义丰富的**潜空间表示 (Latent Tokenization)**。模型需要将自车（Ego）和他车（Agents）的动力学特征编码为潜变量 Token。

在预训练中，模型同时接受三个任务的监督：

1.  **规划任务 (Planning Task)**：预测自车的未来轨迹。
2.  **运动预测任务 (Motion Prediction Task)**：预测场景中其他 Agent 的轨迹和 3D 边界框。这对于理解场景动态至关重要。
3.  **视觉问答任务 (VQA Task)**：生成场景描述并回答关于自车行为的问题。

**数学表达**：
预训练的损失函数 $L_{pre}$ 由三部分组成：
$$L_{pre} = L_{plan} + L_{mot} + L_{VLA}$$

其中，$L_{plan}$ 使用 L1 损失回归航点；$L_{mot}$ 结合了分类损失（Focal Loss）和回归损失（L1 Loss 用于轨迹和 BBox）；$L_{VLA}$ 是标准的语言交叉熵损失。 <alphaxiv-paper-citation title="Pretraining Loss" page="3" first="The planning head" last="Lplan + Lmot + LVLA." />

这一步的关键在于“绑定”：将语言表征与行为表征在潜空间中强耦合，为后续的文本对齐生成打下基础。 <alphaxiv-paper-citation title="Pretraining Goal" page="2" first="This stage is" last="language-action misalignment." />

---

### 第三部分：生成式场景推演 (Generative Scenario Rollouts)

这是 GeRo 最核心的创新点。模型不仅仅是一次性输出轨迹，而是像在大脑中“预演”未来一样，进行自回归的生成。

#### 3.1 推演机制 (Rollout Mechanism)
给定当前时刻 $t$ 的多视角图像、场景描述 $s$ 和关于自车行为的问题 $q$，GeRo 执行以下步骤：

1.  **编码**：计算当前的自车潜变量 Token $z_e^t$ 和他车潜变量 Token $\{z_{a_i}^t\}$。
2.  **自回归生成**：利用 LLM 预测**下一时刻** $t+1$ 的潜在 Token $\{\tilde{z}^{t+1}\}$ 以及针对当前问题的文本回答。
3.  **解码**：将预测出的 Token 解码为具体的自车轨迹、他车轨迹。
4.  **循环**：将预测出的 $t+1$ 时刻的 Token 作为新的输入，配合更新后的场景描述，继续预测 $t+2$ 时刻，如此循环 $T$ 步。 <alphaxiv-paper-citation title="Rollout Process" page="3" first="Given latent tokens" last="ego-action questions." />

#### 3.2 为什么需要推演？
这种机制允许模型进行**长时程推理 (Long-horizon Reasoning)**。传统的 VLA 往往只关注单步输出，而 GeRo 通过 Rollout 强制模型思考：“如果我这样做，环境会变成什么样？接下来的动作是什么？”

#### 3.3 推演一致性损失 (Rollout-Consistency Loss)
为了防止自回归过程中的误差累积（Drift），作者引入了推演一致性损失。

*   **潜空间对齐**：使用 KL 散度（KL-Divergence），强制推演生成的潜变量分布与预训练模型（Teacher）在未来时刻生成的潜变量分布保持一致。
*   **轨迹监督**：如果有 Ground Truth，则直接监督生成的轨迹。

这种设计使得 GeRo 能够生成在时间上连贯（Temporally Consistent）且以语言为基础（Language-Grounded）的推演序列。 <alphaxiv-paper-citation title="Consistency Loss" page="2" first="Predictions are stabilized" last="using KL-Divergence." />

---

### 第四部分：基于 GRPO 的强化学习 (RL with GRPO)

仅仅依靠模仿学习（Imitation Learning）是不够的，因为存在协变量偏移（Covariate Shift）问题，且模仿学习难以处理长尾场景。GeRo 引入了强化学习来进一步优化推演过程。

#### 4.1 GRPO 算法
GeRo 采用了 **GRPO (Group Relative Policy Optimization)** 算法。与 PPO 需要额外的价值网络（Value Network）不同，GRPO 通过对一组输出进行采样并计算其相对优势，从而更加高效且稳定，特别适合大语言模型的微调。 <alphaxiv-paper-citation title="RL Strategy" page="2" first="Therefore, we introduce" last="scenario rollouts." />

#### 4.2 奖励函数设计 (Reward Engineering)
为了兼顾驾驶安全性和语义一致性，作者设计了一套新颖的奖励函数：

1.  **安全性奖励**：包含碰撞避免（Collision Avoidance）和碰撞时间（Time-to-Collision, TTC）。这是硬约束，确保规划出的轨迹是物理安全的。
2.  **语言对齐奖励**：利用问答对（Q&A）作为反馈。模型生成的文本解释必须与实际生成的轨迹相匹配。这增强了模型的可解释性（Interpretability）。 <alphaxiv-paper-citation title="Reward Functions" page="2" first="We propose a" last="time-to-collision." />

通过 RL，模型学会了在生成的幻境中“试错”，并根据安全和逻辑的反馈来调整其策略。

---

### 第五部分：实验结果与讨论 (Experiments & Discussion)

#### 5.1 闭环测试 (Closed-Loop Evaluation)
在 Bench2Drive 榜单上，GeRo 展现了显著的提升。
*   **基线对比**：相比于基础的 Qwen2.5-VL 模型，GeRo (Qwen) 将驾驶得分（Driving Score）提升了 **+15.7**，成功率（Success Rate）提升了 **+26.2%**。
*   **SOTA 对比**：即便是对比强大的 ORION 模型，GeRo 也能带来明显的增益。 <alphaxiv-paper-citation title="Closed-Loop Results" page="7" first="On Bench2Drive," last="respectively." />

#### 5.2 开环测试与零样本泛化 (Open-Loop & Zero-Shot)
在 nuScenes 数据集上，GeRo 展示了极强的泛化能力。
*   **轨迹误差**：相比基线，L2 轨迹误差降低了约 60-70%。
*   **零样本能力**：即便是在 Bench2Drive 上训练，直接在 nuScenes 上测试（Zero-shot），GeRo 依然保持了极低的碰撞率，这证明了生成式推演学到了通用的驾驶逻辑，而非死记硬背训练集。 <alphaxiv-paper-citation title="Zero-Shot Results" page="8" first="In the zero-shot" last="scenario-grounded rollouts." />

#### 5.3 案例分析 (Qualitative Analysis)
论文展示了定性结果（图 4）。在复杂的路口交互、恶劣天气下的事故处理中，GeRo 不仅能生成安全的轨迹，还能生成与动作高度对齐的文本解释（例如：“减速让行，因为检测到行人”）。这种**言行一致**是传统黑盒模型无法做到的。 <alphaxiv-paper-citation title="Qualitative Examples" page="8" first="Each frame includes" last="safety-aware decision." />

#### 5.4 消融实验 (Ablation Study)
通过消融实验（表 4），作者证明了每一个组件的重要性：
*   加入 **Scenario Description** 和 **VQA** 提升了基础性能。
*   加入 **Rollout Consistency Loss** 显著提升了推演的稳定性。
*   加入 **GRPO (RL)** 进一步大幅提升了长尾场景下的成功率。 <alphaxiv-paper-citation title="Ablation Analysis" page="8" first="Using collision and" last="scenario rollouts." />

---

### 总结与思考 (Conclusion)

GeRo 的成功向我们展示了自动驾驶研究的一个新范式：**Thinking Fast and Slow**。
*   传统的端到端模型像是 "Thinking Fast"（直觉反应）。
*   GeRo 引入的生成式推演则像是 "Thinking Slow"（逻辑推理与预演）。

通过将 VLA 模型与自回归生成、强化学习相结合，GeRo 不仅提高了驾驶的安全性，更重要的是赋予了自动驾驶系统**可解释的推理能力**。

**思考**：
1.  GeRo 中的 Rollout 机制会增加推理延迟（Latency），在实际车端部署时应如何平衡生成深度与实时性？
2.  GRPO 的奖励函数目前主要关注安全，如何设计更复杂的奖励函数来体现“舒适性”或“礼貌驾驶”？

## 论文中Figure 2详细解读


### 第一部分：左侧——多任务预训练与潜空间构建 (Stage 1: Multi-Task Pretraining)

Figure 2 的左半部分。这一阶段的核心目标是**“压缩与对齐”**。模型需要将高维的图像信息和复杂的文本信息，压缩成紧凑的数学向量（Latent Tokens），并让这些向量同时包含视觉语义和运动物理规律。

**数据流向示例：**
假设我们的自车正行驶在一个繁忙的十字路口，准备左转。
1.  **输入层**：模型接收两类输入。一是**多视角图像 ($I_t$)**，比如前视摄像头拍到了红绿灯变绿，侧视摄像头拍到了斑马线上有一个正在过马路的行人；二是**文本提示 ($P_{text}$)**，比如“当前任务：在路口左转”。
2.  **编码层**：图像经过 Vision Encoder（如 EVA-ViT）处理，文本经过 Text Encoder 处理。
3.  **LLM 处理与 Tokenization**：这些特征进入 LLM 主干网络。关键点来了，模型将整个场景的动态——包括自车的状态和他车（那个行人）的状态——编码为一组**潜变量 Token ($z_t$)**。
4.  **多任务输出头**：为了确保这些 $z_t$ Token 真的听懂了物理规律，Figure 2 展示了三个监督信号：
    *   **Motion Head (运动头)**：利用 $z_t$ 预测行人的未来。比如，它预测行人下一秒会移动到 $(x=12.5, y=5.0)$ 的位置。
    *   **Planning Head (规划头)**：利用 $z_t$ 预测自车的轨迹。比如，预测自车未来 3 秒的轨迹是一条平滑的左转曲线，且在第 1 秒时速度从 30km/h 降至 10km/h（为了避让行人）。
    *   **VQA Head (问答头)**：利用 $z_t$ 回答文本问题。当被问到“为什么要减速？”时，输出“因为检测到行人正在横穿马路”。

通过这一阶段，模型学会了将“图像像素”转化为“物理概念”和“语言逻辑”。 <alphaxiv-paper-citation title="Pretraining Structure" page="3" first="Specifically, the VLA" last="a shared latent space." />

### 第二部分：右侧——生成式场景推演 (Stage 2: Generative Scenario Rollouts)

现在我们将视线移到 Figure 2 的右半部分，这是 GeRo 的灵魂所在。这一阶段不再是简单的单步预测，而是**时间维度上的自回归生成**。模型开始像人类一样在大脑中“预演”未来的发展。

**推演过程示例：**
继续刚才的路口场景。现在时间是 $t=0$。
1.  **启动推演 ($t \to t+1$)**：
    模型拿着 $t=0$ 时刻的潜变量 $z_0$（包含了当前行人和车的状态）以及一个动作问题 $q$（例如“接下来该怎么做？”），输入到 LLM 中。
    LLM **不是**直接输出轨迹，而是**生成**了下一时刻 $t=1$ 的潜变量 Token $\tilde{z}_1$ 和一段文本解释。
    *   **生成的文本**：“我应该减速并观察行人是否通过。”
    *   **生成的 Token $\tilde{z}_1$**：这个隐向量代表了模型“想象”出的下一秒世界状态。

2.  **解码与验证**：
    我们把生成的 $\tilde{z}_1$ 扔给 Planning Head 和 Motion Head 解码，发现解码出的自车位置前进了 2 米（减速了），而行人的位置向路中间移动了 1 米。这与物理规律是自洽的。

3.  **持续推演 ($t+1 \to t+2$)**：
    模型将刚刚生成的 $\tilde{z}_1$ 作为新的历史记忆，继续询问自己：“然后呢？”
    LLM 再次生成 $t=2$ 的 Token $\tilde{z}_2$ 和文本：“行人已通过车道，可以开始加速。”
    此时解码出的轨迹显示自车速度回升到 20km/h，轨迹开始大幅度左转。

Figure 2 中展示的循环箭头正是这个**自回归（Autoregressive）**过程。它强调了模型是在**潜空间（Latent Space）**中进行推理，而不是在像素空间，这大大降低了计算量并提高了语义一致性。 <alphaxiv-paper-citation title="Rollout Mechanism" page="3" first="Next, GeRo performs" last="long-horizon rollouts." />

### 第三部分：一致性损失与强化学习 (Rollout-Consistency & GRPO)

在 Figure 2 的右下角，你会看到几个关键的损失函数标记，它们是保证推演质量的“监工”。

**1. 推演一致性损失 ($L_{roll}$)**：
模型在“想象”未来时容易跑偏（Drift）。比如推演到第 3 秒，模型可能幻想行人突然消失了。为了防止这种情况，Figure 2 展示了通过 KL 散度（KL-Divergence）来约束生成的分布。简单来说，就是强制要求模型“想象”出的未来 $\tilde{z}_{t+k}$，必须和如果我们真的把车开到那个时刻看到的真实状态（或教师模型的预测）尽可能一致。这确保了推演的**物理真实性**。

**2. GRPO (Group Relative Policy Optimization)**：
这是 Figure 2 中提到的强化学习部分。模型可能会生成多种可能的未来（比如“激进抢行”和“保守礼让”）。GRPO 算法会根据奖励函数（Reward）对这些生成的剧本进行打分：
*   如果模型选择了“抢行”导致碰撞，**Collision Reward** 会给出极低的负分。
*   如果模型选择了“礼让”且文本解释合理，**Text Alignment Reward** 会给出高分。
通过这种机制，Figure 2 展示了模型如何通过自我博弈和反馈，逐渐学会选择最安全、最符合人类逻辑的驾驶策略。 <alphaxiv-paper-citation title="RL Optimization" page="2" first="By integrating reinforcement" last="open-loop performance." />

### other

1. 为了给grpo使用,会进行多次的rollouts， 产生多种可能的未来.
2. ego token, agent tokens 都是learnable tokens, 或者认为是query, ego token有一个, agent tokens有N个, QA的answer在最后面.


## planning head的结构是一个CVAE

在 **3.2 Architecture** 和 **3.3 Pretraining** 章节中，作者明确提到了这一点。这种设计不是随意的，而是为了解决自动驾驶中一个核心问题：**多模态性 (Multi-modality)** 和 **生成能力的增强**。

这个 VAE Planning Head 的结构和工作原理。

### 1. 为什么用 VAE？(Why VAE?)

如果仅仅是用一个 MLP（多层感知机）去回归轨迹（Regression），模型往往会倾向于预测一条“平均轨迹”（Mean Trajectory）。
*   *例子*：前方有障碍物，左边能绕，右边也能绕。
*   *MLP 结果*：输出一条撞向障碍物的中间轨迹（因为左+右的平均值在中间）。
*   *VAE 结果*：学习一个潜在分布（Latent Distribution），可以采样出“左绕”或“右绕”两条截然不同的轨迹。

### 2. Planning Head 的具体结构

虽然论文没有画出详细的 Head 内部图，但根据其描述和标准 VAE 结构，我们可以推断出如下架构：

#### A. 输入端 (Input)
*   **Conditioning (条件)**：来自 LLM 的 **Ego Token ($z_e$)**。这个 $z_e$ 包含了当前环境的上下文信息（Context）。
*   **Target (训练时)**：真实的未来轨迹 $\tau_{gt}$（Ground Truth Trajectory）。

#### B. 编码器 (Encoder - 仅训练时使用)
*   **结构**：一个神经网络（通常是 MLP 或简单的 Transformer Encoder）。
*   **输入**：$z_e$ (条件) + $\tau_{gt}$ (目标)。
*   **输出**：预测潜在变量的分布参数——均值 $\mu$ 和方差 $\sigma$。
    $$ q_\phi(z_{plan} | z_e, \tau_{gt}) = \mathcal{N}(\mu, \sigma^2) $$
*   **作用**：将“当前情况 $z_e$ 下应该走的轨迹 $\tau_{gt}$”编码为一个高斯分布。

#### C. 潜在空间采样 (Latent Sampling)
*   利用重参数化技巧 (Reparameterization Trick)：
    $$ z_{sample} = \mu + \epsilon \cdot \sigma, \quad \epsilon \sim \mathcal{N}(0, I) $$
    这个 $z_{sample}$ 就是具体的“意图向量”。

#### D. 解码器 (Decoder - 训练/推理都用, 我理解即planning head)
*   **结构**：另一个神经网络（MLP / GRU / Transformer Decoder）。
*   **输入**：$z_e$ (条件) + $z_{sample}$ (意图)。
*   **输出**：重构的轨迹 $\hat{\tau}$。
    $$ \hat{\tau} = \text{Decoder}(z_e, z_{sample}) $$
*   **推理时 (Inference)**：直接从标准正态分布 $\mathcal{N}(0, I)$ 中采样 $z_{sample}$，或者直接取均值（如果只想最可能的轨迹），结合 $z_e$ 输入 Decoder 生成轨迹。

### 3. Loss Function (损失函数)

VAE 的训练包括两部分损失：
1.  **重构损失 (Reconstruction Loss)**：
    $$ L_{recon} = || \hat{\tau} - \tau_{gt} ||_1 $$
    希望解码出来的轨迹和真实轨迹越像越好。
2.  **KL 散度 (KL Divergence)**：
    $$ L_{KL} = D_{KL}(q_\phi(z_{plan}|...) || \mathcal{N}(0, I)) $$
    强制让 Encoder 学到的分布接近标准正态分布，防止过拟合，保证采样的多样性。

在 GeRo 中，这个 VAE Head 作为一个整体被集成在 LLM 的输出端。

### 小结

GeRo 的 Planning Head 不是简单的回归器，而是一个**条件 VAE (CVAE)**。
*   **输入**：LLM 提取的 Ego Token ($z_e$)。
*   **核心**：引入随机变量 $z_{plan}$。
*   **输出**：多样化的未来轨迹分布。

这种结构是 GeRo 能够进行 **Generative Scenario Rollouts** 的基石——它允许模型在遇到不确定路况时，不仅能给出一个“最佳答案”，还能探索“多种可能性”。


## 未来推演的多样性来源


GeRo 中**多样性（Diversity）的双重来源**。

**在单步轨迹生成中，VAE 确实负责多样性；但在长时程推演（Rollout）中，LLM 的 Top-K 才是主角。**

GeRo 的多样性其实分两个层次：**“微观动作多样性”** 和 **“宏观剧本多样性”**。

### 1. 微观动作多样性：由 VAE 负责 (Spatial Diversity)

这是指在**同一个时间步 $t$** 内，面对相同的情境 $z_e^t$，车辆具体的**轨迹几何形状**可能有多种微小的变化。

*   **场景**：前面有个水坑，你是从左边绕一点，还是从右边绕一点？或者仅仅是车速快一点慢一点？
*   **机制**：**VAE Planning Head**。
    *   输入：固定的 Ego Token $z_e^t$（来自 LLM）。
    *   采样：从 VAE 的隐空间采样不同的 $\epsilon$。
    *   输出：$\tau_{left}, \tau_{right}$（几何上的不同轨迹）。
*   **作用**：主要处理**连续控制空间**的不确定性，保证轨迹的平滑和多模态。

### 2. 宏观剧本多样性：由 LLM Top-K 负责 (Temporal/Semantic Diversity)

这是指在**时间轴 $t \to t+1 \to t+2$** 的推演过程中，整个**场景发展方向**的分歧。

*   **场景**：黄灯亮了。
    *   **剧本 A**：加速冲过去 $\to$ 下一秒已经在路口中间 $\to$ 再下一秒过了路口。
    *   **剧本 B**：急刹车 $\to$ 下一秒停在停车线前 $\to$ 再下一秒静止不动。
*   **机制**：**LLM Autoregressive Generation**。
    *   输入：历史 Context。
    *   采样：**Top-K Sampling** 预测下一个 **Latent Token $z_e^{t+1}$**。
    *   输出：$z_{accel}$ (代表加速意图的 Token) 或 $z_{brake}$ (代表刹车意图的 Token)。
*   **作用**：决定了**离散语义空间**的走向。这是 GeRo "Scenario Rollout" 的核心。

### 3. 为什么论文强调 LLM 的 Top-K？

因为 **GeRo 的核心贡献是“Scenario Rollout”（场景推演）**。

*   如果只是 VAE 多样性，那是传统的 CVAE-Planner 就能做的（比如 VAD, UniAD 也有类似机制）。
*   GeRo 的突破在于：它能**推演未来**。
    *   它不是说“我现在可能怎么走”（VAE）。
    *   它是说“如果我现在决定加速（由 LLM 采样决定），那么**下一秒的世界**（Agent Tokens $z_a^{t+1}$ 和 Ego Token $z_e^{t+1}$）会变成什么样”。

**LLM 的采样决定了“剧情分支”，而 VAE 只是负责把这个剧情“画”成具体的轨迹线。**

### 总结

我们可以把 GeRo 比作一个**导演（LLM）**和一个**动作指导（VAE）**：

1.  **导演（LLM）说**：“这场戏，主角要**激进地超车**！”（这是通过 **Top-K 采样** 选定的剧本方向）。
2.  **动作指导（VAE）执行**：“好，既然要超车，具体的路线是**向左打方向盘 30 度**。”（这是通过 **VAE 采样** 生成的具体轨迹）。

如果导演选了另一个剧本：“主角要**保守跟车**”，那么动作指导就会生成完全不同的轨迹。

所以，**“多种可能的未来”主要由 LLM 的采样（决定意图和状态流转）主导，VAE 负责将这些意图落实为具体的物理轨迹。** <alphaxiv-paper-citation title="Two-stage Generation" page="3" first="Next, GeRo performs" last="long-horizon rollouts." />
