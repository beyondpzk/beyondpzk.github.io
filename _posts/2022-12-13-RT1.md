---
layout: post
title: RT1
date: 2022-12-13
categories: [VLA]
toc:
    sidebar: left
    max_level: 4
---

[TOC]

# RT1: Robotics Transformer for Real-World Control at Scale

[paper link](https://arxiv.org/abs/2212.06817) 

这篇论文由 Google Robotics（包括现在的 Google DeepMind）团队发表。

**背景**
在计算机视觉（CV）和自然语言处理（NLP）领域，我们已经见证了从“专用小模型”向“通用大模型”的范式转变。GPT 系列和 CLIP 等模型的成功告诉我们：**开放式的任务无关训练（Open-ended task-agnostic training）配合高容量的架构（High-capacity architectures），能够通过吸收海量数据产生涌现能力。**

然而，在机器人领域，这个故事讲得并不顺利。机器人数据的获取极其昂贵，物理世界的交互极其复杂。今天的核心问题是：**我们能否在机器人领域复刻 NLP 的成功？能否训练一个单一的、通用的多任务骨干模型（Backbone），让它展现出对新任务、新环境和新物体的零样本（Zero-shot）泛化能力？**

RT-1 就是对这一问题给出的有力回答。

---

# 第一部分：引言与核心理念 (Introduction & Motivation)

### 1.1 机器人学习的现状与挑战
传统的机器人学习（无论是模仿学习还是强化学习）通常遵循“孤岛式”流程：
1.  定义一个特定任务（例如：抓取苹果）。
2.  收集该特定任务的数据。
3.  训练一个专用模型。

这种方法导致模型无法在任务之间共享知识。虽然近期出现了一些多任务策略（如 Gato, BC-Z），但它们要么在真实世界任务的广度上受限，要么在未见过的任务上泛化性能不佳。

### 1.2 RT-1 的核心假设
论文提出了一个核心论点：机器人通用模型的成功关键在于两点：
1.  **大规模、多样化的数据集**：能够覆盖广泛的现实世界场景。
2.  **高容量的模型架构**：能够“像海绵一样”吸收这些数据中的所有异构经验。

### 1.3 路线图
接下来，我们将从三个维度解构 RT-1：
*   **数据（The Data）**：如何构建通过 13 台机器人历时 17 个月收集的大规模数据集。
*   **模型（The Model）**：RT-1 的架构设计，特别是如何平衡高容量 Transformer 与实时控制（Real-time Control）的需求。
*   **实验（The Experiments）**：如何科学地评估泛化能力，以及跨形态（Cross-embodiment）数据传输的实验结果。

---

# 第二部分：RT-1 模型架构详解 (Model Architecture)


### 2.1 总体设计思路
RT-1 是一个基于 Transformer 的模型，它接收**图像序列**和**自然语言指令**作为输入，直接输出**离散化的动作 Token**。

*   **输入**：$T$ 个时刻的图像历史 + 文本指令。
*   **输出**：当前时刻的动作 $a_t$。
*   **控制频率**：3 Hz。这是一个硬性约束，意味着模型推理加上系统延迟必须控制在 100ms 以内。这对庞大的 Transformer 来说是一个巨大的挑战。

### 2.2 详细组件分析 (参考论文 Figure 3)

我们将模型拆解为三个阶段：**编码（Tokenizer）** -> **压缩（TokenLearner）** -> **序列建模（Transformer Backbone）**。

#### A. 图像与语言的联合 Tokenization (FiLM-EfficientNet)
为了将高维图像和文本转化为 Transformer 可以处理的 Token，RT-1 并没有简单地将图像切片（Patchify，如 ViT 的做法），而是采用了更高效的卷积网络提取特征。

1.  **文本编码**：使用通用句子编码器（Universal Sentence Encoder, USE）将自然语言指令嵌入为向量。
2.  **图像主干**：使用在 ImageNet 上预训练的 **EfficientNet-B3**。
    *   *输入*：6 张 300x300 的历史图像。
    *   *输出*：9x9x512 的特征图（Feature Map）。
3.  **多模态融合（FiLM 层）**：这是关键点。
    *   RT-1 使用 **FiLM (Feature-wise Linear Modulation)** 层将文本嵌入注入到 EfficientNet 中。
    *   *机制*：文本嵌入经过全连接层生成仿射变换参数（$\gamma, \beta$），对图像特征图进行逐通道的缩放和平移：$Feature_{new} = \gamma \cdot Feature_{old} + \beta$。
    *   *技巧*：为了不破坏 EfficientNet 的预训练权重，FiLM 层初始化为恒等变换（Identity-initialized），即初始化 $\gamma=1, \beta=0$。这使得视觉特征在训练初期得以保留，从而加速收敛。
    *   最终，EfficientNet 输出被展平为 **81 个视觉 Token**。

#### B. Token 压缩 (TokenLearner)
如果直接将 6 张图像 * 81 个 Token 输入 Transformer，序列长度将达到 486，这对实时性是致命的（Transformer 的计算复杂度随长度呈二次方增长）。

RT-1 引入了 **TokenLearner (Ryoo et al., 2021)**：
*   **作用**：它学习一组空间注意力掩码（Spatial Attention Masks），从 81 个原始 Token 中筛选并聚合出信息量最大的特征。
*   **结果**：每张图像的 Token 数从 81 被压缩到了 **8** 个。
*   **优势**：这种压缩使得推理速度提升了约 2.4 倍，是 RT-1 能在真机上跑通 3Hz 控制循环的关键。

#### C. Transformer 主干 (Backbone)
*   **结构**：Decoder-only Transformer。
*   **参数**：8 层 Self-attention，总参数量约 35M（整个模型总参数 76M）。
*   **输入序列**：6 张历史图像 * 8 个 Token = 48 个 Token。加上位置编码。

#### D. 动作输出 (Action Tokenization)
RT-1 将连续的机器人控制问题转化为离散的分类问题：
*   **动作空间**：
    *   7 个手臂维度（x, y, z, roll, pitch, yaw, 夹爪开合）。
    *   3 个底座维度（x, y, yaw）。
    *   1 个模式维度（控制手臂 / 控制底座 / 终止任务）。
*   **离散化**：每个维度被离散化为 **256 个 Bin**（区间）。
*   **损失函数**：标准的分类交叉熵损失（Categorical Cross-entropy）。

这种“图像+语言 -> 离散动作”的设计，本质上是将机器人控制看作是一个序列建模问题（Sequence Modeling）。这与 GPT 预测下一个单词在数学形式上是完全一致的。

---

# 第三部分：大规模数据构建 (Data at Scale)

在深度学习时代，数据就是护城河。RT-1 的强大很大程度上归功于其背后的数据工程。

### 3.1 数据集规模
*   **总演示数**：约 130,000 条（Episodes）。
*   **采集方式**：人类通过 VR 设别进行远程遥操作（Teleoperation）。这是高质量的专家数据。
*   **采集周期**：17 个月。
*   **硬件规模**：13 台 Everyday Robots (EDR) 移动操作机器人。

### 3.2 任务与指令的多样性
RT-1 定义了 **700 多条** 独特的指令。这些指令由动词（Skill）和名词（Object）组合而成。
*   **技能（Skills）**：抓取（Pick）、放置（Place）、打开抽屉（Open drawer）、扶正物体（Place upright）、推倒物体（Knock over）等。
*   **物体**：涵盖了各种各样的厨房用品、零食、文具。
*   **环境**：主要是在办公楼的厨房场景中采集。分为“训练厨房”和“真实厨房”。

### 3.3 数据分布的重要性
论文中特别提到，为了让“抓取（Pick）”这个技能具备泛化性，他们特意引入了大量多样化的物体进行抓取训练。而对于其他技能，物体集合相对固定。这种非均匀的数据分布是机器人学习中的常态。

---

# 第四部分：实验结果与分析 (Experiments)

我们要如何科学地评估一个通用机器人模型？仅仅看训练集上的成功率是毫无意义的。RT-1 的评估体系涵盖了四个层级：

### 4.1 评估基准与设置
*   **基准模型（Baselines）**：
    *   **Gato**：DeepMind 的多模态通用模型。
    *   **BC-Z**：之前的 SOTA 模仿学习方法，使用 ResNet + RNN。
*   **评估指标**：任务成功率（Success Rate）。

### 4.2 核心实验结果
我们关注三个维度的泛化能力：

1.  **已知任务（Seen Tasks）**：
    *   在训练过的任务上，RT-1 达到了 **97%** 的惊人成功率。
    *   相比之下，Gato 只有 51%，BC-Z 只有 72%。这说明高容量架构在拟合大规模数据方面具有绝对优势。

2.  **未见任务（Unseen Tasks）**：
    *   这是真正的考验。任务指令从未在训练集中出现，但其组成的技能和物体是见过的。
    *   RT-1 成功率 **76%**。这表明模型学会了组合性（Compositionality），比如它学过“抓苹果”和“放盘子里”，就能执行“抓苹果放盘子里”。

3.  **抗干扰与背景鲁棒性（Distractors & Backgrounds）**：
    *   **L1（新环境）**：换个光照、换个桌子，RT-1 依然稳健。
    *   **L2（干扰物）**：桌上放满没见过的杂物。
    *   **L3（新场景）**：完全不同的真实厨房。
    *   在最具挑战性的 L3 场景中，RT-1 的表现明显优于基准模型，泛化性能提升极大。

> **关键图表解读**：论文中的 **Figure 4**。随着数据量的增加（从 10% 到 100%），RT-1 的性能持续上升，没有饱和迹象。而 ResNet 架构的 BC-Z 在数据量达到一定程度后性能趋于平缓。这完美验证了“苦涩的教训（The Bitter Lesson）”——**计算量和模型容量必须与数据量相匹配**。

### 4.3 异构数据融合（Heterogeneous Data）
这是 RT-1 最令人兴奋的发现之一。

*   **仿真数据（Sim-to-Real）**：
    *   如果我们在仿真器中生成大量数据（比如抓取一些现实中没有的物体），混合到真实数据中训练，会发生什么？
    *   结果：RT-1 在**未见物体**上的抓取成功率从 78% 提升到了 **87%**。这证明了仿真数据可以作为真实数据的有效补充，提升视觉泛化性。

*   **跨机器人形态（Cross-embodiment）**：
    *   Google 还有一种叫 Kuka 的工业机械臂，主要做分拣（Bin-picking）。Kuka 的外观、视角、动作空间与 EDR 移动机器人完全不同。
    *   实验：将 Kuka 的数据混入 RT-1 训练。
    *   结果：EDR 机器人在执行类似 Kuka 的分拣任务时，成功率翻倍（22% -> **39%**）。
    *   **结论**：RT-1 展示了惊人的数据吸收能力，它似乎能够忽略机器人形态的差异，提取出通用的“物体交互物理规律”。

### 4.4 长程任务：SayCan 集成
RT-1 并非只能做短时任务。将其作为底层执行器嵌入到 **SayCan** 框架中（上层由 LLM 进行任务规划），RT-1 能够连续执行多达 50 个步骤的长程任务。
*   在真实厨房 1 中，执行成功率达到 67%。
*   在完全未见过的厨房 2 中，由于 RT-1 强大的底层泛化能力，整个系统依然能保持运转。

---

# 第五部分：深度研讨与批判性思维 (Discussion & Critique)


### 5.1 RT-1 的局限性
1.  **数据依赖**：尽管 RT-1 泛化性好，但它依然依赖于极其昂贵的 130k 条专家演示数据。这对于普通实验室是不可复制的。
2.  **动作空间**：目前的离散化动作空间虽然有效，但在处理需要极高精度的精细操作（如穿针引线）时可能会受限。
3.  **开环 vs 闭环**：虽然是 3Hz 控制，但本质上它主要是基于视觉反馈的反应式策略，缺乏显式的记忆模块（虽然输入了历史图像，但 Transformer 的上下文窗口有限）。

### 5.2 启示与未来方向
*   **RT-2 及之后**：RT-1 之后出现了 RT-2（VLAD），直接将 VLM（视觉语言大模型）微调用于输出动作，进一步利用了互联网级别的语义知识。RT-1 是这个方向的基石。
*   **数据飞轮**：RT-1 证明了“数据规模 + Transformer”在机器人领域是行得通的。未来的重点将是如何更廉价地获取数据（如视频生成数据、Sim2Real、被动视频观察）。

---

# 总结 (Conclusion)

RT-1 是机器人学习领域的一个转折点。它不仅是一个模型，更是一个关于**Scaling Law（缩放定律）**在实体人工智能中是否适用的验证实验。

它告诉我们：
1.  **Transformer 可以实时控制机器人。**（通过 TokenLearner 等架构创新）
2.  **大规模真实世界数据是不可替代的。**
3.  **异构数据（仿真、不同机器人）是可以被统一模型吸收利用的。**

## 动作的预测

1.  **维度数量**：实际是 **11 个维度**，不是 10 个。
2.  **输出机制**：模型确实是**一次性并行输出**这 11 个维度的分布（即 Logits），形状确实是 `(11, 256)`。

我们来详细拆解一下 RT-1 的动作输出头（Action Head）。

### 1. 为什么是 11 个维度？

虽然我们常说 7 自由度机械臂 + 3 自由度底座 = 10 个维度，但 RT-1 增加了一个非常重要的“控制位”。具体的 11 个维度如下：

*   **机械臂（Arm）- 7 维**：
    *   末端执行器位置：$x, y, z$
    *   末端执行器姿态：$roll, pitch, yaw$
    *   夹爪状态：$gripper\_opening$
*   **移动底座（Base）- 3 维**：
    *   移动指令：$x, y, yaw$（底座的前进、横移和旋转）
*   **模式切换（Mode）- 1 维**：
    *   这是一个离散的指令，用于告诉机器人当前应该主要控制什么，或者是否结束任务。
    *   例如：`0`=控制手臂，`1`=控制底座，`2`=任务结束（Terminate Episode）。

所以，总共是 **7 + 3 + 1 = 11 个变量**。

### 2. 输出形状与并行生成（关键点！）

每个维度都被离散化成了 **256 个 Bin**（区间）。

在 Transformer 的最后一层输出之后，RT-1 并不是像 GPT 生成文本那样“一个字接一个字”地生成动作（即 $x \to y \to z \dots$），而是**并行生成**。

*   **输入给 Action Head 的**：是 Transformer 输出序列中的最后一个 Token 的嵌入向量（Embedding），假设维度是 $H$（Hidden Size）。
*   **Action Head 的结构**：通常是一个简单的全连接层（Linear Layer），将 $H$ 映射到 $11 \times 256$。
*   **输出张量形状**：
    $$ \text{Logits} \in \mathbb{R}^{11 \times 256} $$

**这意味着：**
模型在同一个时间步 $t$，**同时**预测手臂的 $x$ 坐标、底座的 $y$ 速度、夹爪的开合等等。它认为在当前图像和指令的条件下，这 11 个维度的动作是**条件独立**的。

### 3. 为什么不使用自回归（Auto-regressive）？

在早期的一些机器人模型（甚至 Gato 的某些设置）中，确实尝试过把动作展平，按顺序生成：
$$ P(a_x, a_y, \dots) = P(a_x) \cdot P(a_y | a_x) \cdot P(a_z | a_x, a_y) \dots $$

但在 RT-1 的论文中，作者做了一个非常重要的消融实验（Ablation Study）：
*   **实验**：对比“并行输出”与“自回归输出”。
*   **发现**：自回归输出虽然理论上能捕捉动作维度间的相关性（比如 $x$ 变了 $y$ 也要变），但在实际测试中，**并没有带来明显的性能提升，反而让推理速度慢了 2 倍以上**。
*   **决策**：为了保证 **3 Hz** 的实时控制频率，RT-1 最终选择了**非自回归（并行）**的方式。

### 总结

RT-1 的动作输出流程是这样的：

1.  **Transformer 输出**：拿到最后一个时刻的特征向量。(最后一个token的Embedding)
2.  **映射**：通过线性层得到形状为 `(11, 256)` 的 Logits。
3.  **采样/Argmax**：对这 11 个维度分别做 `argmax` 或者按概率采样，得到 11 个整数索引（范围 0-255）。
4.  **去离散化（De-tokenization）**：将这 11 个整数映射回连续的物理数值（比如 [-1.0, 1.0] 之间），发送给机器人控制器。

这个 `(11, 256)` 的张量就是 RT-1 在每一个时间步的最终产出。

## RT1训完后不具备VLM的能力


RT-1 是 VLA（Vision-Language-Action）模型的雏形，且它在训练完成后，确实完全不具备我们现在所说的通用 VLM（如 GPT-4V 或 Gemini）的图文对话或推理能力。

我们从架构设计的底层逻辑，将 RT-1 与后来的 RT-2（真正的 VLA）做一个本质上的区分。

### 1. 为什么说它是 VLA 的雏形？

**VLA 的定义**：接受视觉（Vision）和语言（Language）输入，输出动作（Action）。
从这个定义上看，RT-1 完美符合。它确实是迈向“通才机器人”的第一步，试图用一个统一的模型（Transformer）来解决多模态输入到控制信号的映射问题。

### 2. 为什么它不具备 VLM 能力？（核心原因）

RT-1 从架构设计之初，就没有被赋予生成语言的能力。 并不是“遗忘”了，而是“从未拥有”。

RT-1 的几个关键设计：

#### A. 语言编码器是“死的” (Frozen & Discriminative)
*   **RT-1 的做法**：它使用的是 **Universal Sentence Encoder (USE)**。
*   **本质**：USE 是一个**判别式**的嵌入模型（Embedding Model），它的作用是把一句话（比如“拿起苹果”）变成一个固定的向量。
*   **局限**：它只能“读懂”语义并将其压缩成向量，无法“生成”下一个词。它不是 GPT 那样的生成式模型（Generative Model）。

#### B. 输出头被“锁死”在动作空间 (Hard-coded Action Heads)
*   **RT-1 的做法**：RT-1 的输出层是专门设计的 **Action Head**，输出的是 `(11, 256)` 的动作分类分布。
*   **VLM 的做法**：真正的 VLM（如 RT-2 使用的 PaLI-X 或 PaLM-E），其输出层是 **Text Head**（词表，Vocabulary Size 通常是几万到几十万）。
*   **结果**：RT-1 的物理结构决定了它只能输出“底座向前 0.5m”，而绝对无法输出“这是一个苹果”这样的文本。它的输出空间里根本没有“单词”这个概念。

#### C. Transformer 主干的训练目标
*   **RT-1**：Transformer 的训练目标是最小化动作的分类误差（Behavior Cloning）。
*   **VLM**：训练目标是 Next Token Prediction（预测下一个文本 Token）。

### 3. RT-1 vs. RT-2：从“专用架构”到“通用大模型”的飞跃


| 特性 | RT-1 (本论文) | RT-2 (Vision-Language-Action-Model) |
| :--- | :--- | :--- |
| **基础架构** | **专用设计**：EfficientNet + TokenLearner + Transformer | **通用 VLM**：直接使用 PaLI-X 或 PaLM-E |
| **语言能力** | **仅理解**：通过 USE 嵌入向量 | **理解+生成**：具备完整的 LLM 能力 |
| **动作输出** | **专用 Head**：输出离散动作 Bin | **Token 化**：将动作映射为文本 Token（如把动作"128"视为文本"128"） |
| **训练后能力** | **仅控制**：只会做机器人动作，无法对话 | **多模态**：既能做动作，又能回答“图中有什么？” |
| **知识迁移** | **有限**：仅限于ImageNet的视觉特征 | **无限**：继承了互联网级别的语义知识和推理能力 |

### 4. 总结

*   **RT-1 是“专才”**：它借用了 Transformer 的架构优势（长序列建模、注意力机制）来处理机器人数据，但它本质上还是一个**模仿学习策略网络（Policy Network）**。它的“脑子”（Transformer）是专门为了输出动作而从头训练（或微调）的，不具备通用的语言生成逻辑。
*   **RT-2 才是真正的“通才” VLA**：RT-2 才是你想象中那种“既能写诗，又能通过推理决定去拿苹果”的模型。RT-2 证明了我们可以把“动作”伪装成“语言”，强行塞进一个已经训练好的 VLM 里，从而继承 VLM 的强大能力。

所以，RT-1 是 **Robotics Transformer**（用于机器人的 Transformer），而 RT-2 才是 **Vision-Language-Action Model**（具备视觉语言能力的动作模型）。RT-1 是从 0 到 1 的探索，证明了 Transformer 可以控制机器人；RT-2 则是从 1 到 100，证明了机器人模型可以继承互联网大模型的智慧。

