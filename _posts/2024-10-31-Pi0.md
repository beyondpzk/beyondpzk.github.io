---
layout: post
title: Pi0
date: 2024-10-31
categories: [VLA]
toc:
    sidebar: left
    max_level: 4
---

[TOC]

# $\pi_0$: A Vision-Language-Action Flow Model for General Robot Control


[paper link](https://arxiv.org/abs/2410.24164v1) 

Physical Intelligence 公司最近发表的一篇重要论文，题为 **“$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control”**（$\pi_0$：一种用于通用机器人控制的视觉-语言-动作流模型）。这项工作代表了所谓的 *通用机器人策略（generalist robot policies）* 演进中的关键一步——即系统 **从单一任务的专用化向广泛的多具身（multi-embodiment）能力** 转变。

---

### **范式转变——从专用化到通用化**

首先，让我们来梳理一下问题的背景。几十年来，机器人学家在构建专用系统方面表现出色。我们可以制造出以亚毫米级精度焊接汽车或跑酷的机器人。然而，这些系统非常脆弱；改变光照、将物体移动两英寸，或者引入一条新指令，它们就会失效。这篇论文的作者引用了罗伯特·海因莱因（Robert Heinlein）的一句话作为开篇：*“分工是昆虫的事（Specialization is for insects）。”* 这为整项工作奠定了哲学基调：人类智能的定义不在于某项任务的巅峰表现，而在于跨越多种任务的通用性。<alphaxiv-paper-citation title="Philosophy" page="2" first="Specialization is for" last="is for insects." />

$\pi_0$ 的核心论点是，要构建一个通用的机器人大脑，我们不能仅仅依赖机器人数据，因为这些数据稀缺且昂贵。相反，我们需要利用预训练的视觉-语言模型（VLMs）中蕴含的海量语义知识，这些模型是在互联网规模的数据上训练出来的。作者提出了一种 **“视觉-语言-动作”（Vision-Language-Action, VLA）** 模型。这不仅仅是一个输出文本计划的 VLM；它是一个经过“手术式”改造的 VLM，能够输出连续的、低层级的机器人动作。<alphaxiv-paper-citation title="VLA Concept" page="1" first="Our generalist robot" last="vision-language-action" />

这里的潜力是双重的。首先，通过使用 VLM 骨干网络，机器人继承了“常识”——在看到机械臂之前，它就已经知道什么是“海绵”，什么是“擦拭”。其次，通过在大量多样化的机器人数据（不同的手臂、不同的夹爪、不同的任务）上进行训练，模型学习到了物理交互的通用表示。这种方法解决了“数据稀缺挑战”，因为它允许模型将知识从丰富的来源（互联网文本/图像）迁移到数据匮乏的机器人领域。<alphaxiv-paper-citation title="Data Scarcity" page="2" first="This can resolve" last="scarcity challenge," />

我们在大语言模型（LLMs）的成功中看到了这种方法的镜像。正如 GPT-4 通过在多样化文本上进行预训练成为通用推理机一样，$\pi_0$ 通过在多样化机器人数据上进行预训练成为通用操作机。目标是建立一个基础模型，它可以通过提示（prompting）来执行从未见过的任务，或者在少量高质量数据上进行微调，以掌握诸如叠衣服等复杂技能。<alphaxiv-paper-citation title="Goal" page="1" first="can be prompted" last="laundry folding." />

---

### **架构——$\pi_0$ 的解剖学**

$\pi_0$ 的架构是一个混合系统。它不是从零开始的；它初始化自一个名为 **PaliGemma** 的预训练 VLM，这是一个拥有约 30 亿参数的开源模型。这个选择是经过深思熟虑的：30亿参数大到足以包含显著的语义知识，又小到在计算上对于机器人推理循环是可行的。<alphaxiv-paper-citation title="Base Model" page="5" first="we use PaliGemma" last="billion parameter VLM" />

该模型处理多模态观测 $o_t$。这包括：
1.  **图像：** 多个 RGB 摄像头信号（例如，手腕摄像头和第三人称视角）。
2.  **语言：** 描述任务的文本指令 $\ell_t$（例如，“叠衬衫”）。
3.  **本体感知（Proprioception）：** 机器人的关节状态 $q_t$。
所有这些输入都被编码到一个共享的嵌入空间中。图像和关节状态通过线性层投影，以匹配语言 token 的维度，从而允许 Transformer 骨干网络统一处理它们。<alphaxiv-paper-citation title="Observation Space" page="5" first="The observation consists" last="embedding space as" />

(Action Head) 这里的关键创新是添加了一个 **动作专家（Action Expert）**。标准的 VLM 设计用于输出离散的文本 token（下一个词预测）。然而，机器人生活在一个连续的世界里；它们需要连续的关节速度或位置。为了弥合这一差距，$\pi_0$ 增加了一组专门的权重——大约 3 亿个参数——称为“动作专家”。这个专家接收来自 VLM 的处理后特征，并将它们解码为一系列未来的动作。<alphaxiv-paper-citation title="Action Expert" page="5" first="We refer to" last="the action expert." />

至关重要的是，该模型不仅仅预测一个动作步。它预测一个 **动作块（Action Chunk）** $A_t = [a_t, a_{t+1}, \dots, a_{t+H-1}]$，其中 $H$ 是时间视界（在本论文中，$H=50$）。这种被称为 *动作分块（Action Chunking）* 的技术对于时间一致性至关重要。它可以防止在每一毫秒都重新规划的策略中常见的“抖动”。模型预测一个 50 步的轨迹，执行其中的一部分，然后重新规划。<alphaxiv-paper-citation title="Action Chunking" page="5" first="corresponds to an" last="action chunk of" />

---

### **数学引擎——流匹配**

如何生成这些高维的动作块？我们可以使用简单的均方误差（MSE）回归，但这假设了单峰分布（即只有一种拿杯子的有效方法）。实际上，机器人的行为是多峰的；你可以抓手柄，也可以抓杯沿。为了捕捉这一点，$\pi_0$ 使用了 **流匹配（Flow Matching）**，这是扩散模型（Diffusion Models）的一个强有力的替代方案。

形式上，我们要对条件分布 $p(A_t|o_t)$ 进行建模。作者采用了 *条件流匹配（Conditional Flow Matching）* 损失。核心思想是学习一个速度场 $v_\theta$，它在虚拟时间区间 $\tau \in [0, 1]$ 内将简单的噪声分布（高斯分布）推向复杂的有效机器人动作分布。<alphaxiv-paper-citation title="Loss Function" page="5" first="supervise these action" last="flow matching loss" />

损失函数定义为：
$$L_\tau(\theta) = \mathbb{E}_{p(A_t|o_t), q(A_t^\tau|A_t)} ||v_\theta(A_t^\tau, o_t) - u(A_t^\tau|A_t)||^2$$
这里，$u(A_t^\tau|A_t)$ 是目标向量场。作者使用了一种特定的公式，称为 **最优传输（Optimal Transport）** 或 **线性高斯（Linear-Gaussian）** 概率路径。该路径将中间噪声样本 $A_t^\tau$ 定义为干净数据 $A_t$ 和噪声 $\epsilon$ 之间的线性插值：
$$q(A_t^\tau|A_t) = \mathcal{N}(\tau A_t, (1-\tau)I)$$
这给出了样本构造：
$$A_t^\tau = \tau A_t + (1-\tau)\epsilon$$
神经网络试图预测的目标向量场 $u$，实际上指向从噪声到数据的方向：
$$u(A_t^\tau|A_t) = A_t - \epsilon$$
<alphaxiv-paper-citation title="Probability Path" page="5" first="given by q" last="linear-Gaussian (or" />

在训练过程中，我们采样一个时间 $\tau$，将噪声与真实动作块混合得到 $A_t^\tau$，并要求网络预测方向 $A_t - \epsilon$。这种方法计算效率高且稳定。

在 **推理（Inference）** 时，我们从纯噪声 $A_0^\tau \sim \mathcal{N}(0, I)$（其中 $\tau=0$）开始，并求解由我们学习到的向量场定义的常微分方程（ODE），以移动到 $\tau=1$。作者使用简单的 **前向欧拉积分（Forward Euler integration）**，步数为 10（$N=10$，步长 $\delta=0.1$）：
$$A_t^{\tau+\delta} = A_t^\tau + \delta v_\theta(A_t^\tau, o_t)$$
这个迭代过程逐渐将随机潜变量“去噪”，转化为机器人可以执行的连贯、平滑的 50 步动作轨迹。<alphaxiv-paper-citation title="Inference" page="5" first="generate actions by" last="integration step size." />

---

### **数据引擎——预训练与后训练**

模型的好坏取决于其数据。$\pi_0$ 以其严格的“预训练后微调（Pre-training then Post-training）”配方脱颖而出，这反映了 LLM 的演变过程（预训练 $\to$ SFT $\to$ RLHF）。

**第一阶段：预训练（Pre-training）。**
这里的目标是 **鲁棒性和多样性**。模型在海量的混合数据集上进行训练。这包括 Open X-Embodiment (OXE) 数据集——具体是一个经过过滤的子集，称为 "OXE Magic Soup"——以及 Physical Intelligence 收集的专有数据集。这些数据涵盖了 7 种不同的机器人配置（单臂、双臂、移动底座）和超过 68 项任务。这些数据在某种意义上是“低质量”的，因为它可能包含失败、次优路径或有噪声的遥操作数据。然而，它涵盖了极其广泛的物理现象、物体和场景。这一阶段教会机器人“世界是如何运作的”以及如何从错误中恢复。<alphaxiv-paper-citation title="Pre-training" page="2" first="The goal of" last="pre-training phase" />

**第二阶段：后训练（Post-training）。**
这里的目标是 **流畅性和精确性**。在预训练之后，模型针对特定的下游任务在高质量、精心策划的数据上进行微调。作者指出，虽然预训练提供了处理扰动的能力，但后训练才是使机器人变得“熟练（skillful）”的关键。例如，在叠衣服任务中，预训练帮助机器人识别衬衫，但后训练确保折叠整齐且动作流畅。这种关注点的分离——从多样化数据中学习 *做什么*，从高质量数据中学习 *如何做好*——是一个关键的结论。<alphaxiv-paper-citation title="Post-training" page="2" first="post-training dataset should" last="effective task execution," />

值得注意的是 **跨具身（Cross-Embodiment）** 方面。该模型控制具有不同运动学的不同机器人。这是通过共享的动作空间公式和 VLM 关注不同本体感知输入的能力来处理的。网络学会了将“拿起杯子”的视觉理解映射到 Franka Emika panda 机械臂与 UFactory xArm 所需的特定关节速度上。<alphaxiv-paper-citation title="Cross-Embodiment" page="1" first="diverse cross-embodiment" last="manipulation tasks." />

---

### **结果、比较与未来方向**

论文在极具挑战性的任务上评估了 $\pi_0$，如叠衣服、清理餐桌（bussing tables）和组装盒子。最引人注目的演示之一是叠衣服任务，移动操作机器人从烘干机中取出衣服，放入篮子，移动到桌子旁，然后折叠它们。这是一个多阶段、长视界的任务，既需要移动性也需要灵巧性。<alphaxiv-paper-citation title="Laundry Task" page="2" first="controls a mobile" last="article of clothing." />

为了证明 VLM 骨干网络确实是必要的，作者将 $\pi_0$（33亿参数）与基线模型 **$\pi_0$-small**（4.7亿参数）进行了比较。$\pi_0$-small 使用相同的流匹配头和数据，但在初始化时 *没有* 使用 VLM 权重。结果表明，对于复杂的语义任务，VLM 初始化提供了显著的性能提升。“小”模型难以泛化到新指令或未见过的物体，这凸显了来自 VLM 的语义“世界知识”确实迁移到了控制中。<alphaxiv-paper-citation title="Baseline Comparison" page="5" first="Non-VLM baseline model." last="incorporating VLM pertaining." />

**思考：**
1.  **延迟（Latency）：** 每个控制周期运行一个 30 亿参数的模型是昂贵的。50 步的动作分块如何缓解这个问题？（它将有效推理频率从例如 50Hz 降低到了 1Hz）。
2.  **安全性（Safety）：** 流匹配是概率性的。在强大的机器人硬件上运行随机策略是否安全？
3.  **缩放定律（Scaling Laws）：** 如果我们从 PaliGemma (3B) 转移到 700 亿参数的模型，机器人性能会线性扩展吗？还是说物理灵巧性受限于语义理解之外的其他因素？

这篇论文表明，机器人的未来在于“下载”LLM/VLM 的大脑，并将其连接到一个在物理数据上训练的“脊髓”（动作专家）上。对于 NLP、计算机视觉和机器人技术融合为一个单一、统一的研究领域来说，这是一个令人信服的论据。


## 两阶段的侧重点


**Loss 函数在两个阶段是完全一样的，但数据集的构成和侧重点完全不同。**

### 1. Loss 函数：完全一致

无论是在 **预训练 (Pre-training)** 阶段还是 **后训练 (Post-training)** 阶段，$\pi_0$ 使用的都是同一个 **条件流匹配 (Conditional Flow Matching)** 损失函数：

$$L_\tau(\theta) = \mathbb{E}_{p(A_t|o_t),q(A_t^\tau|A_t)} ||v_\theta(A_t^\tau, o_t) - u(A_t^\tau|A_t)||^2$$


*   **统一的优化目标**：这一点非常重要。这意味着模型在两个阶段都在学习同一件事——预测动作分布的“速度场”。不存在从“对比学习”切换到“回归”这种目标的跳跃。
*   **微调机制**：后训练本质上就是在一个更小、更专用的数据分布上继续优化这个 Loss。这类似于 LLM 的 SFT（Supervised Fine-Tuning），目标函数不变，只是数据分布变了。

---

### 2. 数据集：天壤之别

虽然 Loss 一样，但“喂”给模型的数据在性质、规模和目的上有巨大的差异。

#### **A. 预训练数据 (Pre-training Mixture)**
*   **规模与构成**：这部分数据非常庞大，是“大杂烩”。它包含了：
    *   **开源数据 (Open X-Embodiment, OXE)**：这部分约占 9.1%，涵盖了 22 种不同的机器人。
    *   **自有数据 ($\pi$ dataset)**：约 9 亿个时间步 (timesteps)，涵盖 7 种不同的机器人配置（单臂、双臂、移动底座等）和 68 种任务。
*   **数据质量**：这部分数据的质量参差不齐。它可能包含次优的轨迹、失败的尝试、带有噪声的遥操作数据。
*   **目的**：目的是让模型见多识广。通过看各种各样的机器人做各种各样的事，模型学会了“通用物理常识”（比如物体怎么抓，关节怎么动）和“鲁棒性”（如何从奇怪的状态中恢复）。
*   **权重策略**：为了防止某些任务主导训练，他们对数据进行了加权处理（$n^{0.43}$），稍微平衡了不同任务的比例。 <alphaxiv-paper-citation title="Pre-training Mixture" page="5" first="consists of a" last="diverse language labels" />

#### **B. 后训练数据 (Post-training Data)**
*   **规模**：相对非常小。对于简单的任务可能只需要 5 小时的数据，对于像叠衣服这样复杂的任务可能需要 100 小时左右的数据。
*   **数据质量**：**极高 (High-Quality)**。这是经过精心策划和筛选的。这些数据展示了任务执行的“黄金标准”——动作流畅、策略一致、没有多余的动作。
*   **目的**：目的是“专业化”和“对齐”。预训练模型可能知道怎么抓衣服，但可能抓得歪歪扭扭。后训练数据教模型：“在这个特定任务下，不仅要抓到，还要抓得优雅、高效。”
*   **不同任务不同数据**：每个下游应用（比如叠衣服、收拾桌子）都有自己专属的后训练数据集。模型是在特定的数据集上微调，从而变成该任务的专家。 <alphaxiv-paper-citation title="Post-training Data" page="5" first="high-quality curated data" last="specific downstream tasks." />

### 训练数据总结
| 特性 | 预训练 (Pre-training) | 后训练 (Post-training) |
| :--- | :--- | :--- |
| **Loss 函数** | **流匹配 Loss (Flow Matching)** | **流匹配 Loss (Flow Matching)** |
| **数据来源** | OXE (开源) + 内部大规模混合数据 | 针对特定任务的高质量专有数据 |
| **数据规模** | 巨大 (~9亿步) | 较小 (5~100小时) |
| **数据多样性** | 极高 (多机器人、多场景) | 低 (专注于单一任务) |
| **数据质量** | 混合 (含次优/噪声) | 极高 (精心筛选的演示) |
| **学习目标** | 通用能力、鲁棒性、物理常识 | 任务熟练度、动作流畅性 |

这种策略的核心逻辑是：**用海量数据换取泛化能力，用少量精品数据换取专业技能。** Loss 函数保持一致保证了知识的平滑迁移。


** 另外两个阶段的参数更新策略是不一样的**。

### 1. 预训练阶段 (Pre-training)

**策略：几乎所有参数都参与训练 (Full Fine-tuning / Joint Training)**

在预训练阶段，目标是让模型学会“通用的物理常识”。因此，不仅要训练新加的 **Action Expert (动作专家)**，还要让原本的 **VLM Backbone (视觉-语言骨干)** 适应机器人领域的图像和文本分布。

*   **Action Expert (300M)**: 从随机初始化开始，**全程参与训练**。这是必须的，因为它要学会把语义特征翻译成动作。
*   **VLM Backbone (3B)**: 虽然加载了预训练权重，但在这一阶段也是 **解冻 (Unfrozen)** 的。
    *   **为什么？** 因为原本的 VLM (PaliGemma) 是在互联网图片上训练的，它没见过这么多充满杂乱电线、机械臂特写、低分辨率工业相机的图片。如果锁死 VLM 参数，视觉特征提取能力会受限。
    *   **LoRA (Low-Rank Adaptation)**: 为了节省显存和防止灾难性遗忘，作者实际上在这个阶段使用了 **LoRA** 技术微调 VLM 部分的权重，而不是全参数微调。这是一种折中方案。

### 2. 后训练阶段 (Post-training)

**策略：针对特定任务的微调 (Task-Specific Fine-tuning)**

在后训练阶段，我们希望模型专注于特定任务（例如叠衣服），但又不想让它忘记通用的物理知识。

*   **通常做法**：在这个阶段，往往会 **解冻所有参数** 进行微调，但使用 **非常小的学习率**。
    *   因为数据集很小（几小时），如果只训练 Action Expert，模型可能学不到任务特有的视觉特征（比如识别某种特定的衣物折痕）。
    *   如果只训练 VLM，动作输出又跟不上。
    *   所以，**全参数微调 (Full Fine-tuning)** 或 **LoRA微调** 是首选。
*   **特殊情况**：有些论文会选择冻结 VLM，只微调 Action Head，但在 $\pi_0$ 这种追求极致性能的模型中，为了适应任务特有的视觉分布（例如叠衣服时特殊的布料纹理），通常会让 VLM 也跟着微调。

**关键区别总结：**

| 特性 | 预训练 (Pre-training) | 后训练 (Post-training) |
| :--- | :--- | :--- |
| **Action Expert** | **全参数训练 (从零开始)** | **全参数微调 (基于预训练)** |
| **VLM Backbone** | **LoRA 微调 (适应机器人域)** | **全参数/LoRA 微调 (适应特定任务)** |
| **Image Encoders** | **解冻 (Unfrozen)** | **解冻 (Unfrozen)** |
| **学习率** | 较大 (学习通用特征) | 较小 (防止过拟合/遗忘) |

**特别注意：**
虽然论文没有显式地大篇幅讨论“冻结”与否，但在附录 B.2 中提到使用了 LoRA。这暗示了为了保持 VLM 的通用语义能力，他们并没有直接破坏 VLM 的权重，而是通过外挂参数（LoRA）来适配机器人任务。这种做法在多模态大模型微调中非常标准。


## 详细的网络结构

### **1. 输入层 (Input Processing)**

模型的输入是一个元组 $(o_t, A_t^\tau)$，其中 $o_t$ 是观测，$A_t^\tau$ 是带噪声的动作块（仅在训练时输入，推理时是纯噪声）。

**A. 图像 (Images) $I_t^1, \dots, I_t^n$**
*   **来源**：这就好比人的眼睛。$\pi_0$ 接收 $n$ 个摄像头的 RGB 图像。
    *   例如：1个手腕相机 + 2个外部相机 = 3张图。
*   **处理**：这些图像通过 **SigLIP** (类似于 CLIP 的视觉编码器) 进行编码。
*   **Token化**：每张图像被切分成多个 patches，拉平成一串 tokens。
*   **Shape**：假设每张图变成 256 个 tokens，3张图就是 $3 \times 256 = 768$ 个 tokens。每个 token 的维度是 VLM 的隐藏层维度 $w$ (例如 2048)。
    *   $\text{Image Tokens}: [n \times 256, w]$

**B. 语言指令 (Language) $\ell_t$**
*   **来源**：这就好比人的耳朵。比如用户输入的文本 "Fold the towel"。
*   **处理**：通过标准的 Tokenizer 处理。
*   **Shape**：变成一串文本 tokens。
    *   $\text{Text Tokens}: [L_{text}, w]$，其中 $L_{text}$ 是指令长度。

**C. 本体感知 (Proprioception) $q_t$**
*   **来源**：这就好比人的肌肉感觉。机器人当前的关节角度、夹爪状态等。
*   **处理**：这是一个低维向量 (例如 7个关节 + 1个夹爪 = 8维)。它通过一个 **线性投影层 (Linear Projection)** 映射到 VLM 的维度 $w$。
*   **Shape**：
    *   原始：$[1, d_q]$ (例如 $[1, 8]$)
    *   投影后：$[1, w]$ (作为一个单独的 token 混入序列) <alphaxiv-paper-citation title="Proprioception Projection" page="15" first="We add an input" last="a linear projection." />

**D. 带噪声的动作块 (Noisy Action Chunk) $A_t^\tau$**
*   **来源**：这是流匹配的核心。在训练时，它是真实动作 $A_t$ 加了噪声；在推理时，它是从高斯分布采样的纯噪声。
*   **结构**：这是一个包含未来 $H$ 步动作的序列。
    *   $H = 50$ (预测未来50步)
    *   $d$ = 动作维度 (例如 14维：7关节+1夹爪 $\times$ 2个手臂)
*   **时间嵌入 (Time Embedding)**：这是关键细节！模型必须知道当前的去噪进度 $\tau \in [0, 1]$。
    *   $\tau$ 通过正弦位置编码 (Sinusoidal Positional Encoding) 变成向量。
    *   每个动作 token $a_t^\tau$ 都与时间嵌入 $\tau$ 拼接/相加。
*   **处理**：通过 **MLP** 投影到维度 $w$。
*   **Shape**：
    *   原始：$[H, d]$ (例如 $[50, 14]$)
    *   投影后：$[H, w]$ (50个 tokens) <alphaxiv-paper-citation title="Action Tokens" page="15" first="noisy action chunk" last="embedding dimension using" />

---

### **2. 骨干网络 (Backbone Processing)**

现在我们把所有这些 tokens 拼接到一起，形成一个巨大的序列：
$$\text{Token Sequence} = [\underbrace{\text{Image Tokens}}_{\text{VLM}}, \underbrace{\text{Text Tokens}}_{\text{VLM}}, \underbrace{\text{Proprioception Token}}_{\text{Expert}}, \underbrace{\text{Action Tokens}}_{\text{Expert}}]$$

这个序列被送入 **Transformer**。但这里有个特殊的 **混合专家 (MoE) 结构**：

*   **VLM Backbone (PaliGemma)**：处理图像和文本 tokens。参数量 ~3B。
*   **Action Expert**：处理本体感知和动作 tokens。参数量 ~300M。
*   **交互**：虽然有两套权重，但它们在 Transformer 的 **Self-Attention** 层是互通的！
    *   动作 tokens 可以 attend 到 图像 tokens (看图做动作)。
    *   动作 tokens 可以 attend 到 文本 tokens (听指令做动作)。
    *   **但注意**：为了防止破坏预训练 VLM 的知识，VLM tokens (图像/文本) **看不到** 动作 tokens (因果掩码)。

**Attention Mask 细节**：
1.  **[图像, 文本]** $\to$ 可以互看 (双向)，但看不到后面。
2.  **[本体感知]** $\to$ 可以看 [图像, 文本] 和自己。
3.  **[动作块]** $\to$ 可以看 **所有人** (图像, 文本, 本体, 动作)。这是一个 **全双向 Attention** (Full Bidirectional Attention) 在动作块内部，意味着第 1 步动作可以看到第 50 步动作的信息，保证轨迹平滑。 <alphaxiv-paper-citation title="Attention Mask" page="15" first="uses a blockwise" last="attend to each other." />

---

### **3. 输出层 (Output Processing)**

Transformer 吐出处理后的特征序列。我们只关心 **动作 Tokens**对应的输出。

*   **提取**：取出最后 $H$ 个 tokens 的输出特征。
    *   Shape: $[H, w]$
*   **解码**：通过一个 **线性头 (Linear Head)** 将特征映射回动作维度 $d$。
*   **最终输出**：$v_\theta(A_t^\tau, o_t)$，即预测的速度场。
    *   Shape: $[H, d]$ (例如 $[50, 14]$)

这个输出就是告诉我们在当前噪声水平下，应该往哪个方向调整动作，才能让它更像真实的合理动作。

---

### **总结：数据流向图**

1.  **Input**:
    *   Images ($3 \times 256 \times w$)
    *   Text ($L \times w$)
    *   Joints ($1 \times w$)
    *   Noisy Actions ($50 \times w$) + Time $\tau$
2.  **Concat**: 形成长序列 $[N_{total}, w]$
3.  **Transformer**:
    *   VLM layers 处理前两部分
    *   Action Expert layers 处理后两部分
    *   Cross-Attention 让动作部分“看见”视觉和语言
4.  **Output Head**:
    *   只取最后 50 个 tokens
    *   Linear Projection $\to [50, 14]$
5.  **Result**: 预测的速度向量 $v$，用于 ODE 积分器推下一步。

## Some comments

我理解 Action Expert 就是就相当于是 从action 空间到 LLM空间的一个projector, Action Expert 在训练的时候,可以输入的是 learnable tokens 也可以是加noisy 后的 action; 经过LLM输出后的token, 经过head, 直接回归action的"velocity", 这就相当于是把LLM当成了 flow matching中的DiT了. 也可以把 action head 当成 "Dit", 把LLM的输出action tokens当成条件.


第一种理解是对的——**$\pi_0$ 把整个 LLM（或者更准确地说是经过魔改的 VLM Backbone + Action Expert）直接当成了 Flow Matching 中的 DiT (Diffusion Transformer)。**

这并不是一个“把 LLM 当特征提取器，后面接个 DiT Head”的结构（那是 Octo 或者 OpenVLA 的做法），而是一个 **端到端、Token 级别混合的 DiT**。

### 1. Action Expert 不是简单的 Projector
虽然 Action Expert 确实包含了一个 Linear Projection 层（把动作维度 $d$ 映射到 LLM 维度 $w$），但它**不仅仅是**一个 Projector。
在 $\pi_0$ 中，Action Expert 是一组 **Transformer Layers（权重）**，它们与 VLM 的 Transformer Layers 是并行的或者交织的（但在 $\pi_0$ 的实现中，更像是一个 Mixture-of-Experts，即某些 Token 走 VLM 权重，某些 Token 走 Action Expert 权重）。
*   **输入给 Action Expert 的是：** **加噪后的 Action Tokens ($A_t^\tau$)**。
*   **位置编码：** 这些 Tokens 加上了时间步 $\tau$ 的 Embedding。

### 2. LLM 本身就是 DiT
在标准的 Diffusion/Flow Matching 中，我们需要一个网络 $\epsilon_\theta(x_t, t, c)$ 来预测噪声/速度。
在 $\pi_0$ 中，这个网络 $\epsilon_\theta$ **就是整个 VLA 模型**。

*   **Condition ($c$)**: 图像 Tokens + 文本 Tokens + 本体感知 Token。
*   **Noisy Input ($x_t$)**: 加噪动作 Tokens $A_t^\tau$。
*   **Time ($t$)**: $\tau$ Embedding 加在动作 Tokens 上。

这些 Tokens 全部 **拼接 (Concat)** 在一起，送入同一个 Transformer 序列中。
*   图像/文本 Tokens 走 VLM 的权重（Self-Attention + MLP）。
*   动作 Tokens 走 Action Expert 的权重（Self-Attention + MLP）。
*   **关键点**：它们在 **Self-Attention 层是互通的**！
    *   动作 Token $a_i$ 会去 Attend 到 图像 Token $I_j$。
    *   这就是“条件生成”发生的时刻——噪声动作通过 Attention 机制“看”到了图像信息，从而知道该往哪里去噪。

### 3. 输出直接回归 Velocity
经过 $L$ 层 Transformer 处理后，对应 **动作 Tokens 位置的输出** 被拿出来。
*   这些输出特征经过一个简单的 Linear Head（Projector 的逆变换）。
*   直接回归出 **目标速度 $v$** (即 $A_t - \epsilon$)。
*   **没有额外的 Head 网络**：不需要再接一个 MLP 或 ResNet 来做去噪，Transformer 的输出就是去噪后的结果。

### 总结：它是哪种方式？

| 方式 | 描述 | $\pi_0$ 是吗？ |
| :--- | :--- | :--- |
| **方式 A (Head-only)** | LLM 输出 Features $\to$ 小型 DiT Head (Conditioned on Features) | ❌ (这是 Octo/OpenVLA) |
| **方式 B (End-to-End)** | **Noisy Actions $\to$ LLM Input Tokens $\to$ LLM Output Tokens $\to$ Velocity** | **✅ (这是 $\pi_0$)** |

**为什么选方式 B？**
*   **深度融合**：动作生成过程可以利用 LLM 深层的语义理解，而不是只利用最后一层的特征。
*   **参数共享**：虽然分了 Expert，但在 Attention 机制上是共享计算资源的，动作 Tokens 可以直接查询视觉 Tokens 的 Key/Value。
*   **扩展性**：这种结构符合 Scaling Law，模型越大，生成动作的能力理论上越强。

所以，$\pi_0$ 本质上是一个 **多模态的 DiT**，其中一部分模态（图像/文本）是固定的 Condition，另一部分模态（动作）是需要去噪的生成目标。
