---
layout: post
title: JEPA_WM
date: 2025-12-30
categories: []
tags: []
---

[TOC]

# JEPA_WM 

[paper链接](https://arxiv.org/pdf/2512.24497)


# 机器人学习与世界模型进阶专题
**核心论文：** Terver et al., *What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?* (arXiv:2512.24497v2)

---

**目标：** 我们将超越世界模型（World Models）理论层面的“是什么”，深入探讨工程层面的“怎么做”。我们将对应用于机器人规划的**联合嵌入预测架构（JEPA）**进行拆解。与那些提出单一新颖架构的论文不同，这项工作进行了一项极其严谨的**消融实验（Ablation Study）**，旨在找出构建此类模型的最佳“配方”。

**核心学习成果：**
1. 理解用于物理规划的 JEPA-WM 范式。
2. 分析关键设计选择：编码器选择、上下文长度和本体感觉（Proprioception）。
3. 评估潜空间中的规划算法（CEM 对比 梯度下降）。
4. 理解模型扩展性（Scaling）在“仿真到现实（Sim-to-Real）”鸿沟中的差异。

---

## **第一部分：范式转变——从像素到潜空间规划**

### **1.1 背景：为什么要用世界模型？**
在强化学习（RL）中，我们经常受困于样本效率问题。无模型（Model-Free）RL 需要数百万次交互。基于模型的 RL（MBRL）试图通过学习环境动力学来解决这个问题。

然而，传统的 MBRL 在高维视觉空间（像素级）中往往表现不佳，因为预测每一个像素既昂贵又容易受到“噪声”干扰（例如，预测墙壁的确切纹理，而不是门的位置）。

本文关注一种解决方案：**在学习到的表征空间中进行规划**，具体使用的是 **JEPA-WMs**。这类方法的核心在于抽象掉无关的细节，从而产生更高效的规划。 <alphaxiv-paper-citation title="Introduction" page="1" first="Planning is commonly" last="efficient planning." />

### **1.2 架构：JEPA-WM**
让我们将系统形式化。我们预测的不是 $s_{t+1}$（像素），而是 $z_{t+1}$（潜变量）。

**核心组件：**
1.  **编码器（$E_{\phi, \theta}$）：** 将观测值（$o_t$）映射为潜状态（$z_t$）。
2.  **预测器（$P_\theta$）：** 动力学模型。它接收当前状态和动作，预测*下一个*潜状态。
3.  **规划器（Planner）：** 这不是神经网络，而是一种算法（如 MPC），利用预测器来寻找最优动作序列。

论文清晰地定义了这个框架：(编码器,预测器)就是我们所说的**世界模型**。 <alphaxiv-paper-citation title="Definitions" page="2" first="The encoder/predictor pair" last="world model." />

### **1.3 训练循环（Teacher-Forcing）**
如果在不使用重建损失（像素误差）的情况下训练？我们使用联合嵌入（Joint-Embedding）方法。
*   **输入：** 过去的观测和动作的上下文。
*   **目标：** *实际*未来状态的嵌入（由目标编码器计算得出）。
*   **损失：** 潜空间中的距离（L2 或 L1）。

训练过程涉及一个预测器，它接收过去观测和动作的上下文，并在时间步上并行预测下一个状态嵌入。 <alphaxiv-paper-citation title="Training" page="2" first="which is fed" last="state embedding." />

---

## **第二部分：成功的“要素”——系统特征分析**


### **2.1 眼睛：编码器的选择（DINO VS 其他）**
如果你想让机器人抓起一个杯子，它需要理解背后墙壁的纹理吗？不需要。它需要的是物体恒常性和分割能力。

*发现：* 论文将 DINOv2（自监督 ViT）与 V-JEPA 编码器进行了比较。
*结果：* **DINO 获胜。** 为什么？DINO 拥有更优越的细粒度物体分割能力。这对于精确的定位任务至关重要。 <alphaxiv-paper-citation title="Encoder Analysis" page="9" first="DINO has better" last="segmentation capabilities," />

### **2.2 身体：本体感觉（Proprioception）**
*讨论：* 我们应该仅仅依赖视觉（像素），还是让机器人知道它的关节角度？
*   **观察：** 在许多“纯像素”论文中，为了使模型显得“通用”，往往忽略了本体感觉。
*   **论文结果：** 结合本体感觉训练的模型表现始终更好。没有它，机械臂往往会在目标周围震荡，因为仅凭视觉缺乏精细运动停止所需的精度。 <alphaxiv-paper-citation title="Proprioception" page="8" first="models trained with" last="consistently better" />

### **2.3 大脑：预测器上下文与架构**
模型需要多少历史信息？
*   $W=1$（1帧）：模型无法推断速度。
*   $W=2$（2帧）：模型可以推断速度（$p_t - p_{t-1}$）。
*   $W=3$（3帧）：模型可以推断加速度。

*   **关键发现：** $W=1$ 和 $W=2$ 之间存在巨大的性能差距。速度信息是必不可少的。然而，过长的上下文（如 $W=7$）在仿真中反而会降低性能（过拟合/噪声）。有趣的是，真实世界数据（DROID）受益于稍长的上下文（$W=5$），这可能是由于真实物理动力学的复杂性。 <alphaxiv-paper-citation title="Context Length" page="8" first="gap between models" last="infer velocity." />

*   **条件化（Conditioning）：** 我们如何将动作 $a_t$ 输入到 Transformer 预测器中？
    *   *拼接（Concatenation）？* 简单。
    *   *AdaLN（自适应层归一化）？* 复杂但精细。
    *   *结果：* **带 RoPE 的 AdaLN** 平均表现最强，因为它将动作信息注入到*每一层*，防止了信号消失。 <alphaxiv-paper-citation title="Architecture" page="9" first="AdaLN with RoPE" last="average performance," />

### **2.4 训练目标：多步展开（Multistep Rollout）**
如果我们只训练 $t \to t+1$，当我们规划 $t+10$ 时，模型可能会发生漂移。
*   **技术：** 在训练期间增加未来多步的损失项。
*   **结果：** 2步展开（2-step rollout）的损失是最佳的。超过这个步数（例如6步）会降低仿真中的性能，使模型对即时预测任务的专业性下降。 <alphaxiv-paper-citation title="Rollout Loss" page="8" first="performance increases when" last="rollout loss models," />

---

## **第三部分：规划算法与真实世界迁移**

### **3.1 优化问题**
(之前ATOM的算法是不是就有用了?)

一旦我们有了训练好的世界模型，我们需要解以下方程：
$$a_{t:t+H}^* = \arg\min_{a} \sum_{k=t}^{t+H} \text{Cost}(\hat{z}_k, z_{goal})$$

论文比较了三大类规划器：
1.  **CEM（交叉熵方法）：** 基于采样的。采样高斯动作，挑选最好的，重新拟合高斯分布。
2.  **GD（梯度下降）：** 通过学习到的模型反向传播误差，直接更新动作。
3.  **Nevergrad (NG)：** 一个无梯度优化库。

### **3.2 为什么梯度下降（GD）会失败？**
理论直觉表明 GD 应该是最好的，因为我们有一个可微的世界模型。
*   **现实检验：** GD 在导航任务（迷宫/墙壁）上表现糟糕。
*   **原因：** 潜空间中的成本曲面是非凸的，且充满局部极小值（例如，卡在墙边）。GD 无法“跳过”高成本的障碍。
*   **赢家：** CEM（L2 距离）仍然是稳健的冠军。它的探索能力更强。 <alphaxiv-paper-citation title="Planning Optimizers" page="8" first="CEM L2" last="outperforms L1 cost." />

### **3.3 扩展定律的差异（仿真 vs 现实）**
这是现代 AI 的重要一课。
*   **仿真（Metaworld）：** 增加模型大小（ViT-S 到 ViT-L）**没有**帮助。物理很简单；小模型就已经让任务饱和了。
*   **真实世界（DROID）：** 增加模型大小**确实**有帮助。真实世界的图像和动力学包含“偶然不确定性（aleatoric uncertainty）”和复杂性，需要更大的容量。
*   **结论：** 不要为了简单的基准测试浪费算力去扩展模型。要为了现实世界而扩展。 <alphaxiv-paper-citation title="Scaling" page="9" first="simulated environments saturate" last="lower capacities." />

---

## **第四部分：综合与“黄金配方”**

### **4.1 提议的最佳方案**
基于研究，作者提出了一种特定的配置，击败了基线模型（DINO-WM, V-JEPA-2-AC）。

**配方：**
*   **编码器：** DINOv2（若追求照片级真实感可用 v3）。
*   **预测器：** 带 AdaLN 条件化的 ViT。
*   **训练：** 启用本体感觉 + 2步展开损失（2-step rollout loss）。
*   **规划：** 使用 L2 距离的 CEM。

**性能：**
这种特定组合显著优于先前的 SOTA。例如，在“Reach（到达）”任务中，他们实现了高得多的成功率。 <alphaxiv-paper-citation title="Results" page="10" first="outperform DINO-WM and" last="most environments." />

### **4.2 讨论**
1.  **“奖励”难题：** 本文使用目标图像（$z_g$）。如果任务没有目标的具体照片（例如，“尽可能快地跑”），如何调整这个 JEPA-WM？
2.  **潜空间漂移：** 即使有2步展开训练，模型在长时域（50+步）上也可能漂移。为什么 MPC（模型预测控制）能缓解这个问题？（提示：每一步都重新规划）。
3.  **语言的角色：** 论文提到了 VLA（视觉-语言-动作）模型。我们如何将 JEPA 预测器的条件改为文本指令而不是目标图像？

---

这是为您准备的课堂讨论问题参考答案。作为教授，我不仅提供了标准答案，还加入了一些基于论文原理的延伸思考，以便您引导学生深入讨论。

---

### **问题 1：“奖励”难题**
**问题：** 本文使用目标图像（$z_g$）作为导航终点，计算 $Cost = ||\hat{z}_t - z_g||$。如果任务没有目标的具体照片（例如“尽可能快地跑”或“保持直立”），你将如何调整这个 JEPA-WM？

**参考答案：**
我们需要将“目标距离”替换为一个**学习到的奖励函数（Learned Reward Function）**。
1.  **方法：** 我们可以在 JEPA 的预测器或编码器之上训练一个轻量级的多层感知机（MLP），记为 $R(z_t)$。
2.  **训练：** 使用带标注的数据集（或者通过人工反馈 RLHF）来训练这个 MLP，使其输入一个潜状态 $z_t$，输出一个标量值（Reward）。例如，如果任务是奔跑，输入当前状态的 $z$，输出当前的速度估算值。
3.  **规划：** 在规划阶段（CEM），我们不再最小化与目标图像的距离，而是最大化预测轨迹的累积奖励：$\max \sum R(\hat{z}_t)$。
4.  **延伸思考：** 这实际上让 JEPA-WM 从“目标条件化规划”转向了更通用的“基于模型的强化学习（MBRL）”。虽然这增加了训练奖励模型的开销，但大大扩展了模型的适用范围。

---

### **问题 2：潜空间漂移与 MPC**
**问题：** 即使有2步展开训练（2-step rollout），模型在长时域（如预测50步以上）上也必然会产生累积误差（漂移）。为什么 **MPC（模型预测控制）** 机制能缓解这个问题？

**参考答案：**
关键在于 MPC 的 **“闭环反馈”**机制，它并没有完全信任长期的预测。
1.  **执行逻辑：** 虽然我们在大脑中规划了未来 H 步（例如50步），但 MPC **只执行第一个动作** $a_t$。
2.  **重置误差：** 执行完 $a_t$ 后，机器人会通过传感器看到**真实的**新状态 $o_{t+1}$。此时，我们将编码器重新应用于真实的 $o_{t+1}$ 得到真实的 $z_{t+1}$。
3.  **重新规划：** 下一轮规划从真实的 $z_{t+1}$ 开始，而不是从模型预测的（可能带有误差的）$\hat{z}_{t+1}$ 开始。
4.  **结论：** MPC 每一步都用真实的观测值“校准”了当前位置。这就像使用 GPS 导航：虽然它规划了全程路线，但如果你偏离了路线，它会根据你当前的**真实位置**重新计算，而不是假设你还在原来的路线上盲目指挥。

---

### **问题 3：语言的角色**
**问题：** 论文提到了 VLA（视觉-语言-动作）模型。我们如何将 JEPA 预测器的条件改为文本指令（如“拿起蓝色杯子”）而不是目标图像？

**参考答案：**
这涉及到**多模态对齐（Multimodal Alignment）**或**条件注入（Condition Injection）**。主要有两种改法：

**方案 A：潜在空间对齐（CLIP 风格）**
*   **原理：** 使用像 CLIP 这样预训练好的模型，它能将图像和文本映射到同一个共享空间。
*   **操作：**
    1.  将指令“拿起蓝色杯子”通过文本编码器编码为向量 $e_{text}$。
    2.  我们训练 JEPA 的视觉编码器，使其输出的 $z$ 与 CLIP 的空间对齐。
    3.  规划目标变为最小化当前状态与文本嵌入的距离：$Cost = ||\hat{z}_t - e_{text}||$。
*   **优点：** 不需要修改预测器架构。

**方案 B：预测器条件化（Cross-Attention）**
*   **原理：** 将文本指令作为一种“上下文”输入给预测器，告诉预测器“在这种意图下，世界会如何演变”。
*   **操作：**
    1.  在预测器（Predictor）的 Transformer 架构中插入**交叉注意力层（Cross-Attention Layers）**。
    2.  Query 是当前的状态 $z_t$，Key/Value 是文本指令的嵌入。
    3.  这样，预测器不仅仅是在预测物理规律，而是在预测“为了实现该指令”而产生的状态变化。
*   **论文关联：** 论文中提到的 AdaLN（自适应层归一化）其实就是一种条件注入方式。我们可以把注入“动作”的地方，改为注入“动作 + 文本嵌入”，让模型根据语言指令来调节动力学预测。
