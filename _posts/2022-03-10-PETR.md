---
layout: post
title: PETR
date: 2022-03-10
categories: [Perception]
tags: Perception
---
<!--more-->


- [paper地址](https://arxiv.org/abs/2203.05625)

# PETR: Position Embedding Transformation for Multi-View 3D Object Detection

## sketch the main points

先回顾一下相关的两个工作, DETR与DETR3D.

DETR的decoder是 (image features + 2d-pos embedding(learnable)) 作为k,v,和obejct query做cross-attention.

DETR3D的decoder是 (object query -->3d reference points --> 投到2d--> 插值拿2d的features) 作为 k,v 与 object query 做cross-attention.

而PETR的decoder做法是 (image features + 3d-pos embedding(learnable)) 作为k,v 和object query 做cross-attention.

有几个关键的组件

### 3D coordinate generator

因为如果知道pixel的深度的话，根据内参就可以转到相机坐标系下,根据外参就可以转到world/ego系下, 
所以在假设了深度,或者沿着深度方向均匀采样之后, 就相当于有了 HWD 个点了. N个相机的话,那就是 NHWD个点.
然后按照paper里面的公式(2), 依据空间长方体的范围, 把点都normalize到[0,1]. 这样便于统一. 
可以看到这个过程不需要网络参数,只要图片大小确定, 相机内外参确定, 3d的坐标就产生好了.

### 3D position Encoder

这一步的目的就是用image feature 与 3d postion 得到 3d features. 

1. 上面3d的坐标经过一个MLP转成3d pos-embedding, 从而由 (NHWD, 3) 变成了(NHW, embed_dim), 充当3d-pos embedding

如何理解这个3D-pos embeding, 首先其个数是 NHW, 怎么就说是3d-pos了, 我觉得是因为深度的信息被编码进了 embed_dim 那个维度中,
而有了深度,其实就相当于得到了3d点, 从这个角度我觉得可以理解为什么叫 3d-pos embedding.

2. 图像的feature, (N,C,H,W) 经过1x1卷积降channel，变成 (N, embed_dim, H, W)

3. 1和2得到的信息相加 就像DETR中的一样. 充当k,v 与object query 做cross attention. 

### 3D query generator

这里为了加快收敛,用了 Anchor-DETR的想法, 即用一致分布的3d anchor points 经过一个小的MLP 来产生 初始的 obejct Quries.
实验证明这样效果比较好. 
另外一点, 会发现这一点与DETR3D刚好相反, DETR3D里由 每一层都由query--> 3d points, 而这里是由 3d points --> 初始query.
细想一下,他们代表的意义本质上都是3d-object.  这样由query--> 3d points 还是由3d points--> query 本质上一样.

## 3D PE的effect

为了验证3d-pe的有效性, 作者把前视相机对应的pos-embedding取出来了三个点. 然后分别与其他相机的pos-embedding 算相似度.
会发现和这3个点相近的地方确实相似度会更高.

## 实现细节

为了提取2D特征，采用ResNet、Swin-Transform或VoVNetV2作为骨干网络。C5特征（第5级的输出）被上采样并与C4特征（第4级的输出）融合以产生P4特征。具有1/16输入分辨率的P4特征被用作2D特征。
对于3D坐标生成，我们沿着深度轴采样64个点，遵循CaDDN中的线性递增离散化（LID）。我们将区域设置为[−61.2m，X和Y轴61.2m]，Z轴设置为[−10m，10m]。3D世界空间中的3D坐标被归一化为[0,1]。遵循DETR3D[51]，我们设置λcls=2.0以平衡分类和回归。

### CaDDN中的线性递增离散化LID是什么

这个可以直接看CaDDN这个paper里面的公式(2)的实现.

## 思考

1. petr 或者 detr, detr3d 这种方式 获得的sparse query, 对于下游任务来说到底行不行? 相比于把bev feature 送给下游确实会更sparese,但是信息相比bev
feature是否少呢?

2. petr没有用到相机内外参? 
用到了, 在position embedding的时候.

3. 时序的时候怎么做.

见petrv2, 其实就相当于把时序当成了, 多了更多的view了.
