---
layout: post
title: RT2
date: 2023-07-28
categories: [VLA]
toc:
    sidebar: left
    max_level: 4
---

[TOC]

# RT2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control


研讨的这篇论文——**RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control**，是Google DeepMind在2023年发布的一项里程碑式工作。这篇论文的核心贡献在于，它打破了传统机器人学习与互联网大规模预训练模型之间的界限，提出了一种被称为**VLA（Vision-Language-Action）**的新范式。


---

### 第一部分：研究背景与核心动机 (Context & Motivation)

在深入技术细节之前，我们需要理解为什么DeepMind要在这个时间点提出RT-2。

#### 1. 机器人学习的困境：莫拉维克悖论的现代演绎
长期以来，通用机器人的构建面临一个巨大的数据瓶颈。我们在互联网上拥有数十亿的文本和图像数据，这使得GPT-4或PaLM这样的模型能够展现出惊人的推理能力。然而，机器人数据是非常昂贵的。收集真实的物理交互数据需要时间、硬件磨损和人工监督。

这就导致了一个现状：
*   **大模型（Web-scale Models）：** 拥有极其丰富的语义知识（知道什么是“麦当赫”，知道“海绵”是软的），但缺乏物理世界的具身控制能力。
*   **传统机器人策略（Robot Policies）：** 能够执行精准的动作（如抓取），但语义理解能力极差，往往只能在封闭环境下处理预定义的物体。

#### 2. 从RT-1到RT-2的跨越
DeepMind之前的作品RT-1（Robotics Transformer 1）已经证明了基于Transformer架构进行端到端（End-to-End）控制的可行性。RT-1虽然成功，但它并没有真正利用大模型在互联网数据上学到的通用知识。它更像是一个在大规模机器人数据上训练出来的“专家”。

RT-2的核心假设非常大胆且优雅：**如果我们能让大模型直接“讲”机器人的语言，是否就能直接继承大模型在互联网上学到的常识、推理和泛化能力？**

RT-2不仅是学习如何映射观察到动作，更是要享受大规模预训练带来的红利。 <alphaxiv-paper-citation title="Goal" page="1" first="Our goal is" last="vision-language data from the web." />

#### 3. VLA：视觉-语言-动作模型
论文提出了一个新的概念类别：VLA。这不仅仅是VLM（视觉语言模型），而是将“动作（Action）”作为与“文本（Language）”同等地位的一种模态。在RT-2中，机器人动作被处理为一种特殊的“语言”token，直接嵌入到模型的输出空间中。

---

### 第二部分：模型架构与实现细节 (Model Architecture & Methodology)

RT-2并不是从零开始训练的，而是基于已有的强大VLM进行改造。

#### 1. 基础模型骨架 (Backbone)
RT-2探索了两种主要的基础架构，均由Google开发：
*   **PaLI-X (5B & 55B参数)：** 这是一个典型的Encoder-Decoder架构。它使用ViT-22B处理图像，然后通过一个类似于UL2的Encoder-Decoder主干网络处理文本和图像嵌入。
*   **PaLM-E (12B参数)：** 这是一个Decoder-only的架构（类似GPT）。它将图像投影到语言embedding空间，然后作为一个统一的多模态序列输入给LLM。

这两种架构代表了当前VLM的两个主流方向，RT-2证明了这套方法对二者都有效。

#### 2. 动作的Token化 (Action Tokenization)
这是RT-2最关键的技术细节：**如何让一个只会输出文本的模型输出电压或位置指令？**

RT-2继承了RT-1的离散化动作空间设计。
*   **动作空间：** 包括6个自由度的末端执行器位姿（x, y, z, roll, pitch, yaw），1个夹爪开合程度，以及1个终止指令（terminate）。共8个维度。
*   **离散化：** 每个连续维度被均匀离散化为256个区间（bins）。
*   **字符串化：** 一个完整的机器人动作被转换为一个由空格分隔的数字字符串。例如：“1 128 91 241 5 101 127”。

为了将自然语言回答和机器人动作适配到同一种格式中，作者将动作表达为文本token。 <alphaxiv-paper-citation title="Format" page="1" first="in order to fit" last="natural language tokens." />

**具体的Token映射策略：**
由于不同的基础模型使用不同的分词器（Tokenizer），RT-2采用了不同的策略：
*   **对于PaLI-X：** 因为其词表中本就包含代表整数（如"1", "2"... "1000"）的token，所以直接将动作bin的数值映射到对应的整数token上。
*   **对于PaLM-E：** 其词表可能没有这种直接对应。RT-2采取了一种“覆写（Overwriting）”策略，选取词表中256个**使用频率最低**的token，强制赋予它们代表动作bin的含义。这实际上是一种**符号微调（Symbol Tuning）**。

#### 3. 输入与输出流 (I/O Flow)
*   **输入：** 机器人摄像头图像 + 文本指令（例如："Q: what action should the robot take to [task instruction]? A:"）。
*   **输出：** 一串代表动作的token序列。

在推理过程中，模型会像生成文本一样，自回归地生成这些动作token，然后将其“反Token化（De-Tokenize）”回连续的控制信号发送给机器人。

---

### 第三部分：训练策略——联合微调 (Co-Fine-Tuning)

如果我们只拿机器人数据去微调（Fine-tune）一个预训练好的VLM，会发生什么？答案是**灾难性遗忘（Catastrophic Forgetting）**。模型会迅速学会控制机器人，但会忘记它在互联网数据上学到的“什么是泰勒·斯威夫特”或者“什么是能量饮料”。

为了解决这个问题，RT-2提出了**联合微调（Co-Fine-Tuning）**策略。

#### 1. 数据混合 (Data Mixture)
训练数据包含两部分：
1.  **机器人轨迹数据（Robot Data）：** 来自RT-1的数据集，包含图像、指令和动作对。
2.  **互联网视觉语言数据（Web Data）：** 原始VLM预训练任务的数据，如视觉问答（VQA）、图像描述（Captioning）等。

#### 2. 训练平衡
在每一个训练Batch中，必须同时包含这两类数据。这是一个精细的平衡过程。
*   如果机器人数据太少，控制策略学不好。
*   如果Web数据太少，语义概念会丢失。
*   实验表明，在联合微调中，需要适当增加机器人数据的采样权重。例如，在RT-2-PaLM-E中，机器人数据的权重约为66%。

这种联合训练使得模型既能接触到抽象的视觉概念，又能学习低层的机器人动作。 <alphaxiv-paper-citation title="Co-Fine-Tuning" page="6" first="We notice that" last="just robot actions." />

#### 3. 输出约束 (Output Constraint)
在推理（Inference）阶段，如果任务是机器人控制，我们必须保证模型输出的是合法的动作token，而不是突然开始写诗。因此，RT-2在解码时会限制采样范围，只允许模型在动作token集合中进行采样。

---

### 第四部分：实验结果与涌现能力 (Experiments & Emergent Capabilities)

这是验证RT-2是否成功的关键。实验使用了6000次真实的机器人评估试验。

#### 1. 泛化能力的提升 (Generalization)
与RT-1（基线模型）相比，RT-2在见过的任务（Seen Tasks）上表现相当，但在**未见过的任务（Unseen Tasks）**上展现了惊人的泛化能力。
*   **未见过的物体（Unseen Objects）：** 比如训练时没见过“海绵”，但测试时让它“拿起海绵”。RT-2可以直接利用VLM的知识识别出海绵并执行抓取。
*   **未见过的背景与环境：** RT-2对光照、背景杂乱程度的鲁棒性远超RT-1。

数据表明，RT-2在未见过的类别上性能提升了约3倍（从RT-1的约30%提升到RT-2的60%以上）。这证明了视觉-语言-动作模型能够将通用的语义理解迁移到机器人控制中。 <alphaxiv-paper-citation title="Generalization" page="2" first="Besides the expected" last="varied instructions," />

#### 2. 涌现能力 (Emergent Capabilities)
这是RT-2最令人兴奋的部分。由于它保留了Web知识，它展现出了机器人数据中根本不存在的能力。作者将其分为三类：

*   **符号理解 (Symbol Understanding)：**
    *   指令：“把可乐放到数字3那里”。
    *   机器人数据中从未有过数字标签。但VLM认识数字，RT-2成功将物体放到了写有“3”的卡片上。
*   **推理 (Reasoning)：**
    *   指令：“把那个适合疲劳的人喝的饮料拿起来”。
    *   机器人不知道什么是“疲劳”或“能量饮料”。但VLM知道“红牛”适合疲劳的人。RT-2成功抓取了红牛，而不是旁边的水。
    *   指令：“拿起那个可以用来当锤子的东西”。
    *   RT-2选择了一块石头。这种**语义推理到物理动作的映射**是前所未有的。
*   **人类识别 (Human Recognition)：**
    *   指令：“把可乐给戴眼镜的人”。
    *   模型成功识别了戴眼镜的人并规划了动作。

这表明，RT-2不仅仅是在做模式匹配，它实际上是在利用其庞大的知识库来解释指令，并将其转化为物理动作。 <alphaxiv-paper-citation title="Capabilities" page="3" first="This includes significantly" last="another object)." />

#### 3. 思维链推理 (Chain-of-Thought, CoT)
受LLM中CoT技术的启发，作者还尝试让RT-2先生成“计划”，再生成动作。
*   输入：图像 + 指令“我饿了”。
*   输出：Plan: "pick rxbar chocolate." -> Action: "1 128..."
这进一步增强了处理复杂多步推理任务的能力。 <alphaxiv-paper-citation title="CoT" page="3" first="We further show" last="energy drink)." />

---

### 第五部分：局限性与讨论 (Limitations & Discussion)

1.  **物理技能的边界 (Physical Skills Limitation)：**
    RT-2虽然聪明，但它的**动作库**受限于训练数据。它不能无中生有地学会一个新的物理动作（比如“后空翻”或“精细的插拔”），如果这些动作没有在RT-1的数据集中出现过。它只是学会了在新的场景下**调用**已有的动作技能。 <alphaxiv-paper-citation title="Limitation" page="11" first="the robot does" last="additional experience." />
2.  **推理成本 (Inference Cost)：**
    运行一个55B参数的模型进行实时控制是非常昂贵的。虽然作者通过多TPU云服务实现了1-3Hz的控制频率，但这对于高频动态任务（如接球）来说是远远不够的。
3.  **闭源与复现：**
    PaLI-X和PaLM-E是Google的闭源模型，且需要巨大的算力，这使得学术界复现RT-2极其困难。

### 总结 (Conclusion)

RT-2不仅是一个工程上的胜利，它验证了一个重要的假设：**如果我们将动作视为一种语言，那么通用的多模态大模型就可以直接转化为通用的机器人大脑。** 它通过联合微调，成功地将互联网规模的语义知识“蒸馏”到了机器人控制策略中，实现了前所未有的泛化能力和涌现出的推理能力。

## 如何实现对输出做截断, 即只输出action

RT-2本质上是一个大语言模型（LLM/VLM），它的默认行为是在整个词表（Vocabulary，通常有几万到几十万个词）中预测下一个词。如果我们不做任何限制，模型完全可能在输出动作指令时突然“走神”，输出“I think the robot should...”这样的文本，这对于实时的机器人控制是致命的。

“输出约束”（Output Constraint）并不是一种魔法，而是在**推理（Inference）阶段**通过对模型输出概率分布（Logits）进行干预来实现的。我们可以将其称为**词表掩码（Vocabulary Masking）**或**Logit Masking**。

下面我将从原理到实现步骤，详细拆解这个过程：

### 1. 前置概念：动作Token集合 (The Valid Action Set)
首先，我们需要定义什么是“合法的输出”。
在RT-2中，机器人的动作空间被离散化为256个区间（bins）。
*   **对于PaLI-X版本：** 它是直接使用词表中代表数字的Token（例如 "1", "2", ..., "256"）。
*   **对于PaLM-E版本：** 它是覆盖了词表中频率最低的256个Token。

无论哪种情况，我们都能确切地知道，只有这**256个特定的Token**（加上一个代表“终止”的Token）是合法的动作指令。我们将这个集合称为 $V_{action}$。

### 2. 核心机制：Logit Masking (Logit掩码)

这是实现“只输出动作”的关键一步。让我们看看在模型推理的每一步发生了什么：

1.  **计算Logits：**
    模型接收当前的图像和历史文本，经过几十层的Transformer计算，在最后一层输出一个向量。这个向量的维度等于整个词表的大小（例如 $|V| = 250,000$）。向量中的每一个数值叫做 **Logit**，代表模型认为下一个词是该词的“原始得分”。

2.  **施加约束（The Constraint Step）：**
    在将这些Logits送入Softmax层转化为概率之前，我们手动介入。
    我们创建一个掩码（Mask），对于所有**不属于**动作集合 $V_{action}$ 的词，我们将它们的Logit值设为一个极小的负数（例如 $-\infty$ 或 $-10^9$）。

    $$
    \text{Logit}[i] = 
    \begin{cases} 
    \text{Original\_Logit}[i] & \text{if } \text{Token}_i \in V_{action} \\
    -\infty & \text{if } \text{Token}_i \notin V_{action}
    \end{cases}
    $$

3.  **Softmax归一化：**
    当我们对处理后的Logits进行Softmax操作时：
    $$ P(x_i) = \frac{e^{\text{Logit}[i]}}{\sum_j e^{\text{Logit}[j]}} $$
    由于 $e^{-\infty} \approx 0$，所有非动作Token的概率都会变成严格的 **0**。

4.  **采样（Sampling）：**
    现在，无论模型原本多么想说一句闲话，它的概率都被强制归零了。模型只能在合法的动作Token中选择概率最大的那个（Greedy Search）或者根据概率分布采样（Sampling）。

通过这种方式，我们从数学上强制模型**只能**输出动作指令。 <alphaxiv-paper-citation title="Constraint" page="5" first="The action space consists" last="256 bins uniformly." />

### 3. 为什么模型不会“感到困惑”？ (Role of Training)

*“强行禁止模型说其他话，模型会不会不知道该选哪个动作了？”*

这就是**联合微调（Co-Fine-Tuning）**的功劳。
在训练阶段，我们已经通过大量的数据（Prompt: "Action: ..." -> Label: "1 128 91..."）教会了模型：当看到特定的Prompt（如“Action:”）时，它的概率分布本能地就会向这些数字Token倾斜。

**Logit Masking** 只是一个“双保险”。实际上，经过训练的RT-2在绝大多数情况下，其原始Logit中最高的几个值本身就是动作Token。Masking主要是为了防止极其罕见的“幻觉”或者在长序列生成中的漂移。

### 4. 格式的严格性

RT-2的输出不仅仅是单个Token，而是一串Token（代表x, y, z, roll, pitch, yaw, gripper）。
通常在工程实现中，这种约束可以更加精细：
*   **固定长度解码：** 我们知道一个动作包含8个Token。我们可以强制模型只解码8步，然后强制截断。
*   **位置相关约束：** 如果不同维度的动作使用了不同的Token区间（RT-2中通常是统一的256个，但有些变体不同），我们甚至可以在第1步只允许x轴的Token，第2步只允许y轴的Token。

### 总结

“输出约束”并不是模型“自己”控制的，而是我们在模型大脑（神经网络）和嘴巴（输出）之间加了一个**过滤器**。
1.  **定义**合法的动作Token集合。
2.  在推理时，将所有非法Token的得分为**负无穷**。
3.  迫使模型只能在动作空间内进行概率选择。

这样，我们就把一个只会“说话”的AI，变成了一个只会“行动”的机器人大脑。


## VLM能力的保持

这是一个极其重要的问题，因为它触及了**多任务学习（Multi-task Learning）**和**持续学习（Continual Learning）**的核心痛点。

简单直接的回答是：**RT-2 的 VLM 能力保持得非常出色，甚至可以说，它不仅保留了原有的视觉语言能力，还将这些能力成功“接地”（Grounding）到了物理世界中。**

但这并非自然发生的，而是精心设计的训练策略的结果。如果处理不当，模型会迅速“变笨”。让我们从三个层面来详细分析这个问题：

### 1. “灾难性遗忘”的威胁 (The Risk of Catastrophic Forgetting)

在早期的尝试中，研究人员发现，如果你拿一个预训练好的大模型（比如 PaLI-X），只用机器人数据（Robot Data）去微调它，会发生什么？
*   **现象：** 模型会迅速学会控制机器人，但在几天甚至几小时内，它就会忘记“什么是蒙娜丽莎”、“什么是红色”、“什么是大象”。
*   **后果：** 它的泛化能力会退化到和 RT-1（从头训练的专家模型）差不多的水平。它变成了一个“熟练工”，但不再是一个“通才”。

这在深度学习中被称为**灾难性遗忘**。如果 RT-2 失去了 VLM 能力，那么我们做这所有的一切（引入大模型）就失去了意义。

### 2. RT-2 的解决方案：联合微调 (Co-Fine-Tuning)

为了保住 VLM 的能力，RT-2 并没有选择“先训练 VLM，再微调 Robot”，而是选择了**混合训练**。

*   **数据配比：** 在训练 RT-2 的每一个 Batch 中，既包含机器人轨迹数据，也包含原始的互联网视觉语言数据（Web Data，如 VQA、Captioning）。
*   **机制：** 这迫使模型在学习输出“动作 Token”的同时，必须继续复习如何输出“自然语言 Token”。
*   **结果：** 论文中的消融实验（Ablation Study）明确指出，**Co-fine-tuning（联合微调）** 是 RT-2 能够泛化到未见物体和指令的关键。

<alphaxiv-paper-citation title="Co-Fine-Tuning Importance" page="10" first="We attribute this to" last="the VLM training." />

论文中写道：“We attribute this to the fact that keeping the original data around the fine-tuning part of training, allows the model to not forget its previous concepts learned during the VLM training.”（我们将此归因于在微调过程中保留原始数据，使得模型不会忘记在 VLM 训练期间学到的先前概念。）

### 3. 证据：涌现能力的证明 (Evidence from Emergent Capabilities)

虽然论文主要展示的是机器人的成功率，但这些成功率本身就是 VLM 能力得以保持的**铁证**。我们可以通过 RT-2 能完成的任务反推它的 VLM 能力：

*   **证据一：数学与逻辑推理**
    *   任务：“将香蕉放到 2 + 1 的和那里。”（Move banana near the sum of two plus one）
    *   分析：机器人数据里绝对没有数学题。如果 RT-2 忘记了 VLM 的数学能力，它根本无法完成这个任务。它能做，说明它保留了 PaLM/PaLI 的数学推理能力。
*   **证据二：名人与知识识别**
    *   任务：“把可乐移给泰勒·斯威夫特（Taylor Swift）的照片。”
    *   分析：机器人从未见过泰勒·斯威夫特。模型能认出来，说明它保留了互联网预训练中关于名人的视觉知识。
*   **证据三：多语言能力**
    *   任务：使用西班牙语或德语指令控制机器人。
    *   分析：RT-2 依然能听懂。这说明它的多语言编码器没有被破坏。

<alphaxiv-paper-citation title="Reasoning" page="22" first="move banna near the" last="of two plus one" />

### 4. 总结与教学延伸

同学们，这里有一个深刻的洞察：在 RT-2 中，**“动作”被视为一种特殊的“语言”**。

这就好比一个原本精通英语、法语、中文的人（VLM），现在我们要教它一门新语言——“机器人语”（Robot Action Tokens）。
*   如果我们只强迫它说“机器人语”，不准它说别的，时间久了它就会忘记英语。
*   RT-2 的做法是：让它在学“机器人语”的同时，继续练习英语和法语。

最终，RT-2 成为了一位**四语通（English, French, Chinese, Robot-ese）**。它不仅保留了原有的语言能力，还能用原本的语言能力（比如用英语思考逻辑）来指导新的语言（用机器人语行动）。

所以，RT-2 证明了：只要通过**联合微调（Co-fine-tuning）**并保持适当的数据配比，VLM 的通用能力不仅可以保持，还能成为机器人策略泛化的基石。



