---
layout: post
title: DROID
date: 2024-03-19
categories: [VLA]
toc:
    sidebar: left
    max_level: 4
---

[TOC]

# DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset

[论文链接](https://arxiv.org/abs/2403.12945)

# 据驱动的具身智能——解构 DROID 数据集与分布式机器人学习
**目标**：通过深度剖析 DROID 论文，理解 Robot Learning 中“数据中心（Data-Centric）”范式的挑战、解决方案及最新进展。

---

## 机器人学习的“数据墙”与历史沿革

### 1.1 引言：莫拉维克悖论的现代版
*   **讨论引入**：为什么 GPT-4 可以写代码，但机器人还没法帮我收拾洗碗机？
*   **核心论点**：机器人领域的瓶颈已从“算法”转向“数据”。
    *   CV/NLP 的成功公式：Transformer + 大规模算力 + **互联网级数据**。
    *   机器人的困境：缺乏“机器人的互联网”。数据无法爬取，必须物理执行。
    *   现状：大多数策略仅能在训练环境（In-Domain）工作，泛化能力（Generalization）极差。

### 1.2 现有数据集的批判性回顾 (Critical Review)
在介绍 DROID 之前，我们需要回顾它想要取代或补充的“前辈”们。
*   **MIME / RoboTurk**：早期尝试，主要靠人类遥操作，但规模小，场景单一。
*   **BridgeData V2**：这是一个重要的里程碑。DROID 的很多设计哲学继承自 Bridge，但 Bridge 主要局限于加州大学伯克利分校（UC Berkeley）的厨房模型，场景多样性不足。
*   **Open X-Embodiment (OXE)**：这是目前最大的聚合数据集。
    *   *缺点*：它是“大杂烩”，包含各种机器人形态、各种控制频率、各种相机角度。这种**分布的不一致性（Inconsistency）**给训练带来了巨大的噪声。
*   **DROID 的生态位**：DROID 试图在“规模（Scale）”和“一致性（Consistency）”之间找到平衡点。它比单一实验室数据大，但比 OXE 这种聚合数据更一致。 <alphaxiv-paper-citation title="Comparison Table" page="2" first="Comparison to existing" last="in this table." />

### 1.3 核心概念定义
*   **In-the-Wild（野外/非受控）**：不仅是背景变化，还包括光照、物体摆放、干扰物。DROID 强调在 52 个不同建筑中采集。 <alphaxiv-paper-citation title="Scale" page="2" first="564 scenes across" last="52 buildings" />
*   **分布（Distribution）**：从场景（Scene）、任务（Task/Verb）、视点（Viewpoint）三个维度理解数据的丰富程度。

---

## 分布式数据工程——如何构建 DROID

*这是最“硬核”的工程部分*

### 2.1 组织层面的挑战：分布式协作 (The Social Engineering)
*   **问题**：如何让全球 13 个机构、50 个采集员收集出看起来像是一个人收集的数据？
*   **解决方案**：极度的标准化。
    *   **硬件统一**：所有实验室必须使用完全相同的硬件清单（BOM）。
    *   **软件统一**：统一的 Docker 容器和数据记录脚本。

### 2.2 硬件平台架构 (The Hardware Stack)
详细拆解 DROID 的采集设备，这对于学生理解机器人系统集成至关重要：
*   **机器人本体**：Franka Panda (7-DoF)。*讨论：为什么选 Panda？（灵敏的力控、科研界的标准机、易于移动）*。
*   **视觉系统**：
    *   **3个 RGB 摄像头**：为什么需要三个？
        *   *Wrist (手眼)*：处理精细操作，解决遮挡。
        *   *Side & Front (第三视角)*：提供全局上下文。 <alphaxiv-paper-citation title="Camera Setup" page="2" first="three synchronized RGB" last="depth information" />
    *   **Stereo Depth (立体深度)**：使用 ZED 2i 相机。虽然目前的扩散策略主要用 RGB，但深度信息对未来的 3D 策略至关重要。
*   **便携性设计**：整个系统被设计为可移动推车。这是实现“In-the-Wild”的关键——如果搬运需要 3 小时，没人会去浴室采集数据。

### 2.3 数据采集协议与流程 (Collection Protocol)
*   **遥操作 (Teleoperation)**：使用 VR 控制器还是 3D 鼠标？DROID 使用的是 Oculus Quest 2 手柄还是其它？（论文中提到是基于 VR 或 3D 空间鼠标的映射，通常为了灵活性）。
*   **元数据标注**：
    *   **语言指令**：采集时不仅是动动机械臂，还必须记录“我在做什么”。DROID 包含自然语言指令。
    *   **成功/失败标注**：这对于训练离线强化学习（Offline RL）至关重要。

### 2.4 数据的时空分布分析
*   **场景分布**：展示论文中的地图分布。
*   **任务分布**：分析 86 个 Verbs。不仅有简单的 Pick & Place，还有 Open, Close, Wipe, Pour 等接触丰富（Contact-Rich）的任务。 <alphaxiv-paper-citation title="Task Diversity" page="1" first="86 Tasks / Verbs" last="86 Tasks / Verbs" />
*   **视点多样性**：DROID 拥有 1417 个独特的相机外参。这迫使模型学习几何特征，而不是死记硬背像素位置。

---

## 策略学习与算法验证

*这部分重点讲解如何利用这些数据进行训练，涉及深度学习模型细节。*

### 3.1 算法基准：Diffusion Policy
*   **背景补充**：简要回顾 Diffusion Policy（扩散策略）。它为什么比 BC-Transformer 或 LSTM-GMM 好？（能建模多模态分布，抗噪性强）。
*   **DROID 的训练设置**：
    *   **输入**：多视角图像 + 语言指令 + 机器人本体感知（Proprioception）。
    *   **输出**：未来的动作序列（Action Chunking）。
    *   **架构**：基于 CNN（ResNet/EfficientNet）的编码器 + Diffusion Head。

### 3.2 实验设计：Co-training (联合训练)
这是论文的核心发现。**并不是只用 DROID 训练，而是把 DROID 作为一种“通用先验”混入到特定任务数据中。**
*   **实验组设置**：
    1.  仅使用特定任务的小样本数据（Target Data Only）。
    2.  特定任务数据 + OXE 数据。
    3.  特定任务数据 + DROID 数据。
*   **关键结论**：
    *   **结论 1：DROID 显著提升性能**。引入 DROID 数据后，平均成功率提升了 20%。 <alphaxiv-paper-citation title="Performance Boost" page="2" first="boosts policy performance," last="20% on average" />
    *   **结论 2：DROID > OXE**。在同等条件下，混入 DROID 数据比混入 OXE 数据效果更好。
        *   *深度讨论*：为什么？OXE 数据量更大（1.4M vs 76k），为什么输了？
        *   *解释*：**域差异（Domain Gap）**。OXE 里的机器人和环境差异太大，模型很难迁移。而 DROID 虽然场景变了，但机器人的动力学和相机相对位置（部分）是标准化的。这证明了高质量、同构数据的重要性。

### 3.3 鲁棒性与分布外泛化 (OOD Generalization)
*   **干扰物测试**：在桌面上扔一堆没见过的垃圾，模型还能工作吗？实验表明 DROID 训练的模型更不在乎背景噪声。
*   **位置变化**：移动相机或机器人底座。由于 DROID 训练集里包含了大量随机的相机位姿，模型学会了空间不变性。 <alphaxiv-paper-citation title="Robustness" page="8" first="robustness to distractor" last="spatial perturbations." />

---

## 深度研讨与未来展望

### 开放性问题与未来方向
*   **Visual Foundation Models (VFM)**：如何利用 DROID 训练像 CLIP 或 R3M 这样的视觉表征模型？
*   **World Models**：能否用这 350 小时的视频训练一个从当前帧预测下一帧的世界模型？
*   **Language Grounding**：DROID 的语言指令相对简单，如何扩展到复杂的推理任务？

---

## 具体到一个sample


---

### 解剖一只“麻雀”——详解 DROID 训练样本

我们一直在说 76k 条轨迹，但每一条轨迹到底长什么样？如果现在用 Python 打开其中一个 `.h5` 或者 `.zarr` 文件，会看到什么？让我们以一个具体的任务为例：**‘将苹果放入锅中’（Put apple in pot）**，来解构一个标准的 DROID 样本。

#### 1. 基本概念：轨迹 (Trajectory) vs. 样本 (Sample)

首先要澄清，磁盘上存储的是**轨迹（Trajectory）**，而送入神经网络训练的是**样本（Sample/Batch）**。
*   **一条轨迹**：是一个完整的交互过程，从机器人开始动，到任务完成，通常持续 10~30 秒。假设采样频率是 15Hz，一条 20 秒的轨迹就包含 $T=300$ 个时间步（Timesteps）。
*   **一个训练样本**：通常是从这 300 个时间步中随机切片出来的一小段（或者是当前帧 + 历史帧）。

我们将重点分析**一个时间步（Timestep $t$）**包含的所有数据字段。

---

#### 2. 数据字段详解 (The Anatomy of a Timestep)

想象一个 Python 字典 `data_dict`，它包含以下核心键值对：

##### A. 语言指令 (Language Instruction)
这是任务的“意图（Intent）”。
*   **Key**: `language_instruction`
*   **Value (String)**: `"Pick up the red apple and place it inside the silver cooking pot."`
*   **特点**：
    *   **自然语言**：不是由模版生成的僵硬指令（如 "Move to X, Y"），而是人类采集者自己写的或者口述的。这意味着会有同义词、语序变化甚至拼写错误，增加了数据的多样性。
    *   **不变性**：通常在整条轨迹中，这个指令是保持不变的（或者包含分解的子指令，但在 DROID 中主要是高层指令）。

##### B. 视觉观测 (Visual Observations) —— 机器人的“眼睛”
DROID 的核心优势在于多视角。对于每一个时间步 $t$，我们有三张 RGB 图片。
*   **Key**: `image_primary` (或 `cam_high`)
    *   **内容**：第三人称视角，俯瞰整个工作台。能看到机器人底座、桌子、苹果和锅的相对位置。
    *   **作用**：提供全局上下文（Context），防止局部视野丢失目标。
*   **Key**: `image_wrist` (或 `cam_left_wrist`)
    *   **内容**：安装在机械臂手腕上的相机。随着手臂移动而移动。当手抓向苹果时，苹果在画面中会越来越大。
    *   **作用**：解决遮挡问题（Occlusion）。当机械臂去抓苹果时，主相机可能被手臂挡住，这时手眼相机是唯一能看清细节的。
*   **Key**: `image_secondary` (或 `cam_varied`)
    *   **内容**：另一个第三人称视角，通常在侧面或正面。
    *   **作用**：提供立体视差信息，帮助模型理解深度。
*   **数据格式**：通常是 $(3, H, W)$ 或 $(H, W, 3)$ 的张量，像素值归一化到 $[0, 1]$ 或 $[-1, 1]$。
    *   *注：DROID 还包含深度图（Depth Maps），但在训练 Diffusion Policy 时通常只用 RGB。*

##### C. 本体感知 (Proprioception) —— 机器人的“触觉/体感”
机器人需要知道自己的手臂现在在哪里。
*   **Key**: `joint_position` (关节角度)
    *   **Value**: $\mathbb{R}^7$ 向量（因为 Franka Panda 是 7 自由度机械臂）。
    *   **示例**: `[0.12, -0.45, 0.0, -1.2, 0.05, 1.5, 0.8]` (弧度制)。
*   **Key**: `gripper_position` (夹爪开度)
    *   **Value**: $\mathbb{R}^1$ 标量。
    *   **示例**: `0.08` (表示夹爪张开 8cm，处于 Open 状态) 或 `0.0` (Closed)。
*   **Key**: `end_effector_pose` (末端执行器位姿)
    *   **Value**: $\mathbb{R}^7$ 向量 (位置 $x,y,z$ + 四元数 $qx,qy,qz,qw$)。
    *   **作用**：告诉模型“我的手现在在这个笛卡尔坐标系的什么位置”。

##### D. 动作 (Action) —— 模型的“预测目标” (Label)
这是**最关键**的部分。在模仿学习中，我们在时间步 $t$，希望模型预测**未来**的动作。
*   **Key**: `action`
*   **Value**: 通常是**目标位姿**或**增量位姿**。在 DROID 的标准训练设置（如 Diffusion Policy）中，Action 通常表示为**下一步的末端执行器位姿变化**或**绝对关节角度**。
    *   如果使用 **Delta Cartesian Control** (常用)：
        *   Action $\in \mathbb{R}^{6+1}$ (6D 位姿变化 $\Delta x, \Delta y, \dots$ + 1D 夹爪开合)。
        *   示例：`[0.01, 0.0, -0.01, 0, 0, 0, 1]` —— 意思是“向前移动 1cm，向下移动 1cm，夹爪保持张开”。
*   **Action Chunking (动作分块)**:
    *   虽然我们在时间步 $t$，但 Diffusion Policy 不止预测 $t+1$，而是预测未来 $k$ 步的动作序列（例如 $k=16$）。
    *   所以，训练时的 Label 实际上是一个序列：$A_{t:t+k}$，形状为 $(16, 7)$。

---

#### 3. 具体的代码/数据结构演示


```python
# 一个 DROID 训练 Batch 的结构 (Batch Size = B, Chunk Size = T_pred)

batch = {
    # 1. 语言指令 (经过 BERT/T5 编码后的 Embedding)
    "language_embedding": torch.Tensor(shape=(B, 768)), 
    # 对应原文: "Put the apple in the pot"

    # 2. 视觉观测 (当前帧 t 的图像)
    "observation": {
        "image_primary":   torch.Tensor(shape=(B, 3, 256, 256)), # 俯视
        "image_wrist":     torch.Tensor(shape=(B, 3, 256, 256)), # 手眼
        "proprioception":  torch.Tensor(shape=(B, 8))            # 7关节 + 1夹爪
    },

    # 3. 动作标签 (Ground Truth, 从 t 到 t+k 的未来动作序列)
    "action": torch.Tensor(shape=(B, 16, 7)) 
    # 16步长的动作序列。前3维是位置，后3维是旋转(如欧拉角)，最后1维是夹爪
}
```

---

#### 4. 为什么 DROID 的数据难处理?

DROID 数据处理的难点，体现“In-the-Wild”的挑战：
1.  **光照剧变**：你在 `image_primary` 里看到的像素值分布，在不同厨房里完全不同。有的厨房是暖黄光，有的是冷白光，有的甚至有阳光直射造成的过曝。这就要求模型具有极强的**视觉不变性（Visual Invariance）**。
2.  **背景杂乱**：在实验室数据中，背景通常是干净的桌子。但在 DROID 中，`image_primary` 的背景里可能有洗洁精、抹布、微波炉、甚至走动的人。模型必须学会**注意力机制（Attention）**，只关注指令里提到的“Apple”和“Pot”，忽略旁边的“Dish soap”。
3.  **相机抖动**：虽然有外参标定，但推车可能会有微小的晃动，导致外参不仅仅是静态的，这增加了学习坐标映射的难度。

## baseline model

作者选择的 Baseline 是 **Diffusion Policy（扩散策略）**。
具体的实现是基于 **Robomimic** 代码库构建的 **CNN-based U-Net Diffusion Policy**。


---

### Baseline 模型解构——Diffusion Policy

为了验证 DROID 数据的威力，我们不需要发明新的轮子。我们需要的是一辆性能最好的‘赛车’来测试这种‘新燃油’（数据）。在这篇论文中，作者选择了 **Diffusion Policy** [Chi et al., RSS 2023]。为什么选它？因为它目前在模仿学习领域统治力最强，能够处理多模态分布（Multimodal Distribution），非常适合处理复杂的非受控环境数据。

#### 1. 整体架构 (High-Level Architecture)

模型可以分为三个主要部分：**感知编码器（Perception Encoders）**、**特征融合（Feature Fusion）** 和 **扩散生成头（Diffusion Generation Head）**。

##### A. 感知编码器 (The Eyes & Ears)
模型如何处理多模态输入？
*   **视觉主干 (Vision Backbone)**：
    *   使用 **ResNet-50**。
    *   **预训练**：使用标准的 **ImageNet** 预训练权重。这意味着模型在看到机器人数据之前，已经认识了“边缘”、“纹理”和一般的物体特征。
    *   **输入处理**：图像被调整为 $128 \times 128$ 分辨率。为了增加鲁棒性，训练时使用了随机裁剪（Random Crop）和颜色抖动（Color Jitter）作为数据增强。 <alphaxiv-paper-citation title="Visual Encoder" page="18" first="ResNet-50 visual encoder" last="of the visual inputs." />
*   **语言主干 (Language Backbone)**：
    *   使用 **DistilBERT**。
    *   **冻结 (Frozen)**：在训练过程中，DistilBERT 的参数是不更新的。它只负责把自然语言指令（如 "Put the apple in the pot"）转换成一个固定的语义向量（Embedding）。 <alphaxiv-paper-citation title="Language Encoder" page="18" first="frozen DistilBERT [45]" last="language embedding" />

##### B. 特征融合 (Feature Aggregation)
*   **拼接 (Concatenation)**：
    *   作者没有使用复杂的 Cross-Attention，而是简单粗暴但有效地使用了**拼接**。
    *   将 ResNet-50 提取的视觉特征向量、DistilBERT 提取的语言向量、以及机器人的本体感知（关节角度）向量，直接拼接成一个长向量。
    *   这个长向量随后通过一个多层感知机（MLP, specifically `[1024, 512, 512]`）进行降维和特征融合。 <alphaxiv-paper-citation title="MLP Fusion" page="18" first="concatenated features are" last="Processing MLP" />

##### C. 扩散生成头 (Diffusion Head) —— 核心大脑
这是模型最关键的部分。它不是直接输出动作 $a$，而是学习**去噪（Denoising）**。
*   **架构**：**Conditional U-Net**。
    *   这是一个 1D U-Net（处理时间序列）。
    *   它接收两个输入：
        1.  **带噪声的动作序列** (Noisy Action Sequence, $A_k$)。
        2.  **条件特征** (Conditioning Features, 来自上面的 MLP 输出)。
*   **任务**：预测噪声 $\epsilon$。即给定当前状态特征和带噪声的动作，预测加了多少噪声，从而反向推导出干净的动作。

---

#### 2. 输入与输出 (Inputs & Outputs)

这定义了模型实际上在“吃”什么和“产”什么。

*   **输入 (Observation Horizon = 2)**：
    *   模型不仅看当前帧 $t$，还看前一帧 $t-1$。
    *   这意味着输入张量包含了历史信息，有助于推断速度和加速度。 <alphaxiv-paper-citation title="Observation Horizon" page="18" first="Observation Horizon" last="2" />
*   **输出 (Prediction Horizon = 16)**：
    *   **Action Chunking（动作分块）**：模型一次性生成未来 **16 步** 的动作序列 $(a_t, a_{t+1}, \dots, a_{t+15})$。
    *   **执行策略 (Receding Horizon Control)**：
        *   虽然预测了 16 步，但机器人只执行前 **8 步**（Action Horizon = 8）。
        *   执行完 8 步后，重新推理，再预测 16 步，再执行 8 步。
        *   为什么要这样？这增加了动作的连贯性（Smoothness），并允许模型以比控制频率更低的频率进行推理（节省算力）。* <alphaxiv-paper-citation title="Action Horizon" page="18" first="Prediction Horizon 16" last="Action Horizon 8" />

---

#### 3. 训练流程 (Training Process)

1.  **采样**：从 Dataset 中随机抽取一段长度为 16 的真实动作序列 $A_{gt}$。
2.  **加噪**：在 $A_{gt}$ 上添加高斯噪声 $\epsilon \sim \mathcal{N}(0, I)$，得到 $A_{noisy}$。
3.  **前向传播**：
    *   图片/语言 $\rightarrow$ ResNet/BERT $\rightarrow$ 特征向量 $Z$。
    *   U-Net 接收 $(A_{noisy}, Z, \text{timestep } k)$。
4.  **预测**：U-Net 输出预测的噪声 $\hat{\epsilon}$。
5.  **Loss 计算**：计算 MSE Loss: $\mathcal{L} = || \epsilon - \hat{\epsilon} ||^2$。
6.  **反向传播**：更新 ResNet 和 U-Net 的参数（DistilBERT 冻结）。

**关键超参数**：
*   **Batch Size**: 128
*   **Learning Rate**: 1e-4 (with Linear Scheduler)
*   **Optimizer**: AdamW
*   **Training Steps**: 25,000 ~ 50,000 步（相对较短，说明预训练特征很有效）。 <alphaxiv-paper-citation title="Hyperparameters" page="18" first="Batch Size 128" last="Train Steps 25000" />

---

#### 4. 为什么选择这个 Baseline？(Why This Model?)

1.  **处理多模态分布**：机器人操作中，同一个指令（"拿起杯子"）可能有多种合法的动作（左手拿、右手拿、先推后拿）。传统的 MSE Regression 会输出这几种动作的“平均值”，导致机器人卡在中间不动。Diffusion Policy 能表达**多峰分布（Multimodal Distribution）**，这是处理 DROID 这种多样化数据的关键。
2.  **稳定性**：相比于 GAN 或 Flow-based models，Diffusion 训练更稳定，超参数敏感度更低。
3.  **标准化**：作者的目标是评测数据。使用社区公认的 SOTA 模型，排除了“你的模型太弱导致效果不好”的质疑。

---

### 总结

“简单来说，Baseline 就是一个**‘看图说话’的生成模型**。它看着当前的 RGB 图像和语言指令，并在脑海中（U-Net）通过去噪过程，想象出未来 1秒钟（16步）内手该怎么动。DROID 的作用，就是提供了海量的‘图-文-动作’对，让这个模型见多识广，不再因为换了个桌布就不知道手往哪放了。”


## 注意特征融合不是按channel拼接

在 DROID 使用的这套 Diffusion Policy 架构中，融合是发生在**一维向量（1D Vector）**层面的。

接下来详细拆解一下这个过程，这对于理解如何处理“图像+语言+本体”这种异构数据非常关键。

### 1. 为什么不能直接按 Channel 拼接？

按 Channel 拼接（例如 `torch.cat([feat1, feat2], dim=1)`）通常要求两个特征图在**空间维度（Height, Width）**上是一致的。

但在 DROID 的设置中：
*   **视觉特征**：来自 ResNet，原本是有空间结构的。
*   **语言特征**：来自 BERT，是一个语义向量，没有 $H \times W$ 的空间概念。
*   **本体感知**：是关节角度（如 7 个浮点数），更没有空间概念。

你无法把一个 $7 \times 1$ 的关节角度向量直接拼到一个 $128 \times 128$ 的特征图后面。

### 2. DROID 的融合方式：扁平化拼接 (Flat Concatenation)

具体的融合流程如下：

#### 第一步：特征“向量化” (Vectorization)
首先，所有的模态都被压缩或映射成了**一维向量**。

1.  **视觉 (Vision)**：
    *   $128 \times 128$ 的图像经过 ResNet-50。
    *   ResNet 的末端通常会接一个 **Global Average Pooling (全局平均池化)** 或者 **Spatial Softmax**（Robomimic 的常用做法）。
    *   结果：变成了形状为 $(B, D_{vision})$ 的一维向量（例如 $D=2048$ 或 $D=512$）。<alphaxiv-paper-citation title="Visual Embeddings" page="18" first="produce embeddings for" last="visual inputs." />
2.  **语言 (Language)**：
    *   经过 DistilBERT，取 `[CLS]` token 或平均池化。
    *   结果：形状为 $(B, 768)$ 的一维向量。
3.  **本体 (Proprioception)**：
    *   本身就是形状为 $(B, D_{prop})$ 的一维向量（如关节角+夹爪状态）。

#### 第二步：直接拼接 (Direct Concatenation)
将上述三个向量在**特征维度**上直接串联起来。

$$
E_{total} = [E_{vision\_cam1}, E_{vision\_cam2}, E_{vision\_wrist}, E_{language}, E_{prop}]
$$

拼接后的向量会非常长。假设视觉是 512 维，用了 3 个相机，语言 768 维，本体 8 维：
$$ \text{Length} \approx 512 \times 3 + 768 + 8 = 2312 $$

<alphaxiv-paper-citation title="Concatenation" page="18" first="These embeddings are" last="observation keys." />

#### 第三步：MLP 融合 (MLP Fusion)
这个超长的拼接向量 $E_{total}$ 随后被送入一个 **Observation Processing MLP**（多层感知机）。
*   **结构**：`[1024, 512, 512]`
*   **作用**：将不同模态的信息进行非线性混合，并压缩维度。
*   **最终输出**：一个紧凑的条件特征向量（Conditioning Vector），这个向量会被送入 Diffusion U-Net 去控制噪声的预测。

<alphaxiv-paper-citation title="MLP Architecture" page="18" first="fed through an" last="Processing MLP" />

### 3. 这种做法的优缺点

*   **优点**：
    *   **简单高效**：工程实现非常简单，计算量也比 Cross-Attention 小。
    *   **全局信息整合**：MLP 层能很好地学习到“当语言是 X 且 图像是 Y 时”的非线性关系。
*   **缺点**：
    *   **丢失空间信息**：这是最大的痛点。当你把 ResNet 的 Feature Map 池化成一个向量时，你丢失了“苹果在图片的左上角”这种精确的空间坐标信息。
    *   *注：虽然 Spatial Softmax 可以保留一定的坐标信息，但相比于保留完整的 2D Feature Map，信息损失依然很大。*
*   **替代方案 (SOTA 方向)**：
    *   现在的 Transformer-based 策略（如 RT-2, ACT）倾向于保留 Visual Tokens（即不池化），然后用 **Cross-Attention** 来融合语言和图像。这允许模型在像素级别关注特定的物体。

**总结**：DROID 的 Baseline 采用的是**一维向量拼接（Vector Concatenation）**，这是为了适配随后全连接层（MLP）的处理方式。
